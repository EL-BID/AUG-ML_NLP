{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialized Documents in English - Stanza (workpaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: Specialized Documents in English - Stanza (workpaper)\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): pdf files\n",
    "#-- Description:  \n",
    "#                - Specialized Documents in English, transitioned to Stanza\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  1.2\n",
    "#-- Last Update: 02/08/2021\n",
    "#-- Last Revision Date: 12/08/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import joblib\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_selector(knowledge_area, language):\n",
    "    '''\n",
    "    Returns the folder name containing the desired documents.\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (10/14/2020)\n",
    "    '''   \n",
    "    if knowledge_area == 'cybersecurity':\n",
    "        if language == 'en':\n",
    "            return \"/specialized_docs_EN\"\n",
    "        else:\n",
    "            return \"/specialized_docs_SP\"\n",
    "    elif knowledge_area == 'digital_transf':\n",
    "        if language == 'en':\n",
    "            return \"/specialized_docs_Digital_EN\" \n",
    "        \n",
    "        \n",
    "def filter_df(knowledge_area, language, df):\n",
    "    '''\n",
    "    Returns the dataframe df filtered by 'Knowledge_Area' (cybersecurity, digital_transf) and language (en,sp)\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (10/14/2020)\n",
    "    ''' \n",
    "    try: \n",
    "        return df[df.Applicability.str.contains(language) & df.Knowledge_Area.str.contains(knowledge_area)]\n",
    "    except Exception as e:\n",
    "        print('error in dataframe: ', e)\n",
    "\n",
    "def generate_base(knowledge_area, language, df):\n",
    "    '''\n",
    "    Returns a new and filtered dataframe of documents to be read along with their respective path.\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (10/14/2020)\n",
    "    '''     \n",
    "    # English documents:\n",
    "    #knowledge_area = 'cybersecurity'; language = 'en'; df = doc_library\n",
    "    folder_prefix = './' + folder_selector(knowledge_area, language) + '/'\n",
    "    #filter_df('digital_trans', 'en', doc_library)\n",
    "    if language == 'en':\n",
    "        return folder_prefix, filter_df(knowledge_area, language, df).iloc[0:][['Title_EN', 'Filename_EN', 'Short_Name', 'Page_Range']].reset_index(drop=True)\n",
    "    else:\n",
    "        return folder_prefix, filter_df(knowledge_area, language, df).iloc[0:][['Title_SP', 'Filename_SP', 'Short_Name', 'Page_Range']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filename):\n",
    "    '''\n",
    "    Reads a pdf file using Tika+Beautiful Soup and returns a list containing each page as string\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (10/14/2020)\n",
    "    ''' \n",
    "    pages_txt = []\n",
    "    \n",
    "    # Read PDF file\n",
    "    data = parser.from_file(filename, xmlContent=True)\n",
    "    xhtml_data = BeautifulSoup(data['content'])\n",
    "    for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "        # Parse PDF data using TIKA (xml/html)\n",
    "        # It's faster and safer to create a new buffer than truncating it\n",
    "        # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "        _buffer = StringIO()\n",
    "        _buffer.write(str(content))\n",
    "        parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "    \n",
    "        # Add pages\n",
    "        if parsed_content['content'] != None:    # page is not blank page\n",
    "            text = parsed_content['content'].strip()\n",
    "        else: \n",
    "            text = ''\n",
    "        \n",
    "        pages_txt.append(text)\n",
    "    \n",
    "    return pages_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized Docs database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_library = pd.read_excel('./output/LDA_analisis.xlsx', sheet_name='documentos', skiprows=2, usecols='B:P')\n",
    "doc_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_library[doc_library.Applicability.str.contains('en')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cybersecurity documents (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_prefix, df_base = generate_base('cybersecurity', 'en', doc_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# modify the page range to literally be as tuple\n",
    "df_base['Page_Range'] = [literal_eval(x) for x in df_base['Page_Range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Specialized documents and store their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a column to store the file content:\n",
    "df_base['content'] = ''\n",
    "\n",
    "doc_count = 0\n",
    "\n",
    "for index, row in df_base.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    \n",
    "    # get filename:\n",
    "    filename = folder_prefix + df_base.iloc[:,1][index]\n",
    "    \n",
    "    # read pdf file\n",
    "    pages_txt = read_pdf(filename)\n",
    "    \n",
    "    # save results and report status:\n",
    "    df_base.at[index, 'content'] = pages_txt\n",
    "    doc_count += 1\n",
    "    \n",
    "    print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "    del pages_txt\n",
    "    del filename\n",
    "    print('------')\n",
    "    print()\n",
    "print('Documents read:', str(doc_count))\n",
    "del doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.at[9, 'Page_Range'] = (9, 68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.content[9][68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to be removed: \n",
    "hd_list_en = ['April 16, 2018  Cybersecurity Framework Version 1.1 \\n\\n\\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.CSWP.04162018 ', \\\n",
    "              None, \\\n",
    "              'This publication is available free of charge from\\n: https://doi.org/10.6028/N\\n\\n\\nIS\\nT.S\\n\\n\\nP\\n.800-53r5 \\n\\n\\n\\n\\n\\n', \\\n",
    "              'NIST IR 7298 Revision 2, Glossary of Key Information Security Terms \\n\\n\\n', \\\n",
    "              'POWER SECTOR DEPENDENCY ON TIME SERVICE \\nApril 2020 \\n\\n\\n ', \\\n",
    "              ' GUIDE FOR AN ASSET INVENTORY MANAGEMENT IN ICS \\n\\n\\n', \\\n",
    "              'PROCUREMENT GUIDELINES FOR CYBERSECURITY IN HOSPITALS \\nFebruary 2020 \\n\\n\\n', \\\n",
    "              'Communication network dependencies for ICS/SCADA Systems \\nDecember 2016   \\n\\n\\n\\n\\n\\n', \\\n",
    "              'Prime Minister’s Office National Cybersecurity Strategy  2019', \\\n",
    "              None, \\\n",
    "              'SPECIAL PUBLICATION 800-82 REVISION 2                   GUIDE TO INDUSTRIAL CONTROL SYSTEMS (ICS) SECURITY', \\\n",
    "              '  THE (ISC)2 CYBERSECURITY LEXICON THE (ISC)2 CYBERSECURITY LEXICON  ', \\\n",
    "              None, \\\n",
    "              None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the content cleaned:\n",
    "df_base['content_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_base.iterrows():\n",
    "    print('### Processing index: ', str(index), ' - page range:', df_base['Page_Range'][index])\n",
    "    texto = ''\n",
    "    for j in range(df_base['Page_Range'][index][0],df_base['Page_Range'][index][1]+1):\n",
    "        \n",
    "        # header clean-up\n",
    "        if hd_list_en[index] != None:\n",
    "            page = df_base['content'][index][j].replace(hd_list_en[index], ' \\n ')\n",
    "        else:\n",
    "            page = df_base['content'][index][j].replace(\"\\n\\n\\n\", \" \")\n",
    "            \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,3}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,3}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "            \n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "            \n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    # - remove chars:\n",
    "    texto = texto.replace('\\uf0b7', ''); texto = texto.replace('\\uf06e', '')\n",
    "    \n",
    "    # - remove newline:\n",
    "    texto = texto.replace('\\n\\n\\n', ' '); texto = texto.replace('\\n', '')\n",
    "    \n",
    "        \n",
    "    df_base.at[index, 'content_cleaned'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['extracted_cleaned'] = ''\n",
    "\n",
    "for index, row in df_base.iterrows():\n",
    "    texto = df_base['content_cleaned'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not (x.isdigit()), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_base.at[index, 'extracted_cleaned'] = res_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base['extracted_cleaned'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['content'][5][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['content'][5][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: technical documents store (English)\n",
    "f_df_base = 'df_technical_docs_cyber_english_2020-12-08_v1.joblib'\n",
    "joblib.dump(df_base, './output/' + f_df_base + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital Transformation documents (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_prefix, df_base_digital = generate_base('digital_transf', 'en', doc_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# modify the page range to literally be as tuple\n",
    "df_base_digital['Page_Range'] = [literal_eval(x) for x in df_base_digital['Page_Range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital.at[3, 'Short_Name'] = 'dig_workgrp_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Specialized documents and store their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a column to store the file content:\n",
    "df_base_digital['content'] = ''\n",
    "\n",
    "doc_count = 0\n",
    "\n",
    "for index, row in df_base_digital.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    \n",
    "    # get filename:\n",
    "    filename = folder_prefix + df_base_digital.iloc[:,1][index]\n",
    "    \n",
    "    # read pdf file\n",
    "    pages_txt = read_pdf(filename)\n",
    "    \n",
    "    # save results and report status:\n",
    "    df_base_digital.at[index, 'content'] = pages_txt\n",
    "    doc_count += 1\n",
    "    \n",
    "    print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "    del pages_txt\n",
    "    del filename\n",
    "    print('------')\n",
    "    print()\n",
    "print('Documents read:', str(doc_count))\n",
    "del doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital.content[4][5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to be removed: \n",
    "hd_list_digital_en = [None, \\\n",
    "                      'Roundtable on Digitising European Industry: Working Group 1 - Digital Innovation Hubs \\n\\n\\n', \\\n",
    "                      None, \\\n",
    "                      None, \\\n",
    "                      'The Future of Electricity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the content cleaned:\n",
    "df_base_digital['content_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for index, row in df_base_digital.iterrows():\n",
    "    print('### Processing index: ', str(index), ' - page range:', df_base_digital['Page_Range'][index])\n",
    "    texto = ''\n",
    "    for j in range(df_base_digital['Page_Range'][index][0],df_base_digital['Page_Range'][index][1]+1):\n",
    "        \n",
    "        # header clean-up\n",
    "        if hd_list_digital_en[index] != None:\n",
    "            page = df_base_digital['content'][index][j].replace(hd_list_digital_en[index], ' \\n ')\n",
    "        else:\n",
    "            page = df_base_digital['content'][index][j].replace(\"\\n\\n\\n\", \" \")\n",
    "            \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,3}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,3}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "            \n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "            \n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    # - remove chars:\n",
    "    texto = texto.replace('\\uf0b7', ''); texto = texto.replace('\\uf06e', '') ; texto = texto.replace('\\uf0a4', '')\n",
    "    \n",
    "    # - remove newline:\n",
    "    texto = texto.replace('\\n\\n\\n', ' '); texto = texto.replace('\\n', '')\n",
    "    \n",
    "        \n",
    "    df_base_digital.at[index, 'content_cleaned'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital['extracted_cleaned'] = ''\n",
    "\n",
    "for index, row in df_base_digital.iterrows():\n",
    "    texto = df_base_digital['content_cleaned'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not (x.isdigit()), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_base_digital.at[index, 'extracted_cleaned'] = res_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base_digital['extracted_cleaned'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital['content'][3][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: technical documents store (English)\n",
    "f_df_base_digital = 'df_technical_docs_digital_english_2020-12-08_v1.joblib'\n",
    "joblib.dump(df_base_digital, './output/' + f_df_base_digital + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_digital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_base,df_base_digital], ignore_index=False)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: both cyber and digital_transformation technical documents store (English)\n",
    "f_df_test = 'df_technical_docs_all_english_2020-12-08_v1.joblib'\n",
    "joblib.dump(df_test, './output/' + f_df_test + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English: NLP n-Gram Analysis - using Textacy bag-of-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, en_core_web_lg\n",
    "nlp_en = spacy.load('en_core_web_lg', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en.max_length = 1432000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.2 update:\n",
    "df_test = joblib.load('./output/nlp_spec_docs_annotated_2020-12-09_english_v1_final.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_es = stopwords.words('spanish')\n",
    "stop_words_en = stopwords.words('english')\n",
    "\n",
    "final_stop_words = stop_words_es + stop_words_en\n",
    "\n",
    "# Spacy stop_words\n",
    "final_stop_words.extend(nlp_en.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stop_words:\n",
    "final_stop_words.extend(['ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'ill', 'descripción', 'componente', 'objetivo', 'ct', 'mailto', 'país', 'millón', 'millones', \\\n",
    "                   'año', 'años', 'dólar', 'dolar', 'dólares', 'si', 'bid', 'us', 'oc', 'gn', 'tc', 'atn', 'opc', 'pib', 'ar', 'br', 'uy', 'cl', 'co', \\\n",
    "                   'cclip', 'pbl', 'uis', 'ab', 'org', 'pr', 'bo', 'bl', 'pe', 'ec', 'ja', 'mx', 'ca', 'gu', 'su', 'ho', 'hn', 'mr', 'rg', 'ee', 'uu', \\\n",
    "                   'cr', 'tdr', 'rn', 'nº', 'usd', 'gy', 've', 'et', 'the', 'for', 'to', 'grt', 'fm', 'pr', 'pa', 'ni', 'aa', 'es', 'sp', \\\n",
    "                   'inglés', 'cty', 'nv', 'profisco', 'asimismo', 'actual', 'costo', 'resultar', 'esperar', 'ejecutar', 'unidad', 'agencia', 'justificación', \\\n",
    "                   'véase', 'ct', 'dela', 'enel', 'sobrar', 'of', 'único'])\n",
    "\n",
    "final_stop_words = list(set(final_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(final_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Textacy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "# Load Spacy English model in Textacy:\n",
    "en = textacy.load_spacy_lang('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textacy processing on extracted text: \n",
    "df_test['textacy_processing'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused module removed from the pipeline and memory increase for processing the documents:\n",
    "en.remove_pipe('ner'); en.remove_pipe('parser'); en.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.max_length = 1432000 # or even higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_test.at[index, 'textacy_processing'] = textacy.make_spacy_doc(df_test.extracted_cleaned[index], lang=en)\n",
    "    \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. List of Terms (Bag-of-Terms): n-Grams extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. List of Terms Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['textacy_processing'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['alt2_list_terms_base'] = ''\n",
    "df_test['alt2_list_terms'] = ''\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    print('processing:', index)\n",
    "    #generate terms (returns a generator):\n",
    "    terms_list = df_test['textacy_processing'][index]._.to_terms_list(ngrams=(2, 3, 4, 5, 6), entities=False, normalize=\"lower\", weighting=\"count\", as_strings=True, filter_stops=True, filter_punct=True, filter_nums=True, include_pos=['PROPN', 'NOUN', 'ADJ', 'ADP'], min_freq=2)\n",
    "\n",
    "\n",
    "    #convert to list:\n",
    "    terms_list = list(terms_list)\n",
    "\n",
    "    #create dictio {term, count}:\n",
    "    resultado_pre = Counter([item.lower() for item in terms_list])\n",
    "    \n",
    "    #store result:\n",
    "    df_test.at[index, 'alt2_list_terms_base'] = resultado_pre\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.alt2_list_terms_base[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en', use_gpu=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    resultado = []\n",
    "    \n",
    "    print('processing:', index)\n",
    "    #compute desired pos and join terms:\n",
    "    for k,v in df_test['alt2_list_terms_base'][index].items():\n",
    "        doc = stNLP(k); term = ('_'.join([word.lemma for sent in doc.sentences for word in sent.words if word.pos in ['NOUN', 'ADJ', 'PROPN', 'PUNCT']]), v)\n",
    "        if '_' in term[0]:\n",
    "            resultado.append(term)\n",
    "    \n",
    "    #merge repetitive terms and counts :\n",
    "    resultado_pre = {x[0] for x in resultado}\n",
    "    resultado_post = [(i,sum(x[1] for x in resultado if x[0] == i)) for i in resultado_pre]\n",
    "    \n",
    "    #store:\n",
    "    df_test.at[index, 'alt2_list_terms'] = resultado_post\n",
    "    \n",
    "    del resultado; del resultado_pre; del resultado_post\n",
    "    print('done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for index, row in df_test.iterrows():\n",
    "#    resultado = []\n",
    "#    \n",
    "#    print('processing:', index)\n",
    "#    #compute desired pos and join terms:\n",
    "#    for k,v in df_test['alt2_list_terms_base'][index].items():   # 'ADJ' needed: 'telefonía satelital'\n",
    "#        doc = nlp_en(k); term = ('_'.join([word.lemma_ for word in doc]), v)\n",
    "#        if '_' in term[0]:\n",
    "#            resultado.append(term)\n",
    "#    \n",
    "#    #merge repetitive terms and counts :\n",
    "#    resultado_pre = {x[0] for x in resultado}\n",
    "#    resultado_post = [(i,sum(x[1] for x in resultado if x[0] == i)) for i in resultado_pre]\n",
    "#    \n",
    "#    #store:\n",
    "#    df_test.at[index, 'alt2_list_terms'] = resultado_post\n",
    "#    \n",
    "#    del resultado; del resultado_pre; del resultado_post\n",
    "#    print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. List of Terms: Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.alt2_list_terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#expand the list of tuples:\n",
    "for index, row in df_test.iterrows():\n",
    "    print('processing index:', index)\n",
    "    df_test.at[index, 'alt2_list_terms'] = [k for (k,v) in df_test.alt2_list_terms[index] for count in range(v)]\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous values: (36293, 4543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_result = df_test.alt2_list_terms.to_list()\n",
    "\n",
    "terms_grams = []\n",
    "for i in range(len(terms_result)):\n",
    "    for token in terms_result[i]:\n",
    "        terms_grams.append(token)\n",
    "        \n",
    "print((len((terms_grams)),len(set(terms_grams))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_grams = Counter(terms_grams)\n",
    "sort_orders_terms = sorted(terms_grams.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_orders_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove = []\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (sort_orders_terms[i][0].endswith('_iv') or sort_orders_terms[i][0].endswith('_ii') or sort_orders_terms[i][0].endswith('_us$') or sort_orders_terms[i][0].endswith('/') \\\n",
    "        or sort_orders_terms[i][0].endswith('.') or sort_orders_terms[i][0].endswith('_i') or sort_orders_terms[i][0].endswith('_iii') or sort_orders_terms[i][0].endswith('_”')\\\n",
    "        or sort_orders_terms[i][0].endswith('_a') or sort_orders_terms[i][0].startswith('“_') or sort_orders_terms[i][0].startswith('f._') or sort_orders_terms[i][0].startswith('a._') \\\n",
    "        or sort_orders_terms[i][0].startswith('b._') or sort_orders_terms[i][0].startswith('c._') or sort_orders_terms[i][0].startswith('d._') \\\n",
    "        or sort_orders_terms[i][0].startswith('e._') or sort_orders_terms[i][0].startswith('v._') or sort_orders_terms[i][0].startswith('i._') \\\n",
    "        or sort_orders_terms[i][0].startswith('g._') or sort_orders_terms[i][0].startswith('iv._') or sort_orders_terms[i][0].startswith('&_') \\\n",
    "        or sort_orders_terms[i][0].startswith('actividad/_') or sort_orders_terms[i][0].startswith('ct_') or sort_orders_terms[i][0].startswith('atn_/') \\\n",
    "        or sort_orders_terms[i][0].startswith('/_') or sort_orders_terms[i][0].startswith('ii.') or sort_orders_terms[i][0].startswith('iii_') or sort_orders_terms[i][0].startswith('iv_')\\\n",
    "        or sort_orders_terms[i][0].startswith('a_') or sort_orders_terms[i][0].endswith('_rev') or sort_orders_terms[i][0].startswith('x_') or sort_orders_terms[i][0].startswith('p_') \\\n",
    "        or sort_orders_terms[i][0].startswith('d_') or sort_orders_terms[i][0].startswith('enel_') or sort_orders_terms[i][0].endswith('_enel') or sort_orders_terms[i][0].endswith('_p') or sort_orders_terms[i][0].endswith('_d')\\\n",
    "        or sort_orders_terms[i][0].endswith('_figura') or sort_orders_terms[i][0].endswith('_sp') or sort_orders_terms[i][0].endswith('_cis') or sort_orders_terms[i][0].endswith('_csc') or sort_orders_terms[i][0].endswith('_cobit')\\\n",
    "        or sort_orders_terms[i][0].startswith('dela_') or sort_orders_terms[i][0].endswith('_dela') or sort_orders_terms[i][0].endswith('_nist') or sort_orders_terms[i][0].endswith('_cert') \\\n",
    "        or sort_orders_terms[i][0].endswith('_t') or sort_orders_terms[i][0].endswith('_m') or sort_orders_terms[i][0].startswith('m_') or sort_orders_terms[i][0].startswith('is_') or sort_orders_terms[i][0].startswith('for_')\\\n",
    "        or sort_orders_terms[i][0].startswith('and_') or sort_orders_terms[i][0].startswith('of_') or sort_orders_terms[i][0].startswith('or_') or sort_orders_terms[i][0].startswith('this_') or sort_orders_terms[i][0].startswith('does_')\\\n",
    "        or sort_orders_terms[i][0].startswith('are_') or sort_orders_terms[i][0].startswith('j_') or sort_orders_terms[i][0].startswith('c_') or sort_orders_terms[i][0].endswith('_is') \\\n",
    "        or sort_orders_terms[i][0].endswith('_be') or sort_orders_terms[i][0].endswith('_and') or sort_orders_terms[i][0].endswith('_are') or sort_orders_terms[i][0].endswith('_of')\\\n",
    "        or sort_orders_terms[i][0].endswith('_nº') or sort_orders_terms[i][0].endswith('_t(') or sort_orders_terms[i][0].startswith('sp_') or sort_orders_terms[i][0].endswith('_sp')\\\n",
    "        or sort_orders_terms[i][0].startswith('|_') or sort_orders_terms[i][0].endswith('_|') or sort_orders_terms[i][0].startswith('χ_') or sort_orders_terms[i][0].endswith('_χ') \\\n",
    "        or sort_orders_terms[i][0].startswith('_') or sort_orders_terms[i][0].endswith('') or sort_orders_terms[i][0].endswith('_kk') or sort_orders_terms[i][0].endswith('_kl') \\\n",
    "        or sort_orders_terms[i][0].endswith('_se') or sort_orders_terms[i][0].endswith('_ad') or sort_orders_terms[i][0].endswith('_w') or sort_orders_terms[i][0].endswith('_r') \\\n",
    "        or sort_orders_terms[i][0].endswith('_ar') or sort_orders_terms[i][0].endswith('_�') or sort_orders_terms[i][0].startswith('�_')\n",
    "       ):\n",
    "       \n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "# remove grmas containing digits:\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (num_there(sort_orders_terms[i][0]) and not ('covid' in sort_orders_terms[i][0] or '2700' in sort_orders_terms[i][0] \\\n",
    "                                                    or 'p2p' in sort_orders_terms[i][0] or '5g' in sort_orders_terms[i][0])):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('following' in sort_orders_terms[i][0]) or ('received' in sort_orders_terms[i][0]) or ('section' in sort_orders_terms[i][0]) \\\n",
    "    or ('option' in sort_orders_terms[i][0] and not 'adoption' in sort_orders_terms[i][0]) or ('this_component' in sort_orders_terms[i][0]) \\\n",
    "    or ('*' in sort_orders_terms[i][0]) or ('|' in sort_orders_terms[i][0]) or ('' in sort_orders_terms[i][0]) or ('�' in sort_orders_terms[i][0]) \\\n",
    "    or ('_x_' in sort_orders_terms[i][0]) or ('χ' in sort_orders_terms[i][0]) or ('PRON' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('paragraph' in sort_orders_terms[i][0]) or ('abbreviation' in sort_orders_terms[i][0]) or ('conclusion' in sort_orders_terms[i][0]) \\\n",
    "    or ('revision_of' in sort_orders_terms[i][0]) or ('▪' in sort_orders_terms[i][0]) or ('.' in sort_orders_terms[i][0]) or ('+' in sort_orders_terms[i][0]) or ('¶' in sort_orders_terms[i][0]) \\\n",
    "    or ('$' in sort_orders_terms[i][0]) or ('' in sort_orders_terms[i][0]) or ('=' in sort_orders_terms[i][0]) or ('/' in sort_orders_terms[i][0]) or ('*' in sort_orders_terms[i][0]) \\\n",
    "    or ('€' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_x_' in i[0] or i[0].endswith('_x') or i[0].startswith('x_')) or ('_d_' in i[0] or i[0].endswith('_d') or i[0].startswith('d_')) or \\\n",
    "        ('_m_' in i[0] or i[0].endswith('_m') or i[0].startswith('m_')) or (i[0].endswith('_c') or i[0].startswith('c_') or '_c_' in i[0]) or \\\n",
    "        (i[0].endswith('_e') or i[0].startswith('e_') or '_e_' in i[0]):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_l') or i[0].startswith('l_') or i[0].endswith('_del') or i[0].startswith('del_') or i[0].endswith('detallado') or i[0].startswith('detallado_') \\\n",
    "        or i[0].endswith('_n') or i[0].startswith('n_'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_ic') or i[0].startswith('ic_') or i[0].endswith('_r') or i[0].startswith('r_') or i[0].startswith('f_') or i[0].endswith('_v') or i[0].startswith('sa_') \\\n",
    "        or i[0].startswith('an_') or i[0].endswith('_an') or i[0].startswith('by_') or i[0].endswith('_by') or i[0].startswith('or_') or i[0].endswith('_or') \\\n",
    "        or i[0].startswith('can_') or i[0].endswith('_can') or i[0].startswith('to_') or i[0].endswith('_to') or i[0].startswith('cabo_') or i[0].endswith('_cabo') \\\n",
    "        or i[0].startswith('the_') or i[0].endswith('_the') or i[0].startswith('of_') or i[0].endswith('_of') or i[0].startswith('and_') or i[0].endswith('_and') \\\n",
    "        or '_and_' in i[0] or i[0].startswith('h_') or i[0].endswith('_b') or i[0].endswith('_nis') or i[0].endswith('_n') or i[0].startswith('one_') or i[0].endswith('_one') \\\n",
    "        or i[0].endswith('_af') or i[0].endswith('_aes') or i[0].endswith('_cc') or i[0].endswith('_sncti') or i[0].endswith('_cb') or i[0].endswith('_foppa'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'previous'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'et_al'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_x_' in i[0] or i[0].endswith('_x') or i[0].startswith('x_')) or ('_d_' in i[0] or i[0].endswith('_d') or i[0].startswith('d_')) or \\\n",
    "        ('_m_' in i[0] or i[0].endswith('_m') or i[0].startswith('m_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'j_k' in i[0] or 'f_g_' in i[0] or 'g_h' in i[0] or 'g_h' in i[0] or 't_ico' in i[0] or 'icac_ión' in i[0] or 'serv_ic_ios' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '>' in i[0] or '<' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'which' in i[0] or 'where' in i[0] or 'when' in i[0] or 'what' in i[0] or 'that' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_l') or i[0].startswith('l_') or i[0].endswith('_del') or i[0].startswith('del_') or i[0].endswith('detallado') or i[0].startswith('detallado_') \\\n",
    "        or i[0].endswith('_n') or i[0].startswith('n_'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_c') or i[0].startswith('c_') or '_c_' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '_to_the_'  in i[0] or '_for_the_'  in i[0] or '_in_the_'  in i[0] or 'appendix'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'actual' in i[0] and not 'contractual' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'relate' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '.' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_ic') or i[0].startswith('ic_') or i[0].endswith('_r') or i[0].startswith('r_') or i[0].startswith('f_') or i[0].endswith('_v') \\\n",
    "        or i[0].startswith('an_') or i[0].endswith('_an') or i[0].startswith('by_') or i[0].endswith('_by') or i[0].startswith('or_') or i[0].endswith('_or') \\\n",
    "        or i[0].startswith('can_') or i[0].endswith('_can') or i[0].startswith('to_') or i[0].endswith('_to') or i[0].startswith('cabo_') or i[0].endswith('_cabo') \\\n",
    "        or i[0].startswith('the_') or i[0].endswith('_the') or i[0].startswith('of_') or i[0].endswith('_of') or i[0].startswith('and_') or i[0].endswith('_and') \\\n",
    "        or '_and_' in i[0] or i[0].startswith('h_') or i[0].endswith('_b') or i[0].endswith('_nis') or i[0].endswith('_n') or i[0].startswith('one_') or i[0].endswith('_one'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove = terms_to_remove + final_stop_words \n",
    "terms_to_remove = list(set(terms_to_remove))\n",
    "len(terms_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(terms_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '→' in i[0] or 'ˇ' in i[0] or 'μ' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_terms = ['-PRON-', '_', 'aadt', 'aaf', 'aaps', 'aastaraamatu', 'ababa', 'abac', 'abbreviation', 'abc', 'abd', \\\n",
    "                 'abovementioned', 'aforementioned', 'abraham', 'abrams', 'abs', 'zzz', 'ºc', 'õppetunnid', 'ˇthi', 'μs', '_recommendation', \\\n",
    "                 '→_insufficient', '→_insufficient', '→_weak', '→_weak', 'resolution_→', 'resolution_→', '→_weak_strategic_integral_management', '→_weak_strategic_integral_management', '→_lack', '→_lack', 'sector_→', 'sector_→', 'sector_→', 'private_sector_→', 'private_sector_→', 'private_sector_→', '→_weak_strategic_integral', '→_weak_strategic_integral', '→_lack_of_specialist', '→_lack_of_specialist', '→_weak_strategic', '→_weak_strategic', \\\n",
    "                 'framework_©_customer', 'framework_©_customer', 'framework_©', 'framework_©', 'framework_©', 'digital_transformation_framework_©_customer_insight', 'digital_transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'framework_©_customer_insight', 'framework_©_customer_insight', 'use_digital_transformation_framework_©', 'use_digital_transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', '©_customer_insight', '©_customer_insight', 'digital_transformation_framework_©_customer', 'digital_transformation_framework_©_customer', 'transformation_framework_©_customer_insight_customer', 'transformation_framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'transformation_framework_©_customer', 'transformation_framework_©_customer', '©_customer', '©_customer', '©_customer_insight_customer', '©_customer_insight_customer', 'use_digital_transformation_framework_©_customer', 'use_digital_transformation_framework_©_customer', \\\n",
    "                 'μs', 'offset_of_μs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Remove selected terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['alt2_terms'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_test.at[index, 'alt2_terms'] = [word for word in df_test['alt2_list_terms'][index] if word not in terms_to_remove and '_' in word]\n",
    "    \n",
    "    # replace \"datum\":\n",
    "    df_test.at[index, 'alt2_terms'] = [word if 'datum' not in word else word.replace('datum', 'data') for word in df_test['alt2_terms'][index]]\n",
    "    \n",
    "    #print([word for word in df_base['list_of_terms'][index] if word not in terms_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final = df_test.alt2_terms.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final_flat = []\n",
    "for i in range(len(terms_final)):\n",
    "    for token in terms_final[i]:\n",
    "        terms_final_flat.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_final_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "terms_final_flat = Counter(terms_final_flat)\n",
    "sort_orders_terms_final = sorted(terms_final_flat.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms_final:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NLP Token extraction and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Sentences and Clean\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=False) #modificado\n",
    "        yield(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc), deacc=False) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#\n",
    "def lemmatization(texts, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = stNLP(\" \".join(sent)) \n",
    "        texts_out.append([word.lemma.lower() for sent in doc.sentences for word in sent.words if word.pos in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    #texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]   \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df_test['extracted_cleaned'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "pprint(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words, final_stop_words)\n",
    "\n",
    "# Data Lemmatized\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['PROPN', 'NOUN', 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats_only_tokens = []\n",
    "for i in range(len(data_lemmatized)):\n",
    "    for token in data_lemmatized[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats_only_tokens.append(token)\n",
    "len(set(word_stats_only_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stop_words = final_stop_words + ['iec', 'iso', 'nist', 'pr', 'sr', 'apo', 'isa', 'sp', 'cobit', 'rev', 'the', 'dss', 'and', 'of', 'csc', 'cis', 'or', 'vez', 'itu', 'usuarioen', 'sc', 'bai', 'ca', 'ref', 'año', 'co', 'sin', 'embargo', 'an', 'ac', 'respecto', 'pm', 'cabo', 'banco', 'sa', 'inglés', 'cp', 'ic', 're', 'subsector', 'subcategoría', 'rsi', 'tanto', 'ia', 'sigla', 'eldesarrollo', 'sd', 'cm', 'pi', 'pe', 'rc', 'is', 'ens', 'mc', 'gi', 'for', 'cuanto', 'capabilitie', 'so', 'día', 'by', 'that', 'ccn', 'ra', 'gr', 'ver', 'type', 'ga', 'csi', 'oea', 'es', 'ps', 'caribe', 'bid', 'are', 'au', 'ae', 'on', 'note', 'as', 'mes', 'cf', 'be', 'tal', 'decir', 'sf', 'one', 'acrónimo', 'pd', 'taller', 'españa', 'which', 'ad', 'cs', 'ma', 'agencia', 'etapa', 'bahama', 'go', 'mea', 'er', 'tp', 'ión', 'ción', 'solo', 'pl', 'mp', 'diciembre', 'guyana', 'costa', 'chile', 'último', 'panamá', 'with', 'physical', 'can', 'gh', 'falta', 'pesar', 'europa', 'not', 'octubre', 'junio', 'préstamo', 'barbado', 'brasil', 'república', 'acreditación', 'from', 'category', 'plazo', 'hoc', 'program', 'abril', 'enero', 'san', 'edm', 'bia', 'enisa', 'semana', 'ddo', 'inc', 'mano', 'pregunta', 'eps', 'gracias', 'trinidad', 'simposio', 'salvador', 'espíritu', 'santo', 'another', 'more', 'at', 'reunión', 'conferencia', 'fecha', 'idente', 'obstante', 'idad', 'marzo', 'scrm', 'dado', 'ministro', 'based', 'budapest', 'saint', 'nevis', 'provided', 'using', 'this', 'such', 'herrmann', 'uruguay', 'site', 'segur', 'it', 'ion', 'aa', 'noviembre', 'argentino', 'belice', 'haití', 'paraguay', 'rd', 'kitt', 'granadina', 'pgp', 'used', 'any', 'where', 'least', 'related', 'institute', 'uy', 'rp', 'iberseguridad', 'corea', 'ico', 'nes', 'también', 'torno', 'lidad', 'def', 'coord', 'ibersegur', 'lar', 'ds', 'mancomunidad', 'mayo', 'barbuda', 'ii', 'septiembre', 'julio', 'ja', 'perú', 'venezuela', 'par', 'pro', 'crl', 'day', 'owner', 'cnss', 'include', 'applications', 'object', 'other', 'may', 'but', 'their', 'have', 'between', 'order', 'need', 'mua', 'ieee', 'cip', 'latinoamérica', 'ge', 'safe', 'reino', 'ue', 'cr', 'recuperac', 'iber', 'bg', 'febrero', 'policy', 'belize', 'colombia', 'iii', 'granada', 'gy', 'mx', 'santo', 'cesión', 'em', 'presa', 'mente', 'coa', 'dad', 'nado', 'assigned', 'responsibility', 'official', 'inst', 'ability', 'example', 'defined', 'some', 'set', 'party', 'relationship', 'does', 'might', 'support', 'operation', 'customers', 'drae', 'needed', 'run', 'capabilities', 'performance', 'target', 'provide', 'also', 'measure', 'organization', 'cnssi_', 'privilege', 'its', 'know', 'modification', 'default', 'most', 'eco', 'subdivisión', 'csf', 'american', 'ansi', 'abm', 'abreviatura', 'setiembre', 'art', 'north', 'corporation', 'good', 'appropriate', 'measur', 'rs', 'man', 'cci', 'ucrania', 'trend', 'root', 'corp', 'sur', 'situ', 'acls', 'tación', 'tructura', \\\n",
    "                                      'ís', 'responsabi', 'icac', 'ica', 'cana', 'iva', 'lataforma', 'igac', 'fac', 'iesgo', 'serv', 'ios', 'desarro', 'tecno', 'us', 'kiviat', 'fa', 'cea', 'ee', 'cps', 'usc', 'cmm', 'loan', 'pbl', 'ba', 'israel', 'indio', 'ran', 'caminoa', 'mintic', 'india', 'ecuador', 'nº', 'gob', 'guatemala', 'get', 'guyanés', 'francés', 'conatel', 'hondura', 'méxico', 'mitic', 'agosto', 'dpsm', 'suriname', 'tt', 'cita', 'ne', 'lopd', 'tre', 'trato', 'favor', 'español', 'res', 'pue', 'cabida', 'sis', 'mas', 'ans', 'coe', 'otan', 'iv', 'necessarily', 'under', 'put', 'behalf', 'out', 'actions', 'either', 'both', 'overall', 'them', 'dsaas', 'when', 'certain', 'conditions', 'tests', 'instance', 'mentira', 'generally', 'applied', 'sec', 'against', 'objetivo', 'objective', 'levels', 'ss', 'antel', 'ceibal', 'ínsita', 'emg', 'agesic', 'firewa', 'll', 'bidbanco', 'oeaorganización', 'guidela', 'gridsla', 'iisf', 'consortium', 'iic', 'iiot', 'crf', 'unece', 'united', 'nations', 'economic', 'commission', 'deming', 'act', 'erm', 'purduemodelo', 'purdue', 'dnp', 'opc', 'ua', 'gridred', 'dcssistemas', 'rtuunidad', 'ataqueconjunto', 'ataquemétodo', 'idssistema', 'ais', 'central', 'siemsistema', 'soccentro', 'cisodirector', 'middle', 'alc', 'industroyer', 'turquía', 'farewell', 'bellingham', 'australia', 'david', 'salt', 'sobig', 'csx', 'sasser', 'british', 'airway', 'delta', 'tehama', 'colusa', 'dugu', 'gauss', 'dragon', 'exxon', 'shell', 'bp', 'hemisferio', 'oriente', 'asia', 'zotob', 'daimler', 'chrisler', 'ag', 'kwc', 'onion', 'city', 'probalidad', 'rtic', 'ipa', 'nte', 'respuestatoda', 'instar', 'lación', 'proba', 'segmen', 'infra', 'infraestructu', 'op', 'cor', 'pico', 'xx', 'hart', 'eng', 'regis', 'tra', 'end', 'point', 'respuestano', 'cisne', 'cercanía', 'transmisiónuna', 'despositivo', 'adición', 'tan', 'pt', 'edifico', 'restrición', 'un sitio', 'aplicacio', 'billón', 'usd', 'proteccióna', 'naciona', 'igar', 'lega', 'lmente', 'lu', 'jo', 'lat', 'hac', 'abr', 'preparac', 'automát', 'env', 'laborat', 'irtua', 'invest', 'izac', 'mit', 'contro', 'ive', 'orquestac', 'recuperaciónc', 'inac', 'especia', 'ías', 'inado', 'mater', 'imulac', 'inada', 'detecc', 'acc', 'restaurac', 'compet', 'habi', 'desarrol', 'istema', 'prote', 'jan', 'conf', 'idenc', 'ing', 'exce', 'lenc', 'profe', 'doe', 'dhs', 'mil', 'ef', 'ej', 'sumin', 'iclo', 'ponibilidad', 'vulnerabilida', 'dcpd', 'hr', 'dólar', 'rmp', 'fu', 'nc', 'ar', 'ross', 'et', 'quehacer', 'oxford', 'organizacionesy', 'cumbre', 'siglo', 'xxi', 'chino', 'icic', 'pwc', 's', 'puc', 'bolivia', 'cgii', 'br', 'numeral', 'ente', 'colcert', 'colombiano', 'delegatura', 'europol', 'micitt', 'desde', 'costarricense', 'dominiqués', 'dominica', 'arcotel', 'ecucert', 'deloitte', 'sv', 'encargada', 'grenada', 'luz', 'gt', 'getsafeonline', 'summer', 'incibe', 'jamaica', 'mstem', 'sarrollo', 'mexicano', \\\n",
    "                                       'citi', 'py', 'pcm', 'peruano', 'kittsand', 'dpm', 'ttconnect', 'gub', 'suscerte', 'sede', 'sucer', 'venezolano', 'cos', 'ceptible', 'uti', 'dida', 'grama', 'có', 'digo', 'infor', 'mación', 'cas', 'ejem', 'plo', 'jun', 'profesiona', 'poste', 'cer', 'na', 'moneda', 'lssi', 'edi', 'electronic', 'actuali', 'congéner', 'riante', 'comunicacio', 'ghz', 'trozo', 'tico', 'gan', 'cho', 'abo', 'gado', 'di', 'nero', 'kilómetro', 'sue', 'longi', 'interesar', 'conocimien', 'ci', 'frado', 'intimi', 'activi', 'traseña', 'car', 'pio', 'embar', 'ciu', 'mos', 'progra', 'cues', 'tión', 'db', 'utili', 'minio', 'ñan', 'd', 'perso', 'transferen', 'cia', 'gar', 'glés', 'miento', 'apli', 'infrastructu', 'grafía', 'transaccio', 'cla', 'za', 'sqldefinición', 'vali', 'dación', 'intru', 'tec', 'nología', 'cortafue', 'landefinición', 'elenco', 'admi', 'nistración', 'emisor', 'seguri', 'indi', 'tarje', 'ssc', 'nera', 'dose', 'apa', 'renta', 'cumento', 'dar', 'siste', 'gocio', 'tele', 'termediario', 'tario', 'mini', 'proce', 'dente', 'servi', 'cio', 'ri', 'vest', 'shamir', 'adleman', 'ase', 'mencio', 'usua', 'rio', 'ciberdelincuen', 'dress', 'fre', 'cuencia', 'ssldefinición', 'ingle', 'transport', 'fun', 'ciona', 'propagar', 'direc', 'soft', 'ware', 'men', 'ministrador', 'orde', 'tocol', 'tar', 'zero', 'ciberdelin', 'ter', 'sci', 'subsanación', 'configuracio', 'ied', 'atlántico', 'norte', 'igf', 'foc', 'ccd', 'ass', 'om', 'mcafee', 'sl', 'isdefe', 'loud', 'offered', 'invoked', 'possess', 'qualities', 'purpose', 'imply', 'reason', 'was', 'resulted', 'exercising', 'published', 'contains', 'operate', 'made', 'obtained', 'publicly', 'however', 'produced', 'likely', 'following', 'principles', 'unless', 'there', 'provisions', 'contrary', 'activitie', 'mak', 'includ', 'uses', 'entitie', 'deviz', 'csv', 'who', 'compaa', 'than', 'small', 'ón', 'own', 'sport', 'golf', 'easily', 'without', 'required', 'reenter', 'ease', 'moving', 'essence', 'here', 'achieved', 'exactly', 'format', 'accepted', 'even', 'if', 'formats', 'simple', 'straightforward', 'achieve', 'commonly', 'rekeying', 'could', 'described', 'easy', 'three', 'representations', 'manner', 'suitable', 'guide', 'clic', 'cabalgamiento', 'two', 'different', 'instrument', 'engind', 'compound', 'therefore', 'useful', 'whose', 'existence', 'strongly', 'various', 'together', 'iaa', 'underlying', 'over', 'deployed', 'components', 'host firewalls', 'eufemismo', 'ligthweight', 'numbers', 'symbols', 'entities', 'describir', 'according', 'clear', 'rules', 'repeated', 'measuring', 'comparison', 'reference', 'measurements', 'they', 'orient', 'decisions', 'better', 'understanding', 'casual', 'relationships', 'intended', 'expectations', 'observed', 'facts', 'number', 'symbol', 'characterize', 'attribute', 'compensating', 'technical', 'safeguard', 'countermeasure', 'employed', 'lieu', 'recommended', \\\n",
    "                                       'low', 'moderate', 'high', 'baselín', 'provid', 'equivalent', 'decimal', 'metric', 'proposed', 'facilitate', 'decisio', 'n', 'making', 'improve', 'through', 'reporting', 'relevant', 'ribagorda', 'principle', 'should', 'each', 'granted', 'minimum', 'needs', 'multi', 'computations', 'terms', 'express', 'same', 'while', 'processe', 'feature', 'automatically', 'sesgo', 'picaresca', 'paas', 'supported', 'single', 'ce', 'potentially', 'harbor', 'serve', 'statutory', 'staff', 'directs', 'today', 'responsible', 'among', 'organizational', 'elements', 'involved', 'derived', 'after', 'agreed', 'period', 'ver_pag', 'whether', 'int', 'many', 'lead', 'what', 'actually', 'result', 'the', 'precise', 'meaning', 'remain', 'identifies', 'established', 'omb', 'appendix', 'spp', 'saa', 'fip', 'troya', 'erc', 'ksk', 'priori', 'cc', 'lucia', 'subcategorie', 'subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(final_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_final_flat = []\n",
    "for i in range(len(data_lemmatized)):\n",
    "    for token in data_lemmatized[i]:\n",
    "        tokens_final_flat.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_final_flat = Counter(tokens_final_flat)\n",
    "sort_orders_tokens_final = sorted(tokens_final_flat.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_tokens_final:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out tokens\n",
    "df_test['alt2_tokens'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_test.at[index, 'alt2_tokens'] = [word for word in data_lemmatized[index] if word not in final_stop_words]\n",
    "    \n",
    "    # replace \"datum\":\n",
    "    df_test.at[index, 'alt2_tokens'] = [word if 'datum' not in word else word.replace('datum', 'data') for word in df_test['alt2_tokens'][index]]\n",
    "    \n",
    "    #print([word for word in df_base['list_of_terms'][index] if word not in terms_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['alt2_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokens - final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "tokens_final = df_test['alt2_tokens'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_final_flat = []\n",
    "for i in range(len(tokens_final)):\n",
    "    for token in tokens_final[i]:\n",
    "        tokens_final_flat.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokens_final_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_final_flat = Counter(tokens_final_flat)\n",
    "sort_orders_tokens_final = sorted(tokens_final_flat.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_tokens_final:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge tokens and terms/n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously obtained terms/n-grams are added to the dataset\n",
    "data_lemmatized_full = []\n",
    "for index, row in df_test.iterrows():\n",
    "    data_lemmatized_full.append(df_test['alt2_tokens'][index] + df_test['alt2_terms'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lemmatized_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats = []\n",
    "for i in range(len(data_lemmatized_full)):\n",
    "    for token in data_lemmatized_full[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = Counter(word_stats)\n",
    "sort_orders = sorted(word_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data_lemmatized as a new column and store the results:\n",
    "df_test['alt2_data_lemmatized'] = data_lemmatized_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: final clean-up since some additional tokens and terms were flagged to be removed when doing the clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_terms = ['-PRON-', '_', 'aadt', 'aaf', 'aaps', 'aastaraamatu', 'ababa', 'abac', 'abbreviation', 'abc', 'abd', \\\n",
    "                 'abovementioned', 'aforementioned', 'abraham', 'abrams', 'abs', 'zzz', 'ºc', 'õppetunnid', 'ˇthi', 'μs', '_recommendation', \\\n",
    "                 '→_insufficient', '→_insufficient', '→_weak', '→_weak', 'resolution_→', 'resolution_→', '→_weak_strategic_integral_management', '→_weak_strategic_integral_management', '→_lack', '→_lack', 'sector_→', 'sector_→', 'sector_→', 'private_sector_→', 'private_sector_→', 'private_sector_→', '→_weak_strategic_integral', '→_weak_strategic_integral', '→_lack_of_specialist', '→_lack_of_specialist', '→_weak_strategic', '→_weak_strategic', \\\n",
    "                 'framework_©_customer', 'framework_©_customer', 'framework_©', 'framework_©', 'framework_©', 'digital_transformation_framework_©_customer_insight', 'digital_transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'framework_©_customer_insight', 'framework_©_customer_insight', 'use_digital_transformation_framework_©', 'use_digital_transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', '©_customer_insight', '©_customer_insight', 'digital_transformation_framework_©_customer', 'digital_transformation_framework_©_customer', 'transformation_framework_©_customer_insight_customer', 'transformation_framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'transformation_framework_©_customer', 'transformation_framework_©_customer', '©_customer', '©_customer', '©_customer_insight_customer', '©_customer_insight_customer', 'use_digital_transformation_framework_©_customer', 'use_digital_transformation_framework_©_customer', \\\n",
    "                 'μs', 'offset_of_μs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_test.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    #df_test.at[index, 'alt2_tokens'] = [word for word in data_lemmatized[index] if word not in final_stop_words]\n",
    "    \n",
    "    # replace \"datum\":\n",
    "    print(index, [word for word in df_test['alt2_data_lemmatized'][index] if word in flagged_terms])\n",
    "    #print(index, [word for word in df_test['alt2_data_lemmatized'][index] if word == '_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.2: Store df_test containing terms\n",
    "f_df_test = 'nlp_spec_docs_2021-02-08_english_v12_final.joblib'\n",
    "joblib.dump(df_test[['Short_Name', 'extracted_cleaned', 'alt2_terms', 'alt2_tokens', 'alt2_data_lemmatized']], './output/' + f_df_test + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.2: Store df_test full\n",
    "f_df_test_full = 'nlp_spec_docs_2021-02-08_english_v12_FULL.joblib'\n",
    "joblib.dump(df_test, './output/' + f_df_test_full + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** \n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.0: Store df_test containing terms\n",
    "f_df_test = 'nlp_spec_docs_2020-12-09_english_v1_final.joblib'\n",
    "joblib.dump(df_test[['Short_Name', 'extracted_cleaned', 'alt2_terms', 'alt2_tokens', 'alt2_data_lemmatized']], './output/' + f_df_test + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.0: Store data_lemmatized_full (English)\n",
    "with open('./output/data_lemmat_spec_docs_2020-12-09_english_v1_final.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.0: Store df_test full - including annotation done by Spacy\n",
    "f_df_test = 'nlp_spec_docs_annotated_2020-12-09_english_v1_final.joblib'\n",
    "joblib.dump(df_test, './output/' + f_df_test + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                     Change:                                       \n",
    "\n",
    "\n",
    "\n",
    "#   - 1.0           12/08/2020        Emiliano Colina        - Forked from Spanish specialized documents processed\n",
    "#                                                            with Stanza\n",
    "                                                                                                                  \n",
    "#   - 0.1           10/13/2020        Emiliano Colina        - Initial version\n",
    "#                                                            - All type of documents included.\n",
    "\n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
