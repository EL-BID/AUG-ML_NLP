{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 03.0 - NLP Processing (loans and TCs)\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): pdf files\n",
    "#-- Description:  \n",
    "#                - Merge all dataframes (Loans and TCs)\n",
    "#                - Perform NLP tasks\n",
    "#                - Spanish and English documents are processed separately and filtered based on external list\n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @authors:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  1.3\n",
    "#-- Last Update: 01/15/2020\n",
    "#-- Last Revision Date: 10/19/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the es_core_news_lg:\n",
    "#!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- START v1.3 ---------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TCs and Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TCs - v1.3\n",
    "df_tcs = joblib.load('./output/df_resultado_tcs_2021-01-14_v22.joblib.bz2')\n",
    "df_tcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loans - v1.3\n",
    "df_loans = joblib.load('./output/df_resultado_loans_2021-01-14_v07.joblib.bz2')\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = pd.concat([df_tcs, df_loans])\n",
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "data_base = data_base.rename(columns = {'extracted_cleaned_v2':'extracted'})\n",
    "print('* Number of operations:', data_base.shape)\n",
    "print('* Columns:', data_base.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_base.reset_index(inplace=True)\n",
    "data_base.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specific columns \n",
    "data_base.drop(['Document_Content', 'title_inicial', 'title_final', 'lista_paginas', 'extracted_v2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.3\n",
    "# remove trailing spaces:\n",
    "data_base['OPERATION_NUMBER'] = data_base['OPERATION_NUMBER'].str.strip()\n",
    "data_base['FK_OPERATION_ID'] = data_base['FK_OPERATION_ID'].apply(str).str.strip()\n",
    "data_base['DOCUMENT_ID'] = data_base['DOCUMENT_ID'].apply(str).str.strip()\n",
    "data_base['DOCUMENT_REFERENCE'] = data_base['DOCUMENT_REFERENCE'].apply(str).str.strip()\n",
    "data_base['DESCRIPTION'] = data_base['DESCRIPTION'].apply(str).str.strip()\n",
    "\n",
    "# set everything to str\n",
    "data_base['extracted'] = data_base['extracted'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Operations with Document - v1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operations' filters\n",
    "df_filters = pd.read_excel('./input/Lista de operaciones y documentos 2017-2020.xlsx')\n",
    "df_filters['FK_OPERATION_ID'] = df_filters['FK_OPERATION_ID'].apply(str).str.strip()\n",
    "df_filters['DOCUMENT_ID'] = df_filters['DOCUMENT_ID'].apply(str).str.strip()\n",
    "df_filters['DOCUMENT_REFERENCE'] = df_filters['DOCUMENT_REFERENCE'].apply(str).str.strip()\n",
    "df_filters['DESCRIPTION'] = df_filters['DESCRIPTION'].apply(str).str.strip()\n",
    "df_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by selected operations:\n",
    "df1 = df_filters.merge(data_base, on=['FK_OPERATION_ID', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Documents - v1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df1[df1['language'] == 'es']\n",
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_base.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates:\n",
    "df_base[df_base.duplicated(subset=['OPERATION_NUMBER'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base[df_base.OPERATION_NUMBER == 'UR-L1140'])\n",
    "print(df_base[df_base.OPERATION_NUMBER == 'UR-L1156'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.drop([369], inplace=True)\n",
    "df_base.drop([1273], inplace=True)\n",
    "df_base.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_base.shape)\n",
    "df_base.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish: Text Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Textacy processing - Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "# Load Spacy Spanish model in Textacy:\n",
    "es = textacy.load_spacy_lang('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textacy processing on extracted text: \n",
    "df_base['textacy_processing'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_base.at[index, 'textacy_processing'] = textacy.make_spacy_doc(df_base.extracted[index], lang=es)\n",
    "    \n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1.3: save results and continue processing in notebook “Digital Transformation - 03.1 - NLP Processing Spanish (Loans and TCs - Stanza) (workpaper)”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# v1.3: Store df_base, in Spanish, containing terms\n",
    "f_df_base_es2 = 'nlp_2021-01-15_spacy_annotated_spanish.joblib'\n",
    "joblib.dump(df_base, './output/' + f_df_base_es2 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- END v1.3 Spanish ----------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge the Documents Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TCs and Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCs\n",
    "df_tcs = joblib.load('./output/df_resultado_tcs_2020-10-17_v21.joblib.bz2')\n",
    "df_tcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loans\n",
    "df_loans = joblib.load('./output/df_resultado_loans_2020-11-04_v06.joblib.bz2')\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = pd.concat([df_tcs, df_loans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "data_base = data_base.rename(columns = {'extracted_cleaned_v2':'extracted'})\n",
    "print('* Number of operations:', data_base.shape)\n",
    "print('* Columns:', data_base.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_base.reset_index(inplace=True)\n",
    "data_base.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specific columns \n",
    "data_base.drop(['Document_Content', 'title_inicial', 'title_final', 'lista_paginas', 'extracted_v2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing (intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.3\n",
    "# store merged TCs and Loans documents\n",
    "f_df_data_base = 'merged_tcs_and_loans_2021-01-15_vfinal.joblib'\n",
    "joblib.dump(data_base, './output/' + f_df_data_base + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.3\n",
    "data_base.to_excel('merged_tcs_and_loans_2021-01-15_vfinal.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.1\n",
    "# store merged TCs and Loans documents\n",
    "f_df_data_base = 'merged_tcs_and_loans_2020-11-09_vfinal.joblib'\n",
    "joblib.dump(data_base, './output/' + f_df_data_base + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.1\n",
    "data_base.to_excel('merged_tcs_and_loans_2020-11-09_vfinal.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store merged TCs and Loans documents\n",
    "f_df_data_base = 'merged_tcs_and_loans_2020-10-19_vfinal.joblib'\n",
    "joblib.dump(data_base, './output/' + f_df_data_base + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base.to_excel('merged_tcs_and_loans_2020-10-19_vfinal.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load whole document collection (Loans + TCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = joblib.load('./output/merged_tcs_and_loans_2020-11-09_vfinal.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove trailing spaces:\n",
    "data_base['OPERATION_NUMBER'] = data_base['OPERATION_NUMBER'].str.strip()\n",
    "\n",
    "# set everything to str\n",
    "data_base['extracted'] = data_base['extracted'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Language Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operations' filters: SPANISH documents\n",
    "df_filters_1 = pd.read_excel('./input/Lista de Operaciones con Documento Encontrado-ES-EN.xlsx', sheet_name='ES')\n",
    "df_filters_1['OPERATION_NUMBER'] = df_filters_1['OPERATION_NUMBER'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operations' filters: latest documents\n",
    "df_filters_2 = pd.read_excel('./input/Data-30 Sep 2020.xlsx', sheet_name='data_filtered')\n",
    "df_filters_2['OPERATION_NUMBER'] = df_filters_2['OPERATION_NUMBER'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operations' filters: latest documents\n",
    "df_filters_3 = pd.read_excel('./input/Data-01 Nov 2020.xlsx', sheet_name='data_filtered')\n",
    "df_filters_3['OPERATION_NUMBER'] = df_filters_3['OPERATION_NUMBER'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filters = pd.concat([df_filters_1[['OPERATION_NUMBER']], df_filters_2[['OPERATION_NUMBER']], \\\n",
    "                        df_filters_3[['OPERATION_NUMBER']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by selected operations:\n",
    "df_base = data_base[data_base['OPERATION_NUMBER'].isin(df_filters['OPERATION_NUMBER'])]\n",
    "# select the Spanish documents:\n",
    "df_base = df_base[df_base['language'] == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df_base[df_base.duplicated(subset=['OPERATION_NUMBER'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base[df_base.OPERATION_NUMBER == 'UR-L1140'])\n",
    "print(df_base[df_base.OPERATION_NUMBER == 'UR-L1156'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.drop([1393], inplace=True)\n",
    "df_base.drop([1828], inplace=True)\n",
    "df_base.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish: NLP n-Gram Analysis - using Textacy bag-of-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, es_core_news_lg\n",
    "nlp_es = spacy.load('es_core_news_lg', disable=['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "# Spacy stop_words\n",
    "stop_words.extend(nlp_es.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stop_words:\n",
    "stop_words.extend(['ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'ill', 'descripción', 'componente', 'objetivo', 'ct', 'mailto', 'país', 'millón', 'millones', \\\n",
    "                   'año', 'años', 'dólar', 'dolar', 'dólares', 'si', 'bid', 'us', 'oc', 'gn', 'tc', 'atn', 'opc', 'pib', 'ar', 'br', 'uy', 'cl', 'co', \\\n",
    "                   'cclip', 'pbl', 'uis', 'ab', 'org', 'pr', 'bo', 'bl', 'pe', 'ec', 'ja', 'mx', 'ca', 'gu', 'su', 'ho', 'hn', 'mr', 'rg', 'ee', 'uu', \\\n",
    "                   'cr', 'tdr', 'rn', 'nº', 'usd', 'gy', 've', 'et', 'the', 'for', 'to', 'grt', 'fm', 'pr', 'pa', 'ni', 'aa', 'es', 'sp', \\\n",
    "                   'inglés', 'cty', 'nv', 'profisco', 'asimismo', 'actual', 'costo', 'resultar', 'esperar', 'ejecutar', 'unidad', 'agencia', 'justificación', \\\n",
    "                   'véase', 'ct', 'dela', 'enel', 'sobrar', 'of'])\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New version 11/09\n",
    "#stop_words_en = stopwords.words('english')\n",
    "#stop_words_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Textacy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "# Load Spacy Spanish model in Textacy:\n",
    "es = textacy.load_spacy_lang('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textacy processing on extracted text: \n",
    "df_base['textacy_processing'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_base.at[index, 'textacy_processing'] = textacy.make_spacy_doc(df_base.extracted[index].lower(), lang=es)\n",
    "    \n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. List of Terms (Bag-of-Terms): n-Grams extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. List of Terms Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['list_of_terms'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativa 2 (11/11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.drop(['list_of_terms_alt2'], axis=1, inplace=True)\n",
    "df_base['alt2_list_terms_base'] = ''\n",
    "df_base['alt2_list_terms'] = ''\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    print('processing:', index)\n",
    "    #generate terms (returns a generator):\n",
    "    terms_list = df_base['textacy_processing'][index]._.to_terms_list(ngrams=(2, 3, 4, 5, 6), entities=False, normalize=\"lower\", weighting=\"count\", as_strings=True, filter_stops=True, filter_punct=True, filter_nums=True, include_pos=['PROPN', 'NOUN', 'ADJ', 'ADP', 'DET'], min_freq=2)\n",
    "\n",
    "    #convert to list:\n",
    "    terms_list = list(terms_list)\n",
    "\n",
    "    #create dictio {term, count}:\n",
    "    resultado_pre = Counter([item.lower() for item in terms_list])\n",
    "    \n",
    "    #store result:\n",
    "    df_base.at[index, 'alt2_list_terms_base'] = resultado_pre\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    resultado = []\n",
    "    \n",
    "    print('processing:', index)\n",
    "    #compute desired pos and join terms:\n",
    "    for k,v in df_base['alt2_list_terms_base'][index].items():\n",
    "        doc = stNLP(k); term = ('_'.join([word.lemma for sent in doc.sentences for word in sent.words if word.pos in ['NOUN', 'ADJ', 'PROPN', 'PUNCT']]), v)\n",
    "        if '_' in term[0]:\n",
    "            resultado.append(term)\n",
    "    \n",
    "    #merge repetitive terms and counts :\n",
    "    resultado_pre = {x[0] for x in resultado}\n",
    "    resultado_post = [(i,sum(x[1] for x in resultado if x[0] == i)) for i in resultado_pre]\n",
    "    \n",
    "    #store:\n",
    "    df_base.at[index, 'alt2_list_terms'] = resultado_post\n",
    "    \n",
    "    del resultado; del resultado_pre; del resultado_post\n",
    "    print('done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# v1.2 - intermediate\n",
    "# store intermediate processed TCs and Loans documents\n",
    "f_df_data_base = 'merged_tcs_and_loans_2020-11-12_intermed.joblib'\n",
    "joblib.dump(df_base, './output/' + f_df_data_base + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[df_base.OPERATION_NUMBER == 'RG-T3352']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['alt2_list_terms_base'][743]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['alt2_list_terms'][743]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 550\n",
    "#convert to list:\n",
    "terms_list = list(df_base['list_of_terms'][index])\n",
    "\n",
    "#create dictio {term, count}:\n",
    "resultado_pre_1 = Counter([item.lower() for item in terms_list])\n",
    "sorted(resultado_pre_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['alt2_list_terms_base'][550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end alternativa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    \n",
    "    #generate terms:\n",
    "    terms_list = df_base['textacy_processing'][index]._.to_terms_list(ngrams=(2, 3, 4, 5, 6), entities=False, normalize=\"lemma\", weighting=\"count\", as_strings=True, filter_stops=True, filter_punct=True, filter_nums=True, include_pos=['PROPN', 'NOUN', 'ADJ', 'ADP'], min_freq=2)\n",
    "    \n",
    "    #replace blanks with '_':\n",
    "    resultado_pre = Counter([(item.lower()).replace(' ', '_') for item in terms_list])\n",
    "    \n",
    "    #select terms that appear 2 or more times, convert to list including count and store:\n",
    "    df_base.at[index, 'list_of_terms'] = [k for (k,v) in resultado_pre.items() for count in range(v) if v > 1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# remove stop_words from terms\n",
    "for index, row in df_base.iterrows():\n",
    "    lista_test_1 = df_base['list_of_terms'][index]\n",
    "    resultado = []\n",
    "    for item in lista_test_1:\n",
    "        #print(item.split('_'))\n",
    "        subitem = item.split('_')\n",
    "        resultado.append('_'.join([word for word in subitem if word not in stop_words]))\n",
    "\n",
    "    lista_test_1 = [word for word in resultado if '_' in word]        \n",
    "    df_base.at[index, 'list_of_terms'] = lista_test_1\n",
    "    del resultado\n",
    "    del lista_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### alternative #1 (11/10)\n",
    "df_base['list_of_terms_pure'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_base[df_base.OPERATION_NUMBER == 'CO-T1496'][['list_of_terms']]\n",
    "df_base['list_of_terms'][139]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. List of Terms: Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#expand the list of tuples:\n",
    "for index, row in df_base.iterrows():\n",
    "    print('processing index:', index)\n",
    "    df_base.at[index, 'alt2_list_terms'] = [k for (k,v) in df_base.alt2_list_terms[index] for count in range(v)]\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_result = df_base.alt2_list_terms.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_grams = []\n",
    "for i in range(len(terms_result)):\n",
    "    for token in terms_result[i]:\n",
    "        terms_grams.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len((terms_grams)),len(set(terms_grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_grams = Counter(terms_grams)\n",
    "sort_orders_terms = sorted(terms_grams.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_orders_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove = []\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (sort_orders_terms[i][0].endswith('_iv') or sort_orders_terms[i][0].endswith('_ii') or sort_orders_terms[i][0].endswith('_us$') or sort_orders_terms[i][0].endswith('/') \\\n",
    "        or sort_orders_terms[i][0].endswith('.') or sort_orders_terms[i][0].endswith('_i') or sort_orders_terms[i][0].endswith('_iii') or sort_orders_terms[i][0].endswith('_”')\\\n",
    "        or sort_orders_terms[i][0].endswith('_a') or sort_orders_terms[i][0].startswith('“_') or sort_orders_terms[i][0].startswith('f._') or sort_orders_terms[i][0].startswith('a._') \\\n",
    "        or sort_orders_terms[i][0].startswith('b._') or sort_orders_terms[i][0].startswith('c._') or sort_orders_terms[i][0].startswith('d._') \\\n",
    "        or sort_orders_terms[i][0].startswith('e._') or sort_orders_terms[i][0].startswith('v._') or sort_orders_terms[i][0].startswith('i._') \\\n",
    "        or sort_orders_terms[i][0].startswith('g._') or sort_orders_terms[i][0].startswith('iv._') or sort_orders_terms[i][0].startswith('&_') \\\n",
    "        or sort_orders_terms[i][0].startswith('actividad/_') or sort_orders_terms[i][0].startswith('ct_') or sort_orders_terms[i][0].startswith('atn_/') \\\n",
    "        or sort_orders_terms[i][0].startswith('/_') or sort_orders_terms[i][0].startswith('ii.') or sort_orders_terms[i][0].startswith('iii_') or sort_orders_terms[i][0].startswith('iv_')\\\n",
    "        or sort_orders_terms[i][0].startswith('a_') or sort_orders_terms[i][0].endswith('_rev') or sort_orders_terms[i][0].startswith('x_') or sort_orders_terms[i][0].startswith('p_') \\\n",
    "        or sort_orders_terms[i][0].startswith('d_') or sort_orders_terms[i][0].startswith('enel_') or sort_orders_terms[i][0].endswith('_enel') or sort_orders_terms[i][0].endswith('_p') or sort_orders_terms[i][0].endswith('_d')\\\n",
    "        or sort_orders_terms[i][0].endswith('_figura') or sort_orders_terms[i][0].endswith('_sp') or sort_orders_terms[i][0].endswith('_cis') or sort_orders_terms[i][0].endswith('_csc') or sort_orders_terms[i][0].endswith('_cobit')\\\n",
    "        or sort_orders_terms[i][0].startswith('dela_') or sort_orders_terms[i][0].endswith('_dela') or sort_orders_terms[i][0].endswith('_nist') or sort_orders_terms[i][0].endswith('_cert') \\\n",
    "        or sort_orders_terms[i][0].endswith('_t') or sort_orders_terms[i][0].endswith('_m') or sort_orders_terms[i][0].startswith('m_') or sort_orders_terms[i][0].startswith('is_') or sort_orders_terms[i][0].startswith('for_')\\\n",
    "        or sort_orders_terms[i][0].startswith('and_') or sort_orders_terms[i][0].startswith('of_') or sort_orders_terms[i][0].startswith('or_') or sort_orders_terms[i][0].startswith('this_') or sort_orders_terms[i][0].startswith('does_')\\\n",
    "        or sort_orders_terms[i][0].startswith('are_') or sort_orders_terms[i][0].startswith('j_') or sort_orders_terms[i][0].startswith('c_') or sort_orders_terms[i][0].endswith('_is') \\\n",
    "        or sort_orders_terms[i][0].endswith('_be') or sort_orders_terms[i][0].endswith('_and') or sort_orders_terms[i][0].endswith('_are') or sort_orders_terms[i][0].endswith('_of')\\\n",
    "        or sort_orders_terms[i][0].endswith('_nº') or sort_orders_terms[i][0].endswith('_t(')\n",
    "       ):\n",
    "       \n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove grmas containing digits:\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (num_there(sort_orders_terms[i][0]) and not ('covid' in sort_orders_terms[i][0] or '2700' in sort_orders_terms[i][0] \\\n",
    "                                                    or 'p2p' in sort_orders_terms[i][0] or '5g' in sort_orders_terms[i][0])):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('indicado_' in sort_orders_terms[i][0]) or (sort_orders_terms[i][0].endswith('_indicado')):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('mencionado' in sort_orders_terms[i][0]) or ('siguiente' in sort_orders_terms[i][0]) or ('párrafo' in sort_orders_terms[i][0]) or ('referido' in sort_orders_terms[i][0]) or ('lleva' in sort_orders_terms[i][0]) or ('recibi' in sort_orders_terms[i][0]) or\\\n",
    "        ('esperado' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (sort_orders_terms[i][0].endswith('_cabo') or sort_orders_terms[i][0].startswith('cabo_')) and \\\n",
    "            not ('haitiano' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'paralo'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '▪' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'componente' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'agencia'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'eficiente'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'figura' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'consultor'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'actual_'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '/'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_+_' in i[0] or i[0].endswith('_+') or i[0].startswith('+_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'eeo'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'acordado'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_x_' in i[0] or i[0].endswith('_x') or i[0].startswith('x_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_dicho_' in i[0] or i[0].endswith('_dicho') or i[0].startswith('dicho_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_d_' in i[0] or i[0].endswith('_d') or i[0].startswith('d_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_meta_' in i[0] or i[0].endswith('_meta') or i[0].startswith('meta_')) and not 'inflación' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in sort_orders_terms:\n",
    "#    if  'operación_cooperación' in i[0] or 'asistencia_técnico' in i[0] or 'capacidad_institucional' in i[0] or 'estrategia_institucional' in i[0] or 'área_transversal' in i[0] or 'agencia_ejecutora' in i[0]:\n",
    "#        print(i[0], i[1])\n",
    "#        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'ejecutor'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'relacionado'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'utilizado' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'definido' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'sigla' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'útil' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].startswith('único_'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if (i[0].startswith('._') or i[0].endswith('_.') or '.' in i[0]):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if (i[0].startswith('-_') or i[0].endswith('_-') or i[0].startswith('−_')):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if (i[0].startswith('órgano_')):\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    #if (i[0].startswith('-_') or i[0].endswith('_-') or i[0].startswith('−_')):\n",
    "    if 'organización' in i[0]:\n",
    "        print(i[0])#, i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'asistencia_técnico', 'capacidad_institucional', 'estrategia_institucional', 'área_transversal', 'estructurar_ejecución', \\\n",
    " 'componente_descripción', 'operación_préstamo', 'organismo_ejecutor', 'américa_latina', 'cooperación_técnico', 'cooperación_técnica', 'operación_de_cooperación', 'agencia_ejecutora', \\\n",
    "  'operación_de_cooperación_técnico', 'agencia_ejecutor', 'nivel_mundial', 'organismo_ejecutor', 'unidad_ejecutor', 'resultar_esperar', 'producto_esperar', 'esperar_del_componente', \\\n",
    "                                    'resultados_esperar', 'resultar_esperar_del_componente', 'principal_resultar_esperar', 'poner_en_funcionamiento', 'problema_específico', \\\n",
    "                                    'nivel_nacional', 'autoridad_nacional', 'presentar_operación', 'resultar_anual', 'estructurar_organizacional', 'gobernar_central', 'tomar_de_decisión',\\\n",
    "                                    'adicional_con_programar', 'documento_de_marco_sectorial', 'adquisición_de_insumo', 'proyectar_pilotar', 'alto_impactar', 'efectividad_comparar', \\\n",
    "                                    'modelar_de_negociar', 'término_de_referenciar', 'aumentar_sostener', 'aumentar_inicial', 'aumentar_del_nivel', 'lección_aprender', 'et_al', 'new_area', \\\n",
    "                                    'technical_assistance', 'optional_link', 'good_practice', 'sector_framework', 'action_plan', 'short_term', 'long_term', 'medium_term', 'year_implementation'\\\n",
    "                                    'tc_resource', 'year_action', 'grant_operation', 'year_of_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in sort_orders_terms:\n",
    "    #if  i[0].startswith('banco_') or i[0].endswith('_banco'): # (i[0].startswith('nivel_') or or i[0].startswith('−_')):\n",
    "    if 'acompa' in i[0]:\n",
    "       # print(i[0], i[1])\n",
    "        test.append((i[0], i[1]))\n",
    "sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted([item[0] for item in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in sort_orders_terms:\n",
    "    #if (i[0].startswith('-_') or i[0].endswith('_-') or i[0].startswith('−_')):\n",
    "    if 'préstamo' in i[0]:\n",
    "       # print(i[0], i[1])\n",
    "        test.append((i[0], i[1]))\n",
    "sorted(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    #if (i[0].startswith('-_') or i[0].endswith('_-') or i[0].startswith('−_')):\n",
    "    if '+' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    #if (i[0].startswith('-_') or i[0].endswith('_-') or i[0].startswith('−_')):\n",
    "    if 'aplicación' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terms_to_remove = terms_to_remove + ['componente_iia', 'asistencia_técnico', 'capacidad_institucional', 'estrategia_institucional', 'área_transversal', 'estructurar_ejecución', \\\n",
    "# 'componente_descripción', 'operación_préstamo', 'organismo_ejecutor', 'américa_latina', 'cooperación_técnico', 'cooperación_técnica', 'operación_de_cooperación', 'agencia_ejecutora', \\\n",
    "#  'operación_de_cooperación_técnico', 'agencia_ejecutor', 'nivel_mundial', 'organismo_ejecutor', 'unidad_ejecutor', 'resultar_esperar', 'producto_esperar', 'esperar_del_componente', \\\n",
    "#                                     'resultados_esperar', 'resultar_esperar_del_componente', 'principal_resultar_esperar', 'poner_en_funcionamiento', 'problema_específico', \\\n",
    "#                                     'nivel_nacional', 'autoridad_nacional', 'presentar_operación', 'resultar_anual', 'estructurar_organizacional', 'gobernar_central', 'tomar_de_decisión'\\\n",
    "#                                    'adicional_con_programar', 'documento_de_marco_sectorial', 'adquisición_de_insumo', 'proyectar_pilotar', 'alto_impactar', 'efectividad_comparar', \\\n",
    "#                                    'modelar_de_negociar', 'término_de_referenciar', 'aumentar_sostener', 'aumentar_inicial', 'aumentar_del_nivel', 'lección_aprender', 'partir_interesar', \\\n",
    "#                                    'resultar_desear', 'centralizar_capaz', 'área_relacionar', 'información_utilizar', 'decisión_relacionar', 'generación_transmisiónuna_combinación', \\\n",
    "#                                    'meta_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove = list(set(terms_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terms_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Remove selected terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['alt2_terms'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_base.at[index, 'alt2_terms'] = [word for word in df_base['alt2_list_terms'][index] if word not in terms_to_remove and '_' in word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final = df_base.alt2_terms.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final_flat = []\n",
    "for i in range(len(terms_final)):\n",
    "    for token in terms_final[i]:\n",
    "        terms_final_flat.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_final_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "terms_final_flat = Counter(terms_final_flat)\n",
    "sort_orders_terms_final = sorted(terms_final_flat.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms_final:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.2: Store terms processed by stanza (Spanish)\n",
    "with open('./output/terms_lemmat_tcs_and_loans_2020-11-12_spanish.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(df_base.alt2_terms, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish: NLP Token extraction and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Sentences and Clean\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=False) #modificado\n",
    "        yield(sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df_base['extracted'].values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "#def remove_stopwords(texts, stop_words):\n",
    "#    return [[word for word in gensim.utils.simple_preprocess(str(doc), deacc=False) if word not in stop_words] for doc in texts]\n",
    "#\n",
    "##\n",
    "#def lemmatization(texts, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP']):\n",
    "#    texts_out = []\n",
    "#    for sent in texts:\n",
    "#        doc = nlp_es(\" \".join(sent)) \n",
    "#        texts_out.append([token.lemma_ for token in doc if (len(token) > 1 and token.pos_ in allowed_postags)])\n",
    "#    # remove stopwords once more after lemmatization\n",
    "#    texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]   \n",
    "#    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words, stop_words)\n",
    "#\n",
    "## Data Lemmatized\n",
    "#data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def lemmatization(texts, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = stNLP(\" \".join(sent)) \n",
    "        texts_out.append([word.lemma.lower() for sent in doc.sentences for word in sent.words if word.pos in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]   \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words, stop_words)\n",
    "\n",
    "# Data Lemmatized\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats_only_tokens = []\n",
    "for i in range(len(data_lemmatized)):\n",
    "    for token in data_lemmatized[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats_only_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(word_stats_only_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tokens and terms/n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized[550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained terms/n-grams are added to the dataset\n",
    "data_lemmatized_full = []\n",
    "for index, row in df_base.iterrows():\n",
    "    data_lemmatized_full.append(data_lemmatized[index] + df_base.alt2_terms[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ~~ ****** ~~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats = []\n",
    "for i in range(len(data_lemmatized_full)):\n",
    "    for token in data_lemmatized_full[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_stats = Counter(word_stats)\n",
    "\n",
    "sort_orders = sorted(word_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_orders:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['data_lemmatized_full'] = data_lemmatized_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.2: Store data_lemmatized_full ONLY TOKENS (Spanish)\n",
    "with open('./output/data_lemmatized_TOKENS_es_2020-11-12.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## v1.2: Store data_lemmatized_full (Spanish)\n",
    "with open('./output/data_lemmatized_full_es_2020-11-12.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.2: Store df_base, in Spanish, containing terms\n",
    "f_df_base_es = 'nlp_df_base_2020-11-12_spanish.joblib'\n",
    "joblib.dump(df_base[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'Document_Name',\n",
    "       'extracted', 'alt2_list_terms_base', 'alt2_list_terms', 'alt2_terms', 'data_lemmatized_full']], './output/' + f_df_base_es + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# v1.2: Store df_base, in Spanish, containing terms\n",
    "f_df_base_es2 = 'nlp_2020-11-12_spacy_annotated_spanish.joblib'\n",
    "joblib.dump(df_base[['OPERATION_NUMBER', 'DOCUMENT_REFERENCE', 'Document_Name', 'textacy_processing']], './output/' + f_df_base_es2 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.1: Store data_lemmatized_full (Spanish)\n",
    "with open('./output/data_lemmatized_full_es_2020-11-10.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.1: Store df_base, in Spanish, containing terms\n",
    "f_df_base_es = 'nlp_df_base_2020-11-10_spanish.joblib'\n",
    "joblib.dump(df_base[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'extracted', 'terms', 'data_lemmatized_full']], './output/' + f_df_base_es + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.0: Store data_lemmatized_full (Spanish)\n",
    "with open('./output/data_lemmatized_full_es_2020-10-20.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.0: Store df_base, in Spanish, containing terms\n",
    "f_df_base_es = 'nlp_df_base_2020-10-20_spanish.joblib'\n",
    "joblib.dump(df_base[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'extracted', 'terms']], './output/' + f_df_base_es + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ----------------------------- FIN Spanish ---------------------------- ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Language Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Documents - v1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en = df1[df1['language'] == 'en']\n",
    "df_base_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_base_en.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates:\n",
    "df_base_en[df_base_en.duplicated(subset=['OPERATION_NUMBER'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1140'])\n",
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1156'])\n",
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'BR-T1415'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.drop([367], inplace=True)\n",
    "df_base_en.drop([1271], inplace=True)\n",
    "df_base_en.drop([1259], inplace=True)\n",
    "df_base_en.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1140'])\n",
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1156'])\n",
    "print(df_base_en[df_base_en.OPERATION_NUMBER == 'BR-T1415'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_base_en.shape)\n",
    "df_base_en.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English: Text Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Textacy processing - Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "# Load Spacy English model in Textacy:\n",
    "en = textacy.load_spacy_lang('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textacy processing on extracted text: \n",
    "df_base_en['textacy_processing'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base_en.iterrows():\n",
    "    #print('Processing index:', str(index)) - no usar .lower() !!!!\n",
    "    df_base_en.at[index, 'textacy_processing'] = textacy.make_spacy_doc(df_base_en.extracted[index].lower(), lang=en)\n",
    "    \n",
    "df_base_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1.3: save results and continue processing in notebook “Digital Transformation - 03.2 - NLP Processing English (Loans and TCs - Stanza) (workpaper)”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# v1.3: Store df_base, in English, containing terms\n",
    "f_df_base_en2 = 'nlp_2021-01-15_spacy_annotated_english.joblib'\n",
    "joblib.dump(df_base_en, './output/' + f_df_base_en2 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- END v1.3 English ----------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operations' filters: ENGLISH documents\n",
    "df_filters_1_en = pd.read_excel('./input/Lista de Operaciones con Documento Encontrado-ES-EN.xlsx', sheet_name='EN')\n",
    "df_filters_1_en['OPERATION_NUMBER'] = df_filters_1_en['OPERATION_NUMBER'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filters_en = pd.concat([df_filters_1_en[['OPERATION_NUMBER']], df_filters_2[['OPERATION_NUMBER']], \\\n",
    "                        df_filters_3[['OPERATION_NUMBER']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filters_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by selected operations:\n",
    "df_base_en = data_base[data_base['OPERATION_NUMBER'].isin(df_filters_en['OPERATION_NUMBER'])]\n",
    "# select the Spanish documents:\n",
    "df_base_en = df_base_en[df_base_en['language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(df_base_en.OPERATION_NUMBER.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en[df_base_en.duplicated(subset=['OPERATION_NUMBER'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en[df_base_en.OPERATION_NUMBER == 'BR-T1415']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.drop([742], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1140']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.drop([1392], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en[df_base_en.OPERATION_NUMBER == 'UR-L1156']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.drop([1827], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~~ ### ~~ ### ~~ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove the following Loans associated to Haiti since latest searches in Covergence do not show these operations as approved:\n",
    "df_base_en[df_base_en.OPERATION_NUMBER.str.startswith('HA-L')]['OPERATION_NUMBER'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_index_to_remove = list(df_base_en[df_base_en.OPERATION_NUMBER.str.startswith('HA-L')]['OPERATION_NUMBER'].index)\n",
    "for i in lista_index_to_remove:\n",
    "    df_base_en.drop([i], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.doc_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df with both document types:\n",
    "f_df_base_en = 'df_loans_tcs_2020-12-07_english.joblib'\n",
    "joblib.dump(df_base_en, './output/' + f_df_base_en + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English: NLP n-Gram Analysis - using Textacy bag-of-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, en_core_web_lg\n",
    "nlp_en = spacy.load('en_core_web_lg', disable=['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# Spacy stop_words\n",
    "stop_words.extend(nlp_en.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stop_words:\n",
    "stop_words.extend(['ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'ill', 'descripción', 'componente', 'objetivo', 'ct', 'mailto', 'país', 'millón', 'millones', \\\n",
    "                   'año', 'años', 'dólar', 'dolar', 'dólares', 'si', 'bid', 'us', 'oc', 'gn', 'tc', 'atn', 'opc', 'pib', 'ar', 'br', 'uy', 'cl', 'co', \\\n",
    "                   'cclip', 'pbl', 'uis', 'ab', 'org', 'pr', 'bo', 'bl', 'pe', 'ec', 'ja', 'mx', 'ca', 'gu', 'su', 'ho', 'hn', 'mr', 'rg', 'ee', 'uu', \\\n",
    "                   'cr', 'tdr', 'rn', 'nº', 'usd', 'gy', 've', 'et', 'the', 'for', 'to', 'grt', 'fm', 'pr', 'pa', 'ni', 'aa', 'es', 'sp', 'tor', 'tr', \\\n",
    "                   'inglés', 'cty', 'nv', 'profisco', 'asimismo', 'actual', 'costo', 'resultar', 'esperar', 'ejecutar', 'unidad', 'agencia', 'justificación', \\\n",
    "                   'véase', 'ct', 'loan', 'paragraph', 'lac', 'optional'])\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Textacy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "# Load Spacy Spanish model:\n",
    "en = textacy.load_spacy_lang('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textacy processing on extracted text: \n",
    "df_base_en['textacy_processing'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base_en.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_base_en.at[index, 'textacy_processing'] = textacy.make_spacy_doc(df_base_en.extracted[index].lower(), lang=en)\n",
    "    \n",
    "df_base_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_base_en.textacy_processing[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. List of Terms (Bag-of-Terms): n-Grams extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. List of Terms Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en['list_of_terms'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base_en.iterrows():\n",
    "    print('Processing index:', str(index))\n",
    "    \n",
    "    #generate terms:\n",
    "    terms_list_en = df_base_en['textacy_processing'][index]._.to_terms_list(ngrams=(2, 3, 4, 5, 6), entities=False, normalize=\"lemma\", weighting=\"count\", as_strings=True, filter_stops=True, filter_punct=True, filter_nums=True, include_pos=['PROPN', 'NOUN', 'ADJ', 'ADP'], min_freq=2)\n",
    "    \n",
    "    #replace blanks with '_':\n",
    "    resultado_pre_en = Counter([(item.lower()).replace(' ', '_') for item in terms_list_en])\n",
    "    \n",
    "    #select terms that appear 2 or more times, convert to list including count and store:\n",
    "    df_base_en.at[index, 'list_of_terms'] = [k for (k,v) in resultado_pre_en.items() for count in range(v) if v > 1 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. List of Terms: Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_result = df_base_en.list_of_terms.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_grams = []\n",
    "for i in range(len(terms_result)):\n",
    "    for token in terms_result[i]:\n",
    "        terms_grams.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len((terms_grams)),len(set(terms_grams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_grams = Counter(terms_grams)\n",
    "sort_orders_terms = sorted(terms_grams.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sort_orders_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove_en = []\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (sort_orders_terms[i][0].endswith('_iv') or sort_orders_terms[i][0].endswith('_ii') or sort_orders_terms[i][0].endswith('_us$') or sort_orders_terms[i][0].endswith('/') \\\n",
    "        or sort_orders_terms[i][0].endswith('.') or sort_orders_terms[i][0].endswith('_i') or sort_orders_terms[i][0].endswith('_iii') or sort_orders_terms[i][0].endswith('_”')\\\n",
    "        or sort_orders_terms[i][0].endswith('_a') or sort_orders_terms[i][0].startswith('“_') or sort_orders_terms[i][0].startswith('f._') or sort_orders_terms[i][0].startswith('a._') \\\n",
    "        or sort_orders_terms[i][0].startswith('b._') or sort_orders_terms[i][0].startswith('c._') or sort_orders_terms[i][0].startswith('d._') \\\n",
    "        or sort_orders_terms[i][0].startswith('e._') or sort_orders_terms[i][0].startswith('v._') or sort_orders_terms[i][0].startswith('i._') \\\n",
    "        or sort_orders_terms[i][0].startswith('g._') or sort_orders_terms[i][0].startswith('iv._') or sort_orders_terms[i][0].startswith('&_') \\\n",
    "        or sort_orders_terms[i][0].startswith('actividad/_') or sort_orders_terms[i][0].startswith('ct_') or sort_orders_terms[i][0].startswith('atn_/') \\\n",
    "        or sort_orders_terms[i][0].startswith('/_') or sort_orders_terms[i][0].startswith('ii.') or sort_orders_terms[i][0].startswith('iii_') or sort_orders_terms[i][0].startswith('iv_')\\\n",
    "        or sort_orders_terms[i][0].startswith('a_') or sort_orders_terms[i][0].endswith('_rev') or sort_orders_terms[i][0].startswith('x_') or sort_orders_terms[i][0].startswith('p_') \\\n",
    "        or sort_orders_terms[i][0].startswith('d_') or sort_orders_terms[i][0].startswith('enel_') or sort_orders_terms[i][0].endswith('_enel') or sort_orders_terms[i][0].endswith('_p') or sort_orders_terms[i][0].endswith('_d')\\\n",
    "        or sort_orders_terms[i][0].endswith('_figura') or sort_orders_terms[i][0].endswith('_sp') or sort_orders_terms[i][0].endswith('_cis') or sort_orders_terms[i][0].endswith('_csc') or sort_orders_terms[i][0].endswith('_cobit')\\\n",
    "        or sort_orders_terms[i][0].startswith('dela_') or sort_orders_terms[i][0].endswith('_dela') or sort_orders_terms[i][0].endswith('_nist') or sort_orders_terms[i][0].endswith('_cert') \\\n",
    "        or sort_orders_terms[i][0].endswith('_t') or sort_orders_terms[i][0].endswith('_m') or sort_orders_terms[i][0].startswith('m_') or sort_orders_terms[i][0].startswith('is_') or sort_orders_terms[i][0].startswith('for_')\\\n",
    "        or sort_orders_terms[i][0].startswith('and_') or sort_orders_terms[i][0].startswith('of_') or sort_orders_terms[i][0].startswith('or_') or sort_orders_terms[i][0].startswith('this_') or sort_orders_terms[i][0].startswith('does_')\\\n",
    "        or sort_orders_terms[i][0].startswith('are_') or sort_orders_terms[i][0].startswith('j_') or sort_orders_terms[i][0].startswith('c_') or sort_orders_terms[i][0].endswith('_is') \\\n",
    "        or sort_orders_terms[i][0].endswith('_be') or sort_orders_terms[i][0].endswith('_and') or sort_orders_terms[i][0].endswith('_are') or sort_orders_terms[i][0].endswith('_of')\\\n",
    "        or sort_orders_terms[i][0].endswith('_nº') or sort_orders_terms[i][0].endswith('_t(') or sort_orders_terms[i][0].startswith('table_i') or sort_orders_terms[i][0].endswith('_c') or sort_orders_terms[i][0].endswith('_rg') \\\n",
    "        or sort_orders_terms[i][0].endswith('_al') or sort_orders_terms[i][0].endswith('_atn') or sort_orders_terms[i][0].endswith('_aim') or sort_orders_terms[i][0].endswith('_™') or sort_orders_terms[i][0].startswith('δ') \\\n",
    "        or sort_orders_terms[i][0].endswith('_de') or sort_orders_terms[i][0].startswith('de_') or sort_orders_terms[i][0].endswith('_del') or sort_orders_terms[i][0].startswith('del_') or sort_orders_terms[i][0].endswith('_of_new')\n",
    "       ):\n",
    "       \n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_to_remove_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove grmas containing digits:\n",
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if (num_there(sort_orders_terms[i][0]) and not ('covid' in sort_orders_terms[i][0] or '2700' in sort_orders_terms[i][0] or 'revolution' in sort_orders_terms[i][0] \\\n",
    "                                                    or 'p2p' in sort_orders_terms[i][0] or '5g' in sort_orders_terms[i][0])):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('mention' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('following' in sort_orders_terms[i][0]) or ('received' in sort_orders_terms[i][0]) or ('section' in sort_orders_terms[i][0]) \\\n",
    "    or ('component' in sort_orders_terms[i][0] and not 'critical_component' in sort_orders_terms[i][0]) \\\n",
    "    or ('option' in sort_orders_terms[i][0] and not 'adoption' in sort_orders_terms[i][0]) or ('this_component' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if ('paragraph' in sort_orders_terms[i][0]) or ('abbreviation' in sort_orders_terms[i][0]) or ('conclusion' in sort_orders_terms[i][0]) \\\n",
    "    or ('revision_of' in sort_orders_terms[i][0]) or ('▪' in sort_orders_terms[i][0]) or ('.' in sort_orders_terms[i][0]) or ('+' in sort_orders_terms[i][0]) or ('¶' in sort_orders_terms[i][0]) \\\n",
    "    or ('$' in sort_orders_terms[i][0]) or ('' in sort_orders_terms[i][0]) or ('=' in sort_orders_terms[i][0]) or ('/' in sort_orders_terms[i][0]) \\\n",
    "    or ('€' in sort_orders_terms[i][0]):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'figure' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'related'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sort_orders_terms)):\n",
    "    if sort_orders_terms[i][0].endswith('_term') or sort_orders_terms[i][0].startswith('term_'):\n",
    "        print(sort_orders_terms[i][0], sort_orders_terms[i][1])\n",
    "        terms_to_remove_en.append(sort_orders_terms[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'term'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '▪' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'component' in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'executing'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'execut'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if ('_x_' in i[0] or i[0].endswith('_x') or i[0].startswith('x_')) or ('_d_' in i[0] or i[0].endswith('_d') or i[0].startswith('d_')) or \\\n",
    "        ('_m_' in i[0] or i[0].endswith('_m') or i[0].startswith('m_')) or (i[0].endswith('_c') or i[0].startswith('c_') or '_c_' in i[0]) or \\\n",
    "        (i[0].endswith('_e') or i[0].startswith('e_') or '_e_' in i[0]):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_l') or i[0].startswith('l_') or i[0].endswith('_del') or i[0].startswith('del_') or i[0].endswith('detallado') or i[0].startswith('detallado_') \\\n",
    "        or i[0].endswith('_n') or i[0].startswith('n_'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_ic') or i[0].startswith('ic_') or i[0].endswith('_r') or i[0].startswith('r_') or i[0].startswith('f_') or i[0].endswith('_v') or i[0].startswith('sa_') \\\n",
    "        or i[0].startswith('an_') or i[0].endswith('_an') or i[0].startswith('by_') or i[0].endswith('_by') or i[0].startswith('or_') or i[0].endswith('_or') \\\n",
    "        or i[0].startswith('can_') or i[0].endswith('_can') or i[0].startswith('to_') or i[0].endswith('_to') or i[0].startswith('cabo_') or i[0].endswith('_cabo') \\\n",
    "        or i[0].startswith('the_') or i[0].endswith('_the') or i[0].startswith('of_') or i[0].endswith('_of') or i[0].startswith('and_') or i[0].endswith('_and') \\\n",
    "        or '_and_' in i[0] or i[0].startswith('h_') or i[0].endswith('_b') or i[0].endswith('_nis') or i[0].endswith('_n') or i[0].startswith('one_') or i[0].endswith('_one') \\\n",
    "        or i[0].endswith('_af') or i[0].endswith('_aes') or i[0].endswith('_cc') or i[0].endswith('_sncti') or i[0].endswith('_cb') or i[0].endswith('_foppa'):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'country_office'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if i[0].endswith('_inc') or i[0].startswith('inc_') or ('_inc_' in i[0]) or i[0].endswith('_int') or i[0].startswith('int_') or ('_int_' in i[0]):\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in sort_orders_terms:\n",
    "#    if 'utilizado' in i[0] or 'amplio_cobertura' in i[0] or 'definido' in i[0] or 'frecuencia_reportar' in i[0] or 'red_saro' in i[0] or 'brasil' in i[0] \\\n",
    "#        or 'uruguay' in i[0] or 'trinidad' in i[0] or 'concerniente' in i[0] or 'corea_sur' in i[0] or 'example' in i[0] or 'electric_corp' in i[0] or 'haiti' in i[0] or 'colombia' in i[0]\\\n",
    "#        or 'american_' in i[0] or 'lan_recuperac' in i[0] or 'forma_claro' in i[0] or 'costa_rica' in i[0] or 'proyecto_ley' in i[0] or 'ley_específico' in i[0]  or 'venezuela' in i[0]\\\n",
    "#        or 'cuarto_nivel' in i[0] or 'paraguay' in i[0] or 'barbado' in i[0] or 'taller' in i[0] or 'nacional_contratación_público' in i[0] or 'nacional_inteligencia' in i[0] \\\n",
    "#        or 'nacional_telecomunicación' in i[0] or 'aplicación_ens' in i[0] or 'instrumento_línea' in i[0] or 'evento_capacitación' in i[0] or 'curso_capacitación' in i[0] \\\n",
    "#        or 'calidad_software_control' in i[0] or 'ciberseguridadriesgo' in i[0] or 'guyana' in i[0] or 'chile' in i[0] or 'argentin' in i[0] or 'peru' in i[0] \\\n",
    "#        or 'ecuador' in i[0] or 'bolivia' in i[0] or 'guatemala' in i[0] or 'república_dominicano' in i[0] or 'dominican' in i[0] or 'país_caribe_oriental' in i[0]:    \n",
    "#        print(i[0], i[1])\n",
    "#        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'previous'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'wait'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'actual'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'in_fact'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if 'main_'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms:\n",
    "    if '_to_the_'  in i[0] or '_for_the_'  in i[0] or '_in_the_'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove_en = terms_to_remove_en + ['componente_iia', 'asistencia_técnico', 'capacidad_institucional', 'estrategia_institucional', 'área_transversal', 'estructurar_ejecución', \\\n",
    " 'componente_descripción', 'operación_préstamo', 'organismo_ejecutor', 'américa_latina', 'cooperación_técnico', 'cooperación_técnica', 'operación_de_cooperación', 'agencia_ejecutora', \\\n",
    "  'operación_de_cooperación_técnico', 'agencia_ejecutor', 'nivel_mundial', 'organismo_ejecutor', 'unidad_ejecutor', 'resultar_esperar', 'producto_esperar', 'esperar_del_componente', \\\n",
    "                                    'resultados_esperar', 'resultar_esperar_del_componente', 'principal_resultar_esperar', 'poner_en_funcionamiento', 'problema_específico', \\\n",
    "                                    'nivel_nacional', 'autoridad_nacional', 'presentar_operación', 'resultar_anual', 'estructurar_organizacional', 'gobernar_central', 'tomar_de_decisión',\\\n",
    "                                    'adicional_con_programar', 'documento_de_marco_sectorial', 'adquisición_de_insumo', 'proyectar_pilotar', 'alto_impactar', 'efectividad_comparar', \\\n",
    "                                    'modelar_de_negociar', 'término_de_referenciar', 'aumentar_sostener', 'aumentar_inicial', 'aumentar_del_nivel', 'lección_aprender', 'et_al', 'new_area', \\\n",
    "                                    'technical_assistance', 'optional_link', 'good_practice', 'sector_framework', 'action_plan', 'short_term', 'long_term', 'medium_term', 'year_implementation'\\\n",
    "                                    'tc_resource', 'year_action', 'grant_operation', 'year_of_age', 'year_implementation', 'property_of_all_the_document', 'support_of_the_world', 'main_lesson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove_en = list(set(terms_to_remove_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terms_to_remove_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=121\n",
    "#df_base['list_of_terms'][1729]\n",
    "[word for word in df_base_en['list_of_terms'][index] if word not in terms_to_remove_en and '_' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check for \"datum\":\n",
    "\n",
    "for i in sort_orders_terms:\n",
    "    if 'datum'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for replacing \"datum\":\n",
    "test = ['datum_center', 'datum_science', 'datum_initiative', 'open_datum_initiative', 'otra_cosa', 'cosa_otra']\n",
    "[word if 'datum' not in word else word.replace('datum', 'data') for word in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Remove selected terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en['terms'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_base_en.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_base_en.at[index, 'terms'] = [word for word in df_base_en['list_of_terms'][index] if word not in terms_to_remove_en and '_' in word]\n",
    "    # replace \"datum\":\n",
    "    df_base_en.at[index, 'terms'] = [word if 'datum' not in word else word.replace('datum', 'data') for word in df_base_en['terms'][index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final = df_base_en.terms.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_final_flat = []\n",
    "for i in range(len(terms_final)):\n",
    "    for token in terms_final[i]:\n",
    "        terms_final_flat.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(terms_final_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "terms_final_flat = Counter(terms_final_flat)\n",
    "sort_orders_terms_final = sorted(terms_final_flat.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sort_orders_terms_final:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_orders_terms_final:\n",
    "    if 'data'  in i[0]:\n",
    "        print(i[0], i[1])\n",
    "        #terms_to_remove_en.append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Token extraction and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.textacy_processing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data_en = df_base_en['extracted'].values.tolist()\n",
    "data_words_en = list(sent_to_words(data_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_words_en[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "def remove_stopwords_en(texts, stop_words):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc), deacc=False) if word not in stop_words] for doc in texts]\n",
    "\n",
    "#\n",
    "def lemmatization_en(texts, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp_en(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if (len(token) > 1 and token.pos_ in allowed_postags)])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]   \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove Stop Words\n",
    "data_words_nostops_en = remove_stopwords_en(data_words_en, stop_words)\n",
    "\n",
    "# Data Lemmatized\n",
    "data_lemmatized_en = lemmatization_en(data_words_nostops_en, allowed_postags=['PROPN', 'NOUN', 'ADJ', 'ADP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"datum\" with data:\n",
    "for i in range(len(data_lemmatized_en)):\n",
    "    data_lemmatized_en[i] = [word if 'datum' not in word else word.replace('datum', 'data') for word in data_lemmatized_en[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats_only_tokens_en = []\n",
    "for i in range(len(data_lemmatized_en)):\n",
    "    for token in data_lemmatized_en[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats_only_tokens_en.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(word_stats_only_tokens_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tokens and terms/n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_en.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained terms/n-grams are added to the dataset\n",
    "data_lemmatized_full_en = []\n",
    "for index, row in df_base_en.iterrows():\n",
    "    data_lemmatized_full_en.append(data_lemmatized_en[index] + df_base_en.terms[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ~~ ****** ~~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word evaluation:\n",
    "word_stats = []\n",
    "for i in range(len(data_lemmatized_full_en)):\n",
    "    for token in data_lemmatized_full_en[i]:\n",
    "        #if '_' in token:\n",
    "        #    print(str(i),token)\n",
    "        word_stats.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = Counter(word_stats)\n",
    "\n",
    "sort_orders = sorted(word_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_orders:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data_lemmatized as a new column and store the results:\n",
    "df_base_en['data_lemmatized'] = data_lemmatized_full_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.2: Store data_lemmatized_full (English)\n",
    "with open('./output/data_lemmatized_full_en_2020-12-07.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full_en, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.2: Store df_base_en (English documents) containing all terms and tokens\n",
    "f_df_base_en = 'nlp_df_base_2020-12-07_english_v1.2.joblib'\n",
    "joblib.dump(df_base_en, './output/' + f_df_base_en + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ------- ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.0: Store data_lemmatized_full (English)\n",
    "with open('./output/data_lemmatized_full_en_2020-10-20.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(data_lemmatized_full_en, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.0: Store df_base, in English, containing terms\n",
    "f_df_base_en = 'nlp_df_base_2020-10-20_english.joblib'\n",
    "joblib.dump(df_base_en[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'extracted', 'terms']], './output/' + f_df_base_en + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 1.3           01/15/2021        Emiliano Colina    - Filter updated to include all operations from 01/2017 to      \n",
    "#                                                         12/2020\n",
    "\n",
    "#   - 1.2           12/07/2020        Emiliano Colina    - Only English documents NLP Processing with Spacy, since      \n",
    "#                                                         Spanish docs were processed in a separate ntbk using Stanza\n",
    "\n",
    "\n",
    "#   - 1.1           11/10/2020        Emiliano Colina    - Updated version, removing stop-words from terms,      \n",
    "#                                                         tested inflector\n",
    "\n",
    "\n",
    "#   - 1.0           10/19/2020        Emiliano Colina    - Initial version, to include a separate layer for NLP     \n",
    "#                                                         processing on a dedicated notebook\n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
