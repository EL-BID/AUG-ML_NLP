{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.0 - Document Processing TCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 02 - Document Processing\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): pdf files\n",
    "#-- Description:  \n",
    "#                - Approval Documents of TCs\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  1.3\n",
    "#-- Last Update: 01/12/2021\n",
    "#-- Last Revision Date: 08/23/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.3 - data from November and December 2020 (4th Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Approval_Documents_Collection_2021-01-12_v10_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(~data['Document_Name'].str.contains('Approval Document - GA-274-1')) & (data.page_count > 5) & (data.blank_pages.astype(float) < 60)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no additional filtering required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the type of document: tc's, other\n",
    "data['doc_type'] = ''\n",
    "# stores the matching title that defines the type and its page\n",
    "data['doc_identifier'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_review = []\n",
    "tc_count = 0\n",
    "for index, row in data.iterrows():\n",
    "    is_tc = False\n",
    "    for page in range(0,len(data.Document_Content[index])):\n",
    "        if re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('TC header found at page:', str(page))\n",
    "            tc_count += 1\n",
    "            is_tc = True\n",
    "            data.at[index, 'doc_type'] = 'tc'\n",
    "            match_title_type = re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            data.at[index, 'doc_identifier'] = (match_title_type, page)\n",
    "            break\n",
    "    if not is_tc: \n",
    "        print('check regex on:', str(index))\n",
    "        data.at[index, 'doc_type'] = 'other'\n",
    "        data.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('TCs identified:', str(tc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "data['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    if data['doc_type'][index] == 'tc':\n",
    "        data.at[index, 'language'] = detect(''.join(data['Document_Content'][index][:5])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        datae.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.3: TC documents, both languages - November and December 2020\n",
    "df_data_novdec = 'df_pre_tcs_2021-01-12_v12.joblib'\n",
    "joblib.dump(data, './output/' + df_data_novdec + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.2 - data from October 2020 (3rd Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Approval_Documents_Collection_2020-11-04_v09_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(~data['Document_Name'].str.contains('Approval Document - GA-274-1')) & (data.page_count > 5) & (data.blank_pages.astype(float) < 60)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no additional filtering required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the type of document: tc's, other\n",
    "data['doc_type'] = ''\n",
    "# stores the matching title that defines the type and its page\n",
    "data['doc_identifier'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_review = []\n",
    "tc_count = 0\n",
    "for index, row in data.iterrows():\n",
    "    is_tc = False\n",
    "    for page in range(0,len(data.Document_Content[index])):\n",
    "        if re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('TC header found at page:', str(page))\n",
    "            tc_count += 1\n",
    "            is_tc = True\n",
    "            data.at[index, 'doc_type'] = 'tc'\n",
    "            match_title_type = re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            data.at[index, 'doc_identifier'] = (match_title_type, page)\n",
    "            break\n",
    "    if not is_tc: \n",
    "        print('check regex on:', str(index))\n",
    "        data.at[index, 'doc_type'] = 'other'\n",
    "        data.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('TCs identified:', str(tc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "data['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    if data['doc_type'][index] == 'tc':\n",
    "        data.at[index, 'language'] = detect(''.join(data['Document_Content'][index][:5])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        datae.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.1 - Storing (data from July to September 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.1: TC documents, both languages - July to September 2020\n",
    "df_data_july2sept = 'df_pre_july2sept_2020-10-16_v11.joblib'\n",
    "joblib.dump(data, './output/' + df_data_july2sept + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.1 - data from July to September 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Approval_Documents_Collection_2020-10-16_v08_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(~data['Document_Name'].str.contains('Approval Document - GA-274-1')) & (data.page_count > 5) & (data.blank_pages.astype(float) < 60)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no additional filtering required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the type of document: tc's, other\n",
    "data['doc_type'] = ''\n",
    "# stores the matching title that defines the type and its page\n",
    "data['doc_identifier'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_review = []\n",
    "tc_count = 0\n",
    "for index, row in data.iterrows():\n",
    "    is_tc = False\n",
    "    for page in range(0,len(data.Document_Content[index])):\n",
    "        if re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('TC header found at page:', str(page))\n",
    "            tc_count += 1\n",
    "            is_tc = True\n",
    "            data.at[index, 'doc_type'] = 'tc'\n",
    "            match_title_type = re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|Nombre de la CT|TC DOCUMENT|TC Name|TECHNICAL COOPERATION', data.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            data.at[index, 'doc_identifier'] = (match_title_type, page)\n",
    "            break\n",
    "    if not is_tc: \n",
    "        print('check regex on:', str(index))\n",
    "        data.at[index, 'doc_type'] = 'other'\n",
    "        data.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('TCs identified:', str(tc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "data['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in data.iterrows():\n",
    "    if data['doc_type'][index] == 'tc':\n",
    "        data.at[index, 'language'] = detect(''.join(data['Document_Content'][index][:5])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        datae.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1.1 - Storing (data from July to September 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.1: TC documents, both languages - July to September 2020\n",
    "df_data_july2sept = 'df_pre_july2sept_2020-10-16_v11.joblib'\n",
    "joblib.dump(data, './output/' + df_data_july2sept + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataframe from `Digital Transformation Advisory - 01 - Document Collection` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_pre = joblib.load('./output/Approval_Documents_Collection_2020-07-08_v04_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Approval Document` Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Document Location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Test:\n",
    "file_dir + '\\\\' + df_base_pre.Document_Name[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataframe for text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['Document_Content'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the documents and store the content in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in df_base.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + df_base.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        df_base.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        df_base.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.Document_Content[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#v0.3 - Store content\n",
    "f_base = 'TC_Approval_Documents-full_content_v03_2020-07-08.joblib'\n",
    "joblib.dump(df_base, './output/' + f_base + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['blank_pages'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converted and manually adjusted in the dataframe: \n",
    "\n",
    "- 'RG-T3485': `RG-T3485_TC Document - RG-T3485.pdf`\n",
    "- 'RG-T3539': `RG-T3539_Documento CT - RG-T3539.pdf`\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Docx files converted to pdf - using MS-Word:\n",
    "df_base[df_base['Document_Name'].str.endswith('docx')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for index in [1187, 1223]:\n",
    "    df_base['Document_Name'][index] = df_base['Document_Name'][index].replace('docx', 'pdf')\n",
    "    \n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + df_base.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    # extract pages:\n",
    "    # Read PDF file\n",
    "    data = parser.from_file(filename, xmlContent=True)\n",
    "    xhtml_data = BeautifulSoup(data['content'])\n",
    "    for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "        # Parse PDF data using TIKA (xml/html)\n",
    "        # It's faster and safer to create a new buffer than truncating it\n",
    "        # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "        _buffer = StringIO()\n",
    "        _buffer.write(str(content))\n",
    "        parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "    \n",
    "        # Add pages\n",
    "        if parsed_content['content'] != None:    # page is not blank page\n",
    "            text = parsed_content['content'].strip()\n",
    "        else: \n",
    "            text = ''\n",
    "        \n",
    "        pages_txt.append(text)\n",
    "        \n",
    "            \n",
    "    # save results and report status:\n",
    "    df_base.at[index, 'Document_Content'] = pages_txt\n",
    "    doc_count += 1\n",
    "    print()\n",
    "    print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "    del pages_txt\n",
    "    del filename\n",
    "    print('------')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base[1187:1188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, row in df_base.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = df_base['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    df_base.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base['Document_Content'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_pages_per_document = df_base.blank_pages.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "blank_pages_per_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Build the histogram.\n",
    "plt.rcParams[\"figure.figsize\"]=10,10\n",
    "plt.hist(blank_pages_per_document, bins = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], alpha=0.75, histtype='bar', ec='black')\n",
    "plt.xlabel('Number of pages per document')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'blank pages per document (%)')\n",
    "#plt.figure(figsize = (200, 100))\n",
    "#plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pylab as P\n",
    "\n",
    "P.figure()\n",
    "\n",
    "bins = [0, 10,20,30,40,50,60,70,80,90,100]\n",
    "# the histogram of the data with histtype='step'\n",
    "n, bins, patches = P.hist(blank_pages_per_document, bins, histtype='bar', rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['page_count'] = df_base['Document_Content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the document length distribution:\n",
    "\n",
    "P.figure()\n",
    "# the histogram of the data with histtype='step'\n",
    "n, bins, patches = P.hist(df_base.page_count.to_list(), bins, histtype='bar', rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifics - Check document: 'Approval Document-BR-T1408_18Nov2019.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document-BR-T1408_18Nov2019')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base.Document_Content[170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document - AR-T1206_12Dec2018-171614')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document - BO-T1306_12Apr2018-15374')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base.Document_Content[102][1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document - PE-T1424_01Jul2019-17134')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base.Document_Content[707][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document - RG-T3394 _30Apr2019-103759')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base[df_base['Document_Name'].str.contains('Approval Document - SU-T1102_21Jun2018-141515')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering \n",
    "\n",
    "#### Step_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 07/10, <b>filter out</b> files under the following conditions:\n",
    "* documents containing `Approval Document - GA-274-1-` as Document_Name\n",
    "* documents with page_count < 6\n",
    "* documents with more than 60% of blank pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_base[(~df_base['Document_Name'].str.contains('Approval Document - GA-274-1')) & (df_base.page_count > 5) & (df_base.blank_pages.astype(float) < 60)].copy()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the new results:\n",
    "P.figure()\n",
    "# the histogram of the data with histtype='step'\n",
    "n, bins, patches = P.hist(df_filtered.page_count.to_list(), bins, histtype='bar', rwidth=0.8, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base[df_base.blank_pages.astype(float) > 60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_filtered.page_count, color='g').set_title('Document length distribution (in pages)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_filtered.blank_pages).set_title('Blank Pages distribution (% of blank pages)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_base.shape[0], df_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_2 \n",
    "\n",
    "######  classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above filtering decisions, a second filtering level is performed where TC's components -such as common titles- are reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_2 = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the type of document: tc's, other\n",
    "df_filtered_2['doc_type'] = ''\n",
    "# stores the matching title that defines the type and its page\n",
    "df_filtered_2['doc_identifier'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_review = []\n",
    "tc_count = 0\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    is_tc = False\n",
    "    for page in range(0,len(df_filtered_2.Document_Content[index])):\n",
    "        if re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|TC DOCUMENT|TC Name', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('TC header found at page:', str(page))\n",
    "            tc_count += 1\n",
    "            is_tc = True\n",
    "            df_filtered_2.at[index, 'doc_type'] = 'tc'\n",
    "            match_title_type = re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|TC DOCUMENT|TC Name', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'doc_identifier'] = (match_title_type, page)\n",
    "            break\n",
    "    if not is_tc: \n",
    "        print('check regex on:', str(index))\n",
    "        df_filtered_2.at[index, 'doc_type'] = 'other'\n",
    "        df_filtered_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('TCs identified:', str(tc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# indexes of documents to review:\n",
    "#len(to_review)\n",
    "\n",
    "df_filtered_2.loc[to_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2.Document_Content[1357][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for element in to_review:\n",
    "    print(str(element),df_filtered_2.Document_Name[element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_filtered_2.loc[~df_filtered_2.index.isin(to_review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_2\n",
    "df_filtered_2_step_2 = df_filtered_2.loc[~df_filtered_2.index.isin(to_review)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing -intermediate- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.01: NULL_URL TC documents, both languages\n",
    "f_df_pre_null_v101 = 'df_pre_null_2020-08-25_v101.joblib'\n",
    "joblib.dump(df_filtered_null_2, './output/' + f_df_pre_null_v101 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1.0: NULL_URL TC documents, both languages\n",
    "f_df_pre_null_v10 = 'df_pre_null_2020-08-24_v10.joblib'\n",
    "joblib.dump(df_filtered_null_2, './output/' + f_df_pre_null_v10 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.8: English TC documents, content extracted and cleaned - pending supraindexes removal\n",
    "f_df_pre_en_v08 = 'df_pre_en_2020-08-23_v08.joblib'\n",
    "joblib.dump(df_pre_en, './output/' + f_df_pre_en_v08 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.7: Spanish documents, content extracted and cleaned - pending conversion of 6 to ó\n",
    "f_df_pre_es_v07 = 'df_pre_es_2020-07-14_v07.joblib'\n",
    "joblib.dump(df_pre_es, './output/' + f_df_pre_es_v07 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.6: Spanish documents, content extracted\n",
    "f_df_pre_es_v06 = 'df_pre_es_2020-07-14_v06.joblib'\n",
    "joblib.dump(df_pre_es, './output/' + f_df_pre_es_v06 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.5: Added document language (es, en, na), selected spanish, filtered, identified interest sections by title and their respective location\n",
    "f_df_filtered_es_v05 = 'df_filtered_es_2020-07-11_v05.joblib'\n",
    "joblib.dump(df_pre_es, './output/' + f_df_filtered_es_v05 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.4: Added document type (tc, other)\n",
    "f_df_filtered_2_v04 = 'df_filtered_2_step_2_2020-07-11_v04.joblib'\n",
    "joblib.dump(df_filtered_2_step_2, './output/' + f_df_filtered_2_v04 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df_filtered_2_step_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recover df_pre_es: 07/13 v0.5\n",
    "df_pre_es = joblib.load('./output/df_filtered_es_2020-07-11_v05.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection over the 'doc_identifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "df_pre['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_pre.iterrows():\n",
    "    if df_pre['doc_type'][index] == 'tc':\n",
    "        df_pre.at[index, 'language'] = detect(df_pre['Document_Content'][index][df_pre.doc_identifier[index][1]]) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        df_pre.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2nd run on the first 3 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_pre.iterrows():\n",
    "    if df_pre['doc_type'][index] == 'tc':\n",
    "        df_pre.at[index, 'language'] = detect(''.join(df_pre['Document_Content'][index][:3])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        df_pre.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "After inspection and titles search, some documents were found that their language was different from the expected.\n",
    "The following indexes were manually changed into 'en'\n",
    "909, 920, 1045 and 1105\n",
    "<br>\n",
    "Additionally, the documents `CO-T1580_Approval Document-CO-T1580 .pdf` and `PE-T1408_Approval Document - PE-T1408_15Oct2018-143108.pdf` had their intermediate titles also changed on page 4 (indexes 293, 698)\n",
    "<br>\n",
    "Finally, document hadd missing pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually changed language\n",
    "df_pre.at[909, 'language'] = 'en'\n",
    "df_pre.at[920, 'language'] = 'en'\n",
    "df_pre.at[1045, 'language'] = 'en'\n",
    "df_pre.at[1105, 'language'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on language to better process titles:\n",
    "# Spanish:\n",
    "df_pre_es = df_pre[df_pre['language'] == 'es'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of intermediate title for document `CO-T1580_Approval Document-CO-T1580 .pdf`: \n",
    "lista_aux = df_pre_es['Document_Content'][293]\n",
    "lista_aux[4] = df_pre_es['Document_Content'][293][4].replace('Objetivos y Justificación de la CT', 'Descripción de las actividades y resultados')\n",
    "df_pre_es.at[293, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idem for document `PE-T1408_Approval Document - PE-T1408_15Oct2018-143108.pdf`\n",
    "lista_aux = df_pre_es['Document_Content'][698]\n",
    "lista_aux[4] = df_pre_es['Document_Content'][698][4].replace('Ill. Actividades', 'Ill. Descripción de las actividades y resultados')\n",
    "df_pre_es.at[698, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Documents with missing pages: \n",
    "    - `CH-T1197_Approval Document -CH-T1197.pdf` (index 202)  \n",
    "    - `CO-T1439_Approval Document - CO-T1439 .pdf` (index 230)\n",
    "#### Pending decision - meanwhile is removed from df_pre_es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.drop([202], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.drop([230], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31l]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][301][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace specific titles for the following indexes: \n",
    "# 301\n",
    "lista_aux = df_pre_es['Document_Content'][301]\n",
    "lista_aux[1] = df_pre_es['Document_Content'][301][1].replace('II. Objetivo', 'II. Objetivo y justificación')\n",
    "df_pre_es.at[301, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(pattern_es_1, df_pre_es['Document_Content'][301][1], re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Descripción de las actividades y resultados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pattern_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_2 = r'\\n?\\s?\\n?\\s?[IV\\.\\,ll]+\\s+(Descripción (de )((las)?\\s?actividades\\s?|los\\s)?((\\/|\\,\\s)?Componentes y (Resultados|Actividades)|\\/?\\s?componentes3? y presupuesto(\\:|\\.)?|\\sy resultados|\\sdel proyecto|\\sy presupuesto|\\, componentes y presupuesto\\.?|componentes\\s?(\\,|\\/)\\s?actividades y (productos|presupuesto|resultados)|actividades y productos|\\, componentes\\, resultados y presupuesto| y presupuesto\\.?|y Resultados|\\, resultados y presupuesto|\\, los componentes y el presupuesto))|(Actividades\\/componentes y presupuesto)|(Actividades y Componentes)|(Descripción de componentes\\/actividades y presupuesto)|(Descripción de las actividades)|(Descripción de los objetivos actividades y presupuesto)|(Descripción de componentes y productos)|(Descripción Actividades y Resultados)\\s{0,}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "results_in_page_0 = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_2, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            if i == 0:\n",
    "                results_in_page_0.append(index)\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)\n",
    "print('Results in page 0:', results_in_page_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][884][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo Ejecutor|Unidad Ejecutora|Entidad Ejecutora) y estructura de ejecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_pre_es['title_inicial'] = ''\n",
    "df_pre_es['title_medio'] = ''\n",
    "df_pre_es['title_final'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_es['Document_Content'][index][i], re.IGNORECASE).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_es.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.title_inicial[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['title_inicial'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_2, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            medio_match_title = re.search(pattern_es_2, df_pre_es['Document_Content'][index][i], re.IGNORECASE).group()\n",
    "            medio_match_page = i\n",
    "            df_pre_es.at[index, 'title_medio'] = (medio_match_title, medio_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['title_medio'][index][1] # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i], re.IGNORECASE) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_es['Document_Content'][index][i], re.IGNORECASE).group()\n",
    "            final_match_page = i\n",
    "            df_pre_es.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "other_case = []\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    if (df_pre_es.title_inicial[index][1] < df_pre_es.title_medio[index][1] < df_pre_es.title_final[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_pre_es.title_final[index][1]> df_pre_es.title_inicial[index][1] > df_pre_es.title_medio[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        other_case.append(index)\n",
    "        \n",
    "    if (df_pre_es.title_final[index][1] - df_pre_es.title_inicial[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "        print('File to check due to extension between titles:', df_pre_es['Document_Name'][index])\n",
    "        print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 170\n",
    "print(df_pre_es['Document_Name'][index])\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed from df_pre_es: \n",
    "- `BR-T1408_Approval Document-BR-T1408_18Nov2019.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.drop([170], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 321\n",
    "print(df_pre_es['Document_Name'][index])\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1377\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1338\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1328\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 984\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 777\n",
    "#max([df_pre_es.title_inicial[index][1],df_pre_es.title_medio[index][1],df_pre_es.title_final[index][1]])\n",
    "print((df_pre_es.title_inicial[index][0], df_pre_es.title_inicial[index][1]), (df_pre_es.title_medio[index][0], df_pre_es.title_medio[index][1]), (df_pre_es.title_final[index][0], df_pre_es.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### footer and header clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_pre_es['extracted'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean-up routine (v1.0)\n",
    "#for index in [30]:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    \n",
    "    print('### Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_pre_es['Document_Content'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}\\d\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    ini = re.search(pattern_es_1, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "\n",
    "    if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "        fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "        \n",
    "    else:   # search for pattern_3, as border condition\n",
    "        fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    #fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    texto = texto[ini:fin].strip()\n",
    "    #print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_pre_es.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store results as v0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pre_es['extracted'][1359])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_es['extracted_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_es.iterrows():\n",
    "    texto = df_pre_es['extracted'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_es.at[index, 'extracted_cleaned'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_es.extracted_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(store as v0.7 - pending convert 6 to ó)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name',\n",
    "       'Document_Status', 'blank_pages', 'page_count',\n",
    "       'doc_identifier', 'title_inicial',\n",
    "       'title_medio', 'title_final', 'extracted', 'extracted_cleaned']].to_excel('TCs_Approval-Docs_ES_Processing_2020-07-14_v07.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_es.Document_Content[171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.Document_Name[171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.loc[[171]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.title_inicial[171], df_pre_es.title_medio[171], df_pre_es.title_final[171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for index in other_case:\n",
    "    print(str(index))\n",
    "    print(df_pre_es.title_inicial[index], df_pre_es.title_medio[index], df_pre_es.title_final[index])\n",
    "    print('~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pre_es.Document_Content[673][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v0.8 - English TCs processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v0.4\n",
    "# Load source file:\n",
    "df_pre_full = joblib.load('./output/df_filtered_2_step_2_2020-07-11_v04.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v0.8 - Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "df_pre_full['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detection performed on the first 3 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_pre_full.iterrows():\n",
    "    if df_pre_full['doc_type'][index] == 'tc':\n",
    "        df_pre_full.at[index, 'language'] = detect(''.join(df_pre_full['Document_Content'][index][:3])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        df_pre_full.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_full.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "After inspection and titles search, some documents were found that their language was different from the expected.\n",
    "The following indexes were manually changed into 'en'\n",
    "909, 920, 1045 and 1105\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually changed language\n",
    "df_pre_full.at[909, 'language'] = 'en'\n",
    "df_pre_full.at[920, 'language'] = 'en'\n",
    "df_pre_full.at[1045, 'language'] = 'en'\n",
    "df_pre_full.at[1105, 'language'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en = df_pre_full[df_pre_full['language'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Manually removed:\n",
    "<br>index 56 - 'BH-T1059_Approval Document - BH-T1059.pdf'\n",
    "<br>index 65 - 'BH-T1074_Approval Document-BH-T1074 _05Sep2019-104741.pdf'\n",
    "<br> and indexes: [947, 948, 949, 1311] related to hurricane assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.drop([56], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.drop([65], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [947, 948, 949, 1311]:\n",
    "    df_pre_en.drop([i], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objectives and Justification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31l]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]) != None: #, re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Description of activities/components and budget | Description of Activities and Outputs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pattern_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_en_2 = r'\\n?\\s?\\n?\\s?[IV\\.\\·\\,ll3]+\\s+(Description of (the )?[aA]ctivities\\/[cC]omponents?( and [bB]udget)?|Description of components and budget|DESCRIPTION OF COMPONENTS AND BUDGET|DESCRIPTION OF ACTIVITIES\\/COMPONENTS AND BUDGET|Description of Activities and Budget|Description of activities\\, components and budget|Description of activity/component and budget|DESCRIPTION OF ACTIVITIES( AND OUTPUTS)?|Description of (A|a)ctivities and (O|o)utputs|Description of components and activities|Description of activities \\/ components and budget|Description of activities\\/components|Description of components\\/activities and budget|Description of Activities\\/ Components and Budget|Activity and Results Description|Description of Components and Activities|Description of activities and results|Description of activities\\, outputs and budget|Description of Activities \\/ component and budget|Description of Components\\, Activities and Budget|Description of Activities \\/ Components and Budget|Description of activities\\/ components and budget|Description of Activities\\/Outputs and Budget)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "results_in_page_0 = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_2, df_pre_en['Document_Content'][index][i]) != None: #, re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            if i == 0:\n",
    "                results_in_page_0.append(index)\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)\n",
    "print('Results in page 0:', results_in_page_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Executing agency and execution structure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace specific titles for the following indexes: \n",
    "# 121\n",
    "lista_aux = df_pre_en['Document_Content'][121]\n",
    "lista_aux[6] = df_pre_en['Document_Content'][121][6].replace('V. DESCRIPTION OF ACTIVITIES/COMPONENTS AND BUDGET', 'V. EXECUTING AGENCY')\n",
    "df_pre_en.at[121, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace specific titles for the following indexes: \n",
    "# 1164\n",
    "lista_aux = df_pre_en['Document_Content'][1164]\n",
    "lista_aux[6] = df_pre_en['Document_Content'][1164][6].replace('IV. Budget', 'IV. EXECUTING AGENCY')\n",
    "df_pre_en.at[1164, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]) != None: #, re.IGNORECASE) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_pre_en['title_inicial'] = ''\n",
    "df_pre_en['title_medio'] = ''\n",
    "df_pre_en['title_final'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_en.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.title_inicial[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['title_inicial'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_2, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            medio_match_title = re.search(pattern_en_2, df_pre_en['Document_Content'][index][i]).group()\n",
    "            medio_match_page = i\n",
    "            df_pre_en.at[index, 'title_medio'] = (medio_match_title, medio_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['title_medio'][index][1] # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_en.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_case = []\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    if (df_pre_en.title_inicial[index][1] < df_pre_en.title_medio[index][1] < df_pre_en.title_final[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_pre_en.title_final[index][1]> df_pre_en.title_inicial[index][1] > df_pre_en.title_medio[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        other_case.append(index)\n",
    "        \n",
    "    if (df_pre_en.title_final[index][1] - df_pre_en.title_inicial[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "        print('File to check due to extension between titles:', df_pre_en['Document_Name'][index])\n",
    "        print((df_pre_en.title_inicial[index][0], df_pre_en.title_inicial[index][1]), (df_pre_en.title_medio[index][0], df_pre_en.title_medio[index][1]), (df_pre_en.title_final[index][0], df_pre_en.title_final[index][1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 81\n",
    "#max([df_pre_en.title_inicial[index][1],df_pre_en.title_medio[index][1],df_pre_en.title_final[index][1]])\n",
    "print((df_pre_en.title_inicial[index][0], df_pre_en.title_inicial[index][1]), (df_pre_en.title_medio[index][0], df_pre_en.title_medio[index][1]), (df_pre_en.title_final[index][0], df_pre_en.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1294\n",
    "#max([df_pre_en.title_inicial[index][1],df_pre_en.title_medio[index][1],df_pre_en.title_final[index][1]])\n",
    "print((df_pre_en.title_inicial[index][0], df_pre_en.title_inicial[index][1]), (df_pre_en.title_medio[index][0], df_pre_en.title_medio[index][1]), (df_pre_en.title_final[index][0], df_pre_en.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### footer and header clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_pre_en['extracted'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean-up routine (v1.0)\n",
    "#for index in [30]:\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    page_ini = df_pre_en.title_inicial[index][1]\n",
    "    page_fin = df_pre_en.title_final[index][1]\n",
    "    \n",
    "    print('### Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_pre_en['Document_Content'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}\\d\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    if re.search(pattern_en_1, texto).span()[0] != None:\n",
    "        ini = re.search(pattern_en_1, texto).span()[0]\n",
    "    \n",
    "    # alternatively:\n",
    "    else:\n",
    "        ini = re.search(df_pre_en['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    ##fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    ##texto = texto[ini:fin].strip()[:-3]\n",
    "    ##print(texto)\n",
    "    \n",
    "\n",
    "    if re.search(r'Indicative (B|b)udget', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "        fin = re.search(r'Indicative (B|b)udget', texto).span()[0]\n",
    "        \n",
    "    else:   # search for pattern_3, as border condition\n",
    "        fin = re.search(pattern_en_3, texto).span()[0]\n",
    "    \n",
    "    #fin = re.search(pattern_en_3, texto, re.IGNORECASE).span()[0]\n",
    "    texto = texto[ini:fin].strip()\n",
    "    #print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_pre_en.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store results as v0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pre_en['extracted'][177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_en['extracted_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_en.iterrows():\n",
    "    texto = df_pre_en['extracted'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_en.at[index, 'extracted_cleaned'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_en.extracted_cleaned[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name',\n",
    "       'Document_Status', 'blank_pages', 'page_count',\n",
    "       'doc_identifier', 'title_inicial',\n",
    "       'title_medio', 'title_final', 'extracted', 'extracted_cleaned']].to_excel('TCs_Approval-Docs_EN_Processing_2020-08-23_v09.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.0: Spanish language TCs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v0.7\n",
    "# Load source file:\n",
    "df_pre_es = joblib.load('./output/df_pre_es_2020-07-14_v07.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][171][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustment:\n",
    "aux = df_pre_es['Document_Content'][171]\n",
    "#removal of 3rd element since it is an index page\n",
    "aux = aux[:2] + aux[4:]\n",
    "#\n",
    "df_pre_es.at[171, 'Document_Content'] = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of initial title for document on 171: \n",
    "lista_aux = df_pre_es['Document_Content'][171]\n",
    "lista_aux[3] = df_pre_es['Document_Content'][171][3].replace('II. OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA  ', 'II. Objetivos y justificación de la Cooperación Técnica')\n",
    "df_pre_es.at[171, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][171][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.at[171, 'doc_identifier'] = ('DOCUMENTO DE COOPERACIÓN TÉCNICA', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "#pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVO Y JUSTIFICACIÓN DE LA CT|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|JUSTIFICACIÓN Y OBJETIVOS DE LA CT|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|Objetivo y justificación|Objetivos y justificación|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO|Descripción del préstamo\\/garantía asociado)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_es_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agencia Ejecutora y Estructura de Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es[df_pre_es.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-doing for spanish docs title_inicial and title_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_es.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es[df_pre_es.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_es['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_es.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_es['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Name'][96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Name'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "index = 223\n",
    "lista_pages = []\n",
    "page_ini = df_pre_es.title_inicial[index][1]\n",
    "page_fin = df_pre_es.title_final[index][1]\n",
    "print(page_fin - page_ini)\n",
    "if (page_fin - page_ini) < 2: \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "    #lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_final'][index][1]][:re.search(df_pre_es['title_final'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_final'][index][1]]).span()[1]])\n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "else: \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "    for j in range(page_ini+1,page_fin-1): \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][j])\n",
    "    \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "\n",
    "    \n",
    "    \n",
    "#page_inicial\n",
    "#print(df_pre_es['title_inicial'][index][1])\n",
    "#df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:]\n",
    "print('length: ', str(len(lista_pages)))\n",
    "for k in range(0, len(lista_pages)):\n",
    "    print(lista_pages[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile(df_pre_es['title_final'][index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(re.compile(df_pre_es['title_final'][index][0]), df_pre_es['Document_Content'][index][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6][:df_pre_es['Document_Content'][index][6].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6][:351]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"        \", str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_es.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_es['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_es.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pre_es.title_inicial[index], df_pre_es.title_final[index])\n",
    "#print(df_pre_es['lista_paginas'][index])\n",
    "#page_ini = df_pre_es.title_inicial[index][1]\n",
    "#page_fin = df_pre_es.title_final[index][1]\n",
    "#print(str(page_fin - page_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pre_es['Document_Content'][index][3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1355\n",
    "#index = 1\n",
    "for index in [index]:\n",
    "    print(df_pre_es['lista_paginas'][index])\n",
    "    longitud = len(df_pre_es['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_es['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.2 - 08/30/2020)\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    #print(df_pre_es['lista_paginas'][index])\n",
    "    longitud = len(df_pre_es['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_es['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_es.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    #if (len(df_pre_es['extracted'][index]) < len(df_pre_es['extracted_v2'][index])):\n",
    "        #print(True, str((len(df_pre_es['extracted_v2'][index]) - len(df_pre_es['extracted'][index]))))\n",
    "    #    print()\n",
    "    if not (len(df_pre_es['extracted'][index]) < len(df_pre_es['extracted_v2'][index])):\n",
    "        print(\"!!!! Alert on\", str(index), str(len(df_pre_es['extracted'][index]) - len(df_pre_es['extracted_v2'][index])))\n",
    "        count += 1\n",
    "print(str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('''II. Descripción del Préstamo/Garantía Asociado \n",
    "2.1 La cooperación técnica dará apoyo operativo a los Programas de Innovación \n",
    "\n",
    "\n",
    "Empresarial y Emprendimiento I (UR-L1142) y II (UR-L1158), los cuales son \n",
    "ejecutados por la Agencia Nacional de Investigación e Innovación (ANII) y financiados \n",
    "por el Banco Interamericano de Desarrollo (BID). Estos programas forman parte de la \n",
    "Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) aprobada en 2017 \n",
    "por un monto de US$100 millones (UR-O1153). El objetivo de la CCLIP es \n",
    "incrementar la productividad de las empresas mediante una mayor inversión en \n",
    "conocimiento, recursos humanos, innovación y emprendimiento. En particular, la \n",
    "cooperación técnica, apoyará los procesos de monitoreo y evaluación en los \n",
    "mencionados programas, así como también contribuirá a mejorar sus niveles de \n",
    "trasparencia, mediante una difusión automática, a través del desarrollo de un portal \n",
    "que pondrá a disposición de los ciudadanos datos abiertos sobre beneficiarios \n",
    "atendidos y los apoyos realizados.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pre_es['extracted'][1354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pre_es['extracted_v2'][1354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['extracted_final'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up routine (v2.01 - spanish - 2020-08-27)\n",
    "##\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    \n",
    "    print('### Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_pre_es['Document_Content'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    if df_pre_es['language'][index] == 'en':  # English\n",
    "        if re.search(pattern_en_1, texto).span()[0] != None:\n",
    "            ini = re.search(pattern_en_1, texto).span()[0]\n",
    "    \n",
    "        # alternatively:\n",
    "        else:\n",
    "            ini = re.search(df_pre_es['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "    else: # Spanish\n",
    "        if re.search(pattern_es_1, texto).span()[0] != None:\n",
    "            ini = re.search(pattern_es_1, texto).span()[0]\n",
    "    \n",
    "        # alternatively:\n",
    "        else:\n",
    "            ini = re.search(df_pre_es['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    ##fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    ##texto = texto[ini:fin].strip()[:-3]\n",
    "    ##print(texto)\n",
    "    \n",
    "    #Presupuesto Indicativo\n",
    "\n",
    "    if re.search(r'(Indicative [Bb]udget)|(Presupuesto [Ii]ndicativo)', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "        fin = re.search(r'(Indicative [Bb]udget)|(Presupuesto [Ii]ndicativo)', texto).span()[0]\n",
    "        \n",
    "    else:   # search for pattern_3, as border condition\n",
    "        if df_pre_es['language'][index] == 'en':  # English\n",
    "            fin = re.search(pattern_en_3, texto).span()[0]\n",
    "        else:\n",
    "            fin = re.search(pattern_es_3, texto).span()[0]\n",
    "    \n",
    "    #fin = re.search(pattern_en_3, texto, re.IGNORECASE).span()[0]\n",
    "    texto = texto[ini:fin].strip()\n",
    "    #print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_pre_es.at[index, 'extracted_final'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1.0 - NULL_URL TCs processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v0.5\n",
    "# Load source file:\n",
    "df_pre_null = joblib.load('./output/TCs_Approval-NULL_URL-Doc_Collection_2020-07-14_v05_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null = df_pre_null[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'Document_Status']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null['Document_Content'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_NULLs\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the documents and store the content in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in df_pre_null.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + df_pre_null.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        df_pre_null.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        df_pre_null.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the ones not downloaded:\n",
    "df_pre_null.drop([12, 22], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pre_null.drop([\"language\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null['blank_pages'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_null.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = df_pre_null['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    df_pre_null.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null['page_count'] = df_pre_null['Document_Content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_1\n",
    "\n",
    "######  filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents longer than 6 pages lenght and less than 60% of blank pages:\n",
    "df_filtered_null = df_pre_null[(~df_pre_null['Document_Name'].str.contains('Approval Document - GA-274-1')) & (df_pre_null.page_count > 6) & (df_pre_null.blank_pages.astype(float) < 60)].copy()\n",
    "df_filtered_null.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_2 \n",
    "\n",
    "######  classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above filtering decisions, a second filtering level is performed where TC's components -such as common titles- are reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_null_2 = df_filtered_null.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the type of document: tc's, other\n",
    "df_filtered_null_2['doc_type'] = ''\n",
    "# stores the matching title that defines the type and its page\n",
    "df_filtered_null_2['doc_identifier'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_review = []\n",
    "tc_count = 0\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    is_tc = False\n",
    "    for page in range(0,len(df_filtered_null_2.Document_Content[index])):\n",
    "        if re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|TC DOCUMENT|TECHNICAL COOPERATION DOCUMENT|TC Name', df_filtered_null_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('TC header found at page:', str(page))\n",
    "            tc_count += 1\n",
    "            is_tc = True\n",
    "            df_filtered_null_2.at[index, 'doc_type'] = 'tc'\n",
    "            match_title_type = re.search(r'DOCUMENTO DE COOPERACIÓN TÉCNICA|INFORMACIÓN BÁSICA DE LA CT|Información Básica de la (CT|Cooperación Técnica)|lnformaci6n Basica de la Cooperaci6n Tecnica|TC DOCUMENT|TECHNICAL COOPERATION DOCUMENT|TC Name', df_filtered_null_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_null_2.at[index, 'doc_identifier'] = (match_title_type, page)\n",
    "            break\n",
    "    if not is_tc: \n",
    "        print('check regex on:', str(index))\n",
    "        df_filtered_null_2.at[index, 'doc_type'] = 'other'\n",
    "        df_filtered_null_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('TCs identified:', str(tc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1.0 - Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores language identified on the doc_identifier page\n",
    "df_filtered_null_2['language'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Detection performed on the first 3 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    if df_filtered_null_2['doc_type'][index] == 'tc':\n",
    "        df_filtered_null_2.at[index, 'language'] = detect(''.join(df_filtered_null_2['Document_Content'][index][:3])) # run language detection on Document_Content[page]\n",
    "    else:\n",
    "        df_filtered_null_2.at[index, 'language'] = 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2[df_filtered_null_2.language == 'es'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL_URL - Titles search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31l]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31l]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English documents:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Descripción de las actividades y resultados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pattern_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_2 = r'\\n?\\s?\\n?\\s?[IV\\.\\,ll]+\\s+(Descripción (de )((las)?\\s?actividades\\s?|los\\s)?((\\/|\\,\\s)?Componentes y (Resultados|Actividades)|\\/?\\s?componentes3? y presupuesto(\\:|\\.)?|\\sy resultados|\\sdel proyecto|\\sy presupuesto|\\, componentes y presupuesto\\.?|componentes\\s?(\\,|\\/)\\s?actividades y (productos|presupuesto|resultados)|actividades y productos|\\, componentes\\, resultados y presupuesto| y presupuesto\\.?|y Resultados|\\, resultados y presupuesto|\\, los componentes y el presupuesto))|Descripción de las Actividades\\/Componentes y Presupuesto|(Actividades\\/componentes y presupuesto)|(Actividades y Componentes)|DESCRIPCIÓN DE LAS ACTIVIDADES, LOS COMPONENTES Y EL PRESUPUESTO|(Descripción de componentes\\/actividades y presupuesto)|(Descripción de las actividades)|(Descripción de los objetivos actividades y presupuesto)|Descripción de las Actividades\\, Componentes y Presupuesto|(Descripción de componentes y productos)|(Descripción Actividades y Resultados)|DESCRIPCIÓN DE LAS ACTIVIDADES[\\,\\/]\\s?COMPONENTES Y PRESUPUESTO|DESCRIPCIÓN DE ACTIVIDADES\\/COMPONENTES Y PRESUPUESTO6\\s{0,}\\n'\n",
    "pattern_en_2 = r'\\n?\\s?\\n?\\s?[IV\\.\\·\\,ll3]+\\s+(Description of (the )?[aA]ctivities\\/[cC]omponents?( and [bB]udget)?|Description of components and budget|DESCRIPTION OF COMPONENTS AND BUDGET|DESCRIPTION OF ACTIVITIES\\/COMPONENTS AND BUDGET|Description of Activities and Budget|Description of activities\\, components and budget|Description of activity/component and budget|DESCRIPTION OF ACTIVITIES( AND OUTPUTS)?|Description of (A|a)ctivities and (O|o)utputs|Description of components and activities|Description of activities \\/ components and budget|Description of activities\\/components|Description of components\\/activities and budget|Description of Activities\\/ Components and Budget|Activity and Results Description|Description of Components and Activities|Description of activities and results|Description of activities\\, outputs and budget|Description of Activities \\/ component and budget|Description of Components\\, Activities and Budget|Description of Activities \\/ Components and Budget|Description of activities\\/ components and budget|Description of Activities\\/Outputs and Budget|ACTIVITY\\/COMPONENT DESCRIPTION AND BUDGET)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish documents:\n",
    "index_es_to_check = []\n",
    "results_in_page_0 = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_2, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            if i == 0:\n",
    "                results_in_page_0.append(index)\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)\n",
    "print('Results in page 0:', results_in_page_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English documents:\n",
    "index_en_to_check = []\n",
    "results_in_page_0 = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_2, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            if i == 0:\n",
    "                results_in_page_0.append(index)\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)\n",
    "print('Results in page 0:', results_in_page_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo Ejecutor|Unidad Ejecutora|Entidad Ejecutora) y estructura de ejecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|Agencia ejecutora y estructura de ejecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL_URL - Titles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_null_2['title_inicial'] = ''\n",
    "df_filtered_null_2['title_medio'] = ''\n",
    "df_filtered_null_2['title_final'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['title_inicial'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_2, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            medio_match_title = re.search(pattern_es_2, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            medio_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_medio'] = (medio_match_title, medio_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['title_inicial'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_2, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            medio_match_title = re.search(pattern_en_2, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            medio_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_medio'] = (medio_match_title, medio_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['title_medio'][index][1] # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_filtered_null_2[df_filtered_null_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_null_2['title_medio'][index][1] # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_filtered_null_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_filtered_null_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_filtered_null_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_filtered_null_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_case = []\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    if (df_filtered_null_2.title_inicial[index][1] < df_filtered_null_2.title_medio[index][1] < df_filtered_null_2.title_final[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_filtered_null_2.title_final[index][1]> df_filtered_null_2.title_inicial[index][1] > df_filtered_null_2.title_medio[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        other_case.append(index)\n",
    "        \n",
    "    if (df_filtered_null_2.title_final[index][1] - df_filtered_null_2.title_inicial[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "        print('File to check due to extension between titles:', df_filtered_null_2['Document_Name'][index])\n",
    "        print((df_filtered_null_2.title_inicial[index][0], df_filtered_null_2.title_inicial[index][1]), (df_filtered_null_2.title_medio[index][0], df_filtered_null_2.title_medio[index][1]), (df_filtered_null_2.title_final[index][0], df_filtered_null_2.title_final[index][1]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "#max([df_filtered_null_2.title_inicial[index][1],df_filtered_null_2.title_medio[index][1],df_filtered_null_2.title_final[index][1]])\n",
    "print((df_filtered_null_2.title_inicial[index][0], df_filtered_null_2.title_inicial[index][1]), (df_filtered_null_2.title_medio[index][0], df_filtered_null_2.title_medio[index][1]), (df_filtered_null_2.title_final[index][0], df_filtered_null_2.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 18\n",
    "#max([df_filtered_null_2.title_inicial[index][1],df_filtered_null_2.title_medio[index][1],df_filtered_null_2.title_final[index][1]])\n",
    "print((df_filtered_null_2.title_inicial[index][0], df_filtered_null_2.title_inicial[index][1]), (df_filtered_null_2.title_medio[index][0], df_filtered_null_2.title_medio[index][1]), (df_filtered_null_2.title_final[index][0], df_filtered_null_2.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered_null_2['Document_Content'][18][3]\n",
    "df_filtered_null_2['Document_Name'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "#max([df_filtered_null_2.title_inicial[index][1],df_filtered_null_2.title_medio[index][1],df_filtered_null_2.title_final[index][1]])\n",
    "print((df_filtered_null_2.title_inicial[index][0], df_filtered_null_2.title_inicial[index][1]), (df_filtered_null_2.title_medio[index][0], df_filtered_null_2.title_medio[index][1]), (df_filtered_null_2.title_final[index][0], df_filtered_null_2.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2['Document_Name'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsitute page 3 on documents index 18 and 25 with a blank_page:\n",
    "df_filtered_null_2['Document_Content'][18][3] = ''\n",
    "df_filtered_null_2['Document_Content'][25][3] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### footer and header clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_filtered_null_2['extracted'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean-up routine (v2.0 - multilingual approach - 2020-08-25)\n",
    "##\n",
    "\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    page_ini = df_filtered_null_2.title_inicial[index][1]\n",
    "    page_fin = df_filtered_null_2.title_final[index][1]\n",
    "    \n",
    "    print('### Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_filtered_null_2['Document_Content'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    if df_filtered_null_2['language'][index] == 'en':  # English\n",
    "        if re.search(pattern_en_1, texto).span()[0] != None:\n",
    "            ini = re.search(pattern_en_1, texto).span()[0]\n",
    "    \n",
    "        # alternatively:\n",
    "        else:\n",
    "            ini = re.search(df_filtered_null_2['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "    else: # Spanish\n",
    "        if re.search(pattern_es_1, texto).span()[0] != None:\n",
    "            ini = re.search(pattern_es_1, texto).span()[0]\n",
    "    \n",
    "        # alternatively:\n",
    "        else:\n",
    "            ini = re.search(df_filtered_null_2['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    ##fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    ##texto = texto[ini:fin].strip()[:-3]\n",
    "    ##print(texto)\n",
    "    \n",
    "    #Presupuesto Indicativo\n",
    "\n",
    "    if re.search(r'(Indicative [Bb]udget)|(Presupuesto [Ii]ndicativo)', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "        fin = re.search(r'(Indicative [Bb]udget)|(Presupuesto [Ii]ndicativo)', texto).span()[0]\n",
    "        \n",
    "    else:   # search for pattern_3, as border condition\n",
    "        if df_filtered_null_2['language'][index] == 'en':  # English\n",
    "            fin = re.search(pattern_en_3, texto).span()[0]\n",
    "        else:\n",
    "            fin = re.search(pattern_es_3, texto).span()[0]\n",
    "    \n",
    "    #fin = re.search(pattern_en_3, texto, re.IGNORECASE).span()[0]\n",
    "    texto = texto[ini:fin].strip()\n",
    "    #print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_filtered_null_2.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store results as v0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(df_filtered_null_2['title_inicial'][32][0][:-1], texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2['extracted'][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2['Document_Name'][43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2['title_inicial'][43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\n\\n\\nII. Objectives and Justification of the TC  \\n\\n\\n'.find('TC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_null_2['Document_Content'][43][df_filtered_null_2['title_inicial'][43][1]][re.search(df_filtered_null_2['title_inicial'][43][0], df_filtered_null_2['Document_Content'][43][df_filtered_null_2['title_inicial'][43][1]]).span()[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Title\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    print(\"processing index\", str(index))\n",
    "    print(df_filtered_null_2['Document_Content'][index][df_filtered_null_2['title_inicial'][index][1]][re.search(df_filtered_null_2['title_inicial'][index][0], df_filtered_null_2['Document_Content'][index][df_filtered_null_2['title_inicial'][index][1]]).span()[0]:])\n",
    "    print('***')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Title\n",
    "for index, row in df_filtered_null_2.iterrows():\n",
    "    print(\"processing index\", str(index))\n",
    "    print(df_filtered_null_2['Document_Content'][index][df_filtered_null_2['title_final'][index][1]][:re.search(df_filtered_null_2['title_final'][index][0], df_filtered_null_2['Document_Content'][index][df_filtered_null_2['title_final'][index][1]]).span()[0]])\n",
    "    print('***')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pre_en['extracted'][177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_en['extracted_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_en.iterrows():\n",
    "    texto = df_pre_en['extracted'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_en.at[index, 'extracted_cleaned'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_en.extracted_cleaned[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name',\n",
    "       'Document_Status', 'blank_pages', 'page_count',\n",
    "       'doc_identifier', 'title_inicial',\n",
    "       'title_medio', 'title_final', 'extracted', 'extracted_cleaned']].to_excel('TCs_Approval-Docs_EN_Processing_2020-08-23_v09.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 25\n",
    "#max([df_filtered_null_2.title_inicial[index][1],df_filtered_null_2.title_medio[index][1],df_filtered_null_2.title_final[index][1]])\n",
    "print((df_filtered_null_2.title_inicial[index][0], df_filtered_null_2.title_inicial[index][1]), (df_filtered_null_2.title_medio[index][0], df_filtered_null_2.title_medio[index][1]), (df_filtered_null_2.title_final[index][0], df_filtered_null_2.title_final[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the above cases, the index page identified is removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### footer and header clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_pre_en['extracted'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean-up routine (v1.0)\n",
    "#for index in [30]:\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    page_ini = df_pre_en.title_inicial[index][1]\n",
    "    page_fin = df_pre_en.title_final[index][1]\n",
    "    \n",
    "    print('### Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_pre_en['Document_Content'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}\\d\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    if re.search(pattern_en_1, texto).span()[0] != None:\n",
    "        ini = re.search(pattern_en_1, texto).span()[0]\n",
    "    \n",
    "    # alternatively:\n",
    "    else:\n",
    "        ini = re.search(df_pre_en['title_inicial'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    ##fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    ##texto = texto[ini:fin].strip()[:-3]\n",
    "    ##print(texto)\n",
    "    \n",
    "\n",
    "    if re.search(r'Indicative (B|b)udget', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "        fin = re.search(r'Indicative (B|b)udget', texto).span()[0]\n",
    "        \n",
    "    else:   # search for pattern_3, as border condition\n",
    "        fin = re.search(pattern_en_3, texto).span()[0]\n",
    "    \n",
    "    #fin = re.search(pattern_en_3, texto, re.IGNORECASE).span()[0]\n",
    "    texto = texto[ini:fin].strip()\n",
    "    #print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_pre_en.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store results as v0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pre_en['extracted'][177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_en['extracted_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_en.iterrows():\n",
    "    texto = df_pre_en['extracted'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_en.at[index, 'extracted_cleaned'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_en.extracted_cleaned[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name',\n",
    "       'Document_Status', 'blank_pages', 'page_count',\n",
    "       'doc_identifier', 'title_inicial',\n",
    "       'title_medio', 'title_final', 'extracted', 'extracted_cleaned']].to_excel('TCs_Approval-Docs_EN_Processing_2020-08-23_v09.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# 1) remove headers\n",
    "# 2) remove footers (different types)\n",
    "# 3) remove supraindexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Area -- NOT TO BE USED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy all files to another folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "#path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCR location:\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\PCR\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to store the result:\n",
    "df['Status'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy each doc, with name stored in the dataframe, to another folder:\n",
    "for index, row in df.iterrows():\n",
    "    file = df.Document_Name[index]\n",
    "    filename = file_dir + '\\\\' + file\n",
    "    if path.exists(filename):\n",
    "        try: \n",
    "            shutil.copyfile(filename, \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\PCR_\" + '\\\\' + file)\n",
    "            df.at[index, 'Status'] = 'Copied'\n",
    "        except Exception as e:\n",
    "            df.at[index, 'Status'] = 'Not copied'\n",
    "    else:\n",
    "        df.at[index, 'Status'] = 'File not exists'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 1.2           01/12/2021        Emiliano Colina    - New TCs from November to December 2020\n",
    "#\n",
    "#   - 1.1           10/16/2020        Emiliano Colina    - New TCs from July to September 2020 are processed\n",
    "#\n",
    "#   - 1.0           08/24/2020        Emiliano Colina    - TCs with NULL_URL processing\n",
    "#\n",
    "#   - 0.9           08/23/2020        Emiliano Colina    - Documents extracted and cleaned\n",
    "#\n",
    "#   - 0.8           08/22/2020        Emiliano Colina    - English documents processing\n",
    "#\n",
    "#   - 0.7           07/14/2020        Emiliano Colina    - Supra-indexes removed from content extracted\n",
    "#                                                        - Multiple spacing also cleaned in the process\n",
    "\n",
    "#   - 0.6           07/14/2020        Emiliano Colina    - Content extracted\n",
    "#                                                        - Headers and footnotes cleaned up on selected Spanish \n",
    "#                                                        documents\n",
    "\n",
    "#   - 0.5           07/12/2020        Emiliano Colina    - Language detection implemented, worked w/Spanish docs\n",
    "#                                                        - Filtering and processing logic defined and implemented\n",
    "#                                                        using sections' titles and their respective locations\n",
    "\n",
    "#   - 0.4           07/09/2020        Emiliano Colina    - Added docx documents as pdf (converted w/MS-Word)\n",
    "#                                                        - Count blank pages as %\n",
    "#                                                        - Distribution analysis and filtering\n",
    "                                                        \n",
    "#   - 0.3           07/08/2020        Emiliano Colina    - Using new document sources\n",
    "\n",
    "#   - 0.2           06/29/2020        Emiliano Colina    - Filtered TC and other types of documents using regex\n",
    "                                                                                                                  \n",
    "#   - 0.1           06/23/2020        Emiliano Colina    - Initial version, starting with 'Approval Registry'    \n",
    "#                                                        - Description type of documents                         \n",
    "\n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
