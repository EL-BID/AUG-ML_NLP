{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.0 - Document Processing Procurement_Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 02 - Document Processing - Procurement_Plans\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): xlsx, doc, pdf files\n",
    "#-- Description:  \n",
    "#                - Procurement Plans, multiple types of documents: xlsx, doc, pdf\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  2.1\n",
    "#-- Last Update: 11/18/2020\n",
    "#-- Last Revision Date: 10/28/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.1: Procurement_Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_pre_2 = pd.read_excel('./input/Data-30 Sep 2020-All documents - original.xlsx', sheet_name='duplicates_filtered')\n",
    "df_pre_2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df_pre_2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\procur_plans\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_pre_2[['Oper_type', 'OPERATION_ID', 'OPERATION_NUMBER', 'Country', 'Region',\n",
    "       'Sector', 'Sector_Subsector', 'OPERATION_NAME_ES', 'OPERATION_YEAR',\n",
    "       'APPROVAL_DATE', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION',\n",
    "       'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.Document_Content == 'not available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to adjust reading xlsx:\n",
    "data[data.Document_Content.str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read OK:\n",
    "data[(data.Document_Content.str.len() == 0) & ~(data.Document_Content == 'not available')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the following doc since it is not a Procurement Plan:\n",
    "data[data.OPERATION_NUMBER == 'UR-L1142']\n",
    "data.drop([261], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing excel files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel(file_xlsx):\n",
    "    '''\n",
    "    Reads all sheets from a procurement plan file in excel, keeping only str type of content\n",
    "    and returns a list.\n",
    "    Caveats: all hiden sheets will are read as well.\n",
    "    date: 11/01/2020\n",
    "    author: emilianoco@iadb.org\n",
    "    version: 0.1\n",
    "    '''\n",
    "    xls = pd.ExcelFile(file_xlsx)\n",
    "    #xls.sheet_names\n",
    "    lista_result = []\n",
    "    for i in range(0,len(xls.sheet_names)):\n",
    "        test = pd.read_excel(file_xlsx, sheet_name = xls.sheet_names[i])\n",
    "        lista_test = test.values.tolist()\n",
    "        flat_list = [item for sublist in lista_test for item in sublist] # flaten a list of lists\n",
    "        flat_list = [item for item in flat_list if type(item) == str]\n",
    "        lista_result.append(flat_list)\n",
    "        \n",
    "    lista_result = [item.strip() for sublist in lista_result for item in sublist] # flaten the result as well\n",
    "    return lista_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "def process_docx(file_docx):\n",
    "    '''\n",
    "    Reads all tables from a procurement plan file in ms-word, keeping only str type of content\n",
    "    and returns a list.\n",
    "    date: 11/01/2020\n",
    "    author: emilianoco@iadb.org\n",
    "    version: 0.1\n",
    "    '''\n",
    "    #from docx import Document\n",
    "    document = docx.Document(file_docx)\n",
    "    tables = []\n",
    "    for table in document.tables:\n",
    "        df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                if cell.text:\n",
    "                    df[i][j] = cell.text\n",
    "        tables.append(pd.DataFrame(df))\n",
    "    lista_result = []\n",
    "    for i in range(0,len(tables)):\n",
    "        lista_test = tables[i].values.tolist()\n",
    "        flat_list = [item for sublist in lista_test for item in sublist] # flaten a list of lists\n",
    "        flat_list = [item for item in flat_list if type(item) == str]\n",
    "        lista_result.append(flat_list)\n",
    "        \n",
    "    lista_result = [item.strip() for sublist in lista_result for item in sublist] # flaten the result as well\n",
    "    del tables\n",
    "    return lista_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_index = data[(data.Document_Content.str.len() == 0) & ~(data.Document_Content == 'not available')].index.tolist()\n",
    "for indice in file_index: \n",
    "    file_xlsx = file_dir + '/' + data.Document_Name[indice]\n",
    "    print('Index:', str(indice))\n",
    "    print('Processing file:', file_xlsx)\n",
    "    \n",
    "    if file_xlsx.endswith('xlsx') or file_xlsx.endswith('xls'):\n",
    "        data.at[indice, 'Document_Content'] = process_excel(file_xlsx)\n",
    "        \n",
    "    elif file_xlsx.endswith('docx'):\n",
    "        data.at[indice, 'Document_Content'] = process_docx(file_xlsx)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.Document_Content[145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/26521266/using-pandas-to-pd-read-excel-for-multiple-worksheets-of-the-same-workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Document_Content_2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check all pdf's:\n",
    "for index, row in data.iterrows():\n",
    "    if data['Document_Name'][index].endswith('pdf'):\n",
    "        print(index)\n",
    "        lista_res = []\n",
    "        for i in range(0, len(data['Document_Content'][index])):\n",
    "            lista_res.append(data['Document_Content'][index][i].lower().strip().split('\\n\\n\\n'))\n",
    "        flat_list = [item.strip().replace('\\n', '') for sublist in lista_res for item in sublist] # flaten a list of lists\n",
    "        data.at[index, 'Document_Content_2'] = flat_list\n",
    "        \n",
    "    else:\n",
    "        data.at[index, 'Document_Content_2'] = data['Document_Content'][index]\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Document_Content_2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Document_Content'][data[data.OPERATION_NUMBER == 'PE-L1229'].index.astype(int)[0]][4].lower().strip().split('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Store intermediate results as xlsx and joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to excel:\n",
    "data.to_excel('./output/procurement_plans.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1: all documents \n",
    "f_data = 'df_procurement_plans_2020-11-01_v21.joblib'\n",
    "joblib.dump(data, './output/' + f_data + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2.2: continuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous run\n",
    "data = joblib.load('./output/df_procurement_plans_2020-11-01_v21.joblib.bz2')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge list content and remove '/n'\n",
    "data['Document_Content_3'] = ''\n",
    "for index, row in data.iterrows():\n",
    "    data.at[index, 'Document_Content_3'] = '. '.join(data.Document_Content_2[index]).replace('/n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run language detection on 'Document_Content_3'\n",
    "data['language'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    data.at[index, 'language'] = detect(data['Document_Content_3'][index])\n",
    "\n",
    "data.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['language'] == 'ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stanza:\n",
    "import stanza\n",
    "from stanza import *\n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en', package='ewt', processors='tokenize,mwt,pos,lemma', verbose=True) \n",
    "stNLP_en = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='en', use_gpu=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('pt', package='bosque', processors='tokenize,mwt,pos,lemma', verbose=True) \n",
    "stNLP_pt = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='pt', use_gpu=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data_lemmatized'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "allowed_postags = ['PROPN', 'NOUN', 'ADJ']\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('processing index:', index)\n",
    "    if data['language'][index] == 'es':\n",
    "        test = stNLP(data['Document_Content_3'][index])\n",
    "        data.at[index, 'data_lemmatized'] = [word.lemma.lower() for sent in test.sentences for word in sent.words if word.pos in allowed_postags]\n",
    "        \n",
    "    elif data['language'][index] == 'en':\n",
    "        test = stNLP_en(data['Document_Content_3'][index])\n",
    "        data.at[index, 'data_lemmatized'] = [word.lemma.lower() for sent in test.sentences for word in sent.words if word.pos in allowed_postags]\n",
    "        \n",
    "    elif data['language'][index] == 'pt':\n",
    "        test = stNLP_pt(data['Document_Content_3'][index])\n",
    "        data.at[index, 'data_lemmatized'] = [word.lemma.lower() for sent in test.sentences for word in sent.words if word.pos in allowed_postags]\n",
    "        \n",
    "    else:\n",
    "        data.at[index, 'data_lemmatized'] = 'Not available'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Terms to search for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms_orig = ['web', 'Sistema', 'Informático ', 'Digital', 'Analítico ', 'Dato', 'Información', 'dato', 'Hardware', 'Software', 'Integración', 'Implementación', 'Desarrollo', 'Control', 'Tecnología', 'Interoperable', 'Automatización ', 'Aplicación', 'Aplicativo ', 'Protocolo', 'Plataforma', 'Electrónico', 'Servidor', \\\n",
    "                 'Biométrico', 'Internet', 'Big', 'data', 'Inteligencia',  'artificial', 'Red', 'Sitio', 'Licencia', 'Comunicaciones', 'Comunicación', 'Equipos', 'Equipo']\n",
    "keyterms_sp = [item.lower().strip() for item in keyterms_orig]\n",
    "#keyterms_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keyterms_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyterms_en = ['web', 'website', 'system', 'information', 'digital', 'analytical', 'data', 'information', 'data', 'hardware', 'software', 'integration', 'implementation', 'developing', 'control', 'technology', 'interoperable', 'automation', 'application', 'applicative', 'protocol', 'platform', 'electronic', 'server', 'biometric', 'internet', 'big', 'data', 'intelligence', 'artificial', 'network', 'site', 'license', 'communication', 'equipment']\n",
    "keyterms_pt = ['web', 'sistema', 'informático', 'digital', 'analítico', 'dado', 'informação', 'dados', 'hardware', 'software', 'integração', 'implementação', 'desenvolvimento', 'controle', 'tecnologia', 'interoperável', 'automação', 'aplicação', 'aplicativo', 'protocolo', 'plataforma', 'eletrônico', 'electrónico', 'electronico',  'servidor', 'server', 'biométrico', 'internet', 'grande', 'big', 'data', 'dados', 'inteligência', 'artificial', 'internet', 'rede', 'network', 'licença', 'license', 'comunicações', 'comunicação', 'equipes', 'equipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function: \n",
    "def words_in_string(word_list, a_string):\n",
    "    return set(word_list).intersection(a_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_in_string(keyterms_sp, ' '.join(data['data_lemmatized'][155])):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(words_in_string(keyterms_sp, ' '.join(data['data_lemmatized'][155])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['search_results'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    print('processing index:', index)\n",
    "    if data['language'][index] == 'es':\n",
    "        data.at[index, 'search_results'] = list(words_in_string(keyterms_sp, ' '.join(data['data_lemmatized'][index])))\n",
    "        \n",
    "    elif data['language'][index] == 'en':\n",
    "        data.at[index, 'search_results'] = list(words_in_string(keyterms_en, ' '.join(data['data_lemmatized'][index])))\n",
    "        \n",
    "    elif data['language'][index] == 'pt':\n",
    "        data.at[index, 'search_results'] = list(words_in_string(keyterms_pt, ' '.join(data['data_lemmatized'][index])))\n",
    "        \n",
    "    else:\n",
    "        data.at[index, 'search_results'] = 'Not available'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('./output/procurement_plans-keyterms_search_2020-11-18.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Document_Content'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "\n",
    "#   - 2.1           11/18/2020        Emiliano Colina    - Procurement Plans in diverse formats: xlsx, doc, pdf\n",
    "#                                                        - xlsx files with multiple sheets\n",
    "                                                                                                                  \n",
    "#   - 2.0           08/31/2020        Emiliano Colina    - Initial version, starting with already read TCs from     \n",
    "#                                                        notebook \"Digital Transformation Advisory - 02.0 - Document Processing TCs\"\n",
    "\n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
