{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.0 - Document Processing TCs - updated v2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 02 - Document Processing\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): pdf files\n",
    "#-- Description:  \n",
    "#                - Approval Documents of TCs\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  2.2\n",
    "#-- Last Update: 01/14/2021\n",
    "#-- Last Revision Date: 10/17/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.2: Both languages - TCs processing - Nov & Dec 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_pre_2 = joblib.load('./output/df_pre_tcs_2021-01-12_v12.joblib.bz2')\n",
    "df_pre_2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1\n",
    "#pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVO Y JUSTIFICACIÓN DE LA CT|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|(ANTECEDENTES\\,\\s)?JUSTIFICACIÓN Y OBJETIVOS DE LA CT|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Justificación y objetivo de la CT|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|Objetivo y justificación|Objetivos y justificación|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO|Descripción del préstamo\\/garantía asociado|Descripción del Préstamo/Garantía Asociado|Objetivos$|Objetivos\\n\\n\\n)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add column to store content\n",
    "df_pre_2['title_inicial'] = ''\n",
    "\n",
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English:\n",
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_pre_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agencia Ejecutora y estructura de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución|Agencia ejecutoria y estructura de ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to store title final\n",
    "df_pre_2['title_final'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_2['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.loc[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.Document_Content[77][6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_2['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_pre_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_2.iterrows():\n",
    "    page_ini = df_pre_2.title_inicial[index][1]\n",
    "    page_fin = df_pre_2.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_2['title_inicial'][index], df_pre_2['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_2['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_2['title_inicial'][index], df_pre_2['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_2.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_2.title_inicial[index][1]\n",
    "    page_fin = df_pre_2.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]][re.search(df_pre_2['title_inicial'][index][0], df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][page_fin][:df_pre_2['Document_Content'][index][page_fin].find(df_pre_2['title_final'][index][0])+len(df_pre_2['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]][re.search(df_pre_2['title_inicial'][index][0], df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_2['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][page_fin][:df_pre_2['Document_Content'][index][page_fin].find(df_pre_2['title_final'][index][0])+len(df_pre_2['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_2.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.3 - 10/17/2020)\n",
    "\n",
    "for index, row in df_pre_2.iterrows():\n",
    "    #print(df_pre_2['lista_paginas'][index])\n",
    "    longitud = len(df_pre_2['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_2['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "\n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_2.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_2['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_2.iterrows():\n",
    "    texto = df_pre_2['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_2.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2.2\n",
    "# Need to load previous results: './output/df_resultado_tcs_2020-10-17_v21.joblib.bz2'\n",
    "\n",
    "resultado_tcs = joblib.load('./output/df_resultado_tcs_2020-10-17_v21.joblib.bz2')\n",
    "resultado_tcs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2.2\n",
    "# Merge all TCs results\n",
    "resultado_tcs_2 = pd.concat([resultado_tcs, df_pre_2[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name', 'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']]], ignore_index=True)\n",
    "\n",
    "resultado_tcs_2.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultado_tcs_2.shape)\n",
    "resultado_tcs_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2.2 - Jump to the storing section, to save the dataframe (01/14/2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.1: Both languages - TCs processing - July to Sept 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_pre_2 = joblib.load('./output/df_pre_july2sept_2020-10-16_v11.joblib.bz2')\n",
    "df_pre_2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1\n",
    "#pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVO Y JUSTIFICACIÓN DE LA CT|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|(ANTECEDENTES\\,\\s)?JUSTIFICACIÓN Y OBJETIVOS DE LA CT|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Justificación y objetivo de la CT|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|Objetivo y justificación|Objetivos y justificación|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO|Descripción del préstamo\\/garantía asociado)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTECEDENTES, JUSTIFICACIÓN Y OBJETIVOS DE LA CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English documents:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.drop(['title_inicial'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-doing: docs title_inicial and title_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add column to store content\n",
    "df_pre_2['title_inicial'] = ''\n",
    "\n",
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English:\n",
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_pre_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_2.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to store title final\n",
    "df_pre_2['title_final'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_2['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_2[df_pre_2.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_2['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_2['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_pre_2['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_2.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_2.iterrows():\n",
    "    page_ini = df_pre_2.title_inicial[index][1]\n",
    "    page_fin = df_pre_2.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_2['title_inicial'][index], df_pre_2['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_2['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_2['title_inicial'][index], df_pre_2['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_2.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_2.title_inicial[index][1]\n",
    "    page_fin = df_pre_2.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]][re.search(df_pre_2['title_inicial'][index][0], df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][page_fin][:df_pre_2['Document_Content'][index][page_fin].find(df_pre_2['title_final'][index][0])+len(df_pre_2['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]][re.search(df_pre_2['title_inicial'][index][0], df_pre_2['Document_Content'][index][df_pre_2['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_2['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_2['Document_Content'][index][page_fin][:df_pre_2['Document_Content'][index][page_fin].find(df_pre_2['title_final'][index][0])+len(df_pre_2['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_2.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.3 - 10/17/2020)\n",
    "\n",
    "for index, row in df_pre_2.iterrows():\n",
    "    #print(df_pre_2['lista_paginas'][index])\n",
    "    longitud = len(df_pre_2['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_2['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "\n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_2.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_2['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_2.iterrows():\n",
    "    texto = df_pre_2['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_2.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous results (up to July 1st)\n",
    "resultado_tcs = joblib.load('./output/df_resultado_tcs_2020-08-31_v20.joblib.bz2')\n",
    "print(resultado_tcs.shape)\n",
    "resultado_tcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all TCs results\n",
    "resultado_tcs_2 = pd.concat([resultado_tcs, df_pre_2[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name', 'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']]], ignore_index=True)\n",
    "\n",
    "resultado_tcs_2.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_tcs_2[resultado_tcs_2['OPERATION_NUMBER'] == 'RG-T3704'].index.values.astype(int)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultado_tcs_2['Document_Content'][resultado_tcs_2[resultado_tcs_2['OPERATION_NUMBER'] == 'UR-T1230'].index.values.astype(int)[0]][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'PLAN DE ADQUISICIONES PARA OPERACIONES EJECUTADAS POR EL BANCO'|'PLAN DE ADQUISICIONES PARA OPERACIONES EJECUTADAS POR EL BID'|'PROCUREMENT PLAN FOR IDB-EXECUTED OPERATIONS'|'PROCUREMENT PLAN FOR BANK EXECUTED OPERATIONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1\n",
    "pattern_poa = r'PLAN DE ADQUISICIONES PARA OPERACIONES EJECUTADAS POR EL BANCO|PLAN DE ADQUISICIONES PARA OPERACIONES EJECUTADAS POR EL BID|PLAN DE ADQUISICIONES\\s+DE COOPERACIONES TECNICAS NO REEMBOLSABLES|PROCUREMENT PLAN FOR IDB-EXECUTED OPERATIONS|PROCUREMENT PLAN FOR BANK EXECUTED OPERATIONS|PROCUREMENT PLAN FOR NON-REIMBURSABLE TECHNICAL COOPERATIONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_to_check = []\n",
    "\n",
    "for index, row in resultado_tcs_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page_base = resultado_tcs_2['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(len(resultado_tcs_2['Document_Content'][index])-1, 0, -1):\n",
    "        if re.search(pattern_poa, resultado_tcs_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_to_check.append(index)\n",
    "print('Index to check', index_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_tcs_2['Document_Content'][462][-9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_tcs_2['extracted_cleaned_v2'][1253]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.0: Spanish language TCs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v0.7\n",
    "# Load source file:\n",
    "df_pre_es = joblib.load('./output/df_pre_es_2020-07-14_v07.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][171][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjustment:\n",
    "aux = df_pre_es['Document_Content'][171]\n",
    "#removal of 3rd element since it is an index page\n",
    "aux = aux[:2] + aux[4:]\n",
    "#\n",
    "df_pre_es.at[171, 'Document_Content'] = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of initial title for document on 171: \n",
    "lista_aux = df_pre_es['Document_Content'][171]\n",
    "lista_aux[3] = df_pre_es['Document_Content'][171][3].replace('II. OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA  ', 'II. Objetivos y justificación de la Cooperación Técnica')\n",
    "df_pre_es.at[171, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][171][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.at[171, 'doc_identifier'] = ('DOCUMENTO DE COOPERACIÓN TÉCNICA', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "#pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVO Y JUSTIFICACIÓN DE LA CT|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|JUSTIFICACIÓN Y OBJETIVOS DE LA CT|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|Objetivo y justificación|Objetivos y justificación|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO|Descripción del préstamo\\/garantía asociado)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es[df_pre_es.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-doing for spanish docs title_inicial and title_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_es['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_es['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_es.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_es[df_pre_es.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_es['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_es['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_es['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_es.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_es['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Name'][96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Name'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "index = 223\n",
    "lista_pages = []\n",
    "page_ini = df_pre_es.title_inicial[index][1]\n",
    "page_fin = df_pre_es.title_final[index][1]\n",
    "print(page_fin - page_ini)\n",
    "if (page_fin - page_ini) < 2: \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "    #lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_final'][index][1]][:re.search(df_pre_es['title_final'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_final'][index][1]]).span()[1]])\n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "else: \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "    for j in range(page_ini+1,page_fin-1): \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][j])\n",
    "    \n",
    "    lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "\n",
    "    \n",
    "    \n",
    "#page_inicial\n",
    "#print(df_pre_es['title_inicial'][index][1])\n",
    "#df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:]\n",
    "print('length: ', str(len(lista_pages)))\n",
    "for k in range(0, len(lista_pages)):\n",
    "    print(lista_pages[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile(df_pre_es['title_final'][index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(re.compile(df_pre_es['title_final'][index][0]), df_pre_es['Document_Content'][index][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6][:df_pre_es['Document_Content'][index][6].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['Document_Content'][index][6][:351]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"        \", str(index), page_fin - page_ini, df_pre_es['title_inicial'][index], df_pre_es['title_final'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_pre_es.title_inicial[index], df_pre_es.title_final[index])\n",
    "#print(df_pre_es['lista_paginas'][index])\n",
    "#page_ini = df_pre_es.title_inicial[index][1]\n",
    "#page_fin = df_pre_es.title_final[index][1]\n",
    "#print(str(page_fin - page_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pre_es['Document_Content'][index][3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 1355\n",
    "#index = 1\n",
    "for index in [index]:\n",
    "    print(df_pre_es['lista_paginas'][index])\n",
    "    longitud = len(df_pre_es['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_es['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_es.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_es.title_inicial[index][1]\n",
    "    page_fin = df_pre_es.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]][re.search(df_pre_es['title_inicial'][index][0], df_pre_es['Document_Content'][index][df_pre_es['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_es['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_es['Document_Content'][index][page_fin][:df_pre_es['Document_Content'][index][page_fin].find(df_pre_es['title_final'][index][0])+len(df_pre_es['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_es.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.2 - 08/30/2020)\n",
    "\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    #print(df_pre_es['lista_paginas'][index])\n",
    "    longitud = len(df_pre_es['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_es['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_es.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index, row in df_pre_es.iterrows():\n",
    "    #if (len(df_pre_es['extracted'][index]) < len(df_pre_es['extracted_v2'][index])):\n",
    "        #print(True, str((len(df_pre_es['extracted_v2'][index]) - len(df_pre_es['extracted'][index]))))\n",
    "    #    print()\n",
    "    if not (len(df_pre_es['extracted'][index]) < len(df_pre_es['extracted_v2'][index])) and (len(df_pre_es['extracted'][index]) - len(df_pre_es['extracted_v2'][index]) > 4000):\n",
    "        print(\"!!!! Alert on\", str(index), str(len(df_pre_es['extracted'][index]) - len(df_pre_es['extracted_v2'][index])))\n",
    "        count += 1\n",
    "print(str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('''II. Descripción del Préstamo/Garantía Asociado \n",
    "2.1 La cooperación técnica dará apoyo operativo a los Programas de Innovación \n",
    "\n",
    "\n",
    "Empresarial y Emprendimiento I (UR-L1142) y II (UR-L1158), los cuales son \n",
    "ejecutados por la Agencia Nacional de Investigación e Innovación (ANII) y financiados \n",
    "por el Banco Interamericano de Desarrollo (BID). Estos programas forman parte de la \n",
    "Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) aprobada en 2017 \n",
    "por un monto de US$100 millones (UR-O1153). El objetivo de la CCLIP es \n",
    "incrementar la productividad de las empresas mediante una mayor inversión en \n",
    "conocimiento, recursos humanos, innovación y emprendimiento. En particular, la \n",
    "cooperación técnica, apoyará los procesos de monitoreo y evaluación en los \n",
    "mencionados programas, así como también contribuirá a mejorar sus niveles de \n",
    "trasparencia, mediante una difusión automática, a través del desarrollo de un portal \n",
    "que pondrá a disposición de los ciudadanos datos abiertos sobre beneficiarios \n",
    "atendidos y los apoyos realizados.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df_pre_es['extracted'][31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df_pre_es['extracted_v2'][31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_es.iterrows():\n",
    "    texto = df_pre_es['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_es.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction on scanned content: replace '6' with 'ó'\n",
    "df_pre_es.Document_Name[1062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_es.extracted_cleaned_v2[1062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'^[A-Za-z]+6[A-Za-z]+$', 'revoluci6n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(r'^[A-Za-z]+6[A-Za-z]+$', 'revoluci6n'):\n",
    "    print(\"\".join(filter(lambda x: not x.isdigit(), 'revoluci6n')))\n",
    "    line = line[:10].replace(';', ':') + line[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_6 = df_pre_es.extracted_cleaned_v2[1062].split()\n",
    "resultado_6 = [word.replace('6', 'ó') if re.search(r'^[A-Za-z]+6[A-Za-z]+$', word) else word for word in texto_6]\n",
    "res_clean_6 = ' '.join(resultado_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_es.at[1062, extracted_cleaned_v2] = res_clean_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.2: all documents \n",
    "f_df_resultado_tcs = 'df_resultado_tcs_2021-01-14_v22.joblib'\n",
    "joblib.dump(resultado_tcs_2, './output/' + f_df_resultado_tcs + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.2: all documents to Excel:\n",
    "resultado_tcs_2.to_excel('TCs_Approval-Docs_all_2021-01-14_v22.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1: all documents \n",
    "f_df_resultado_tcs = 'df_resultado_tcs_2020-10-17_v21.joblib'\n",
    "joblib.dump(resultado_tcs_2, './output/' + f_df_resultado_tcs + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1: all documents to Excel:\n",
    "df_previous.to_excel('TCs_Approval-Docs_all_2020-10-17_v21.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0: all documents \n",
    "f_df_resultado_tcs = 'df_resultado_tcs_2020-08-31_v20.joblib'\n",
    "joblib.dump(resultado_tcs, './output/' + f_df_resultado_tcs + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0: all documents to Excel:\n",
    "resultado_tcs.to_excel('TCs_Approval-Docs_all_2020-08-31_v20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0: NULL URL TCs, both spanish and english documents, content extracted under v2.0\n",
    "f_df_pre_null = 'df_pre_null_2020-08-31_v20.joblib'\n",
    "joblib.dump(df_pre_null, './output/' + f_df_pre_null + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0: English documents, content extracted under v2.0\n",
    "f_df_pre_en = 'df_pre_en_2020-08-31_v20.joblib'\n",
    "joblib.dump(df_pre_en, './output/' + f_df_pre_en + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0: Spanish documents, content extracted under v2.0\n",
    "f_df_pre_es = 'df_pre_es_2020-08-31_v20.joblib'\n",
    "joblib.dump(df_pre_es, './output/' + f_df_pre_es + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Minimal adjustment on filtering footnotes supraindexes (performed on 2020-10-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load previous results\n",
    "df_previous = joblib.load('./output/df_resultado_tcs_2020-10-17_v21.joblib.bz2')\n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_previous.iterrows():\n",
    "    texto = df_previous['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_previous.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.1: all documents \n",
    "f_df_resultado_tcs = 'df_resultado_tcs_2020-10-17_v21.joblib'\n",
    "joblib.dump(df_previous, './output/' + f_df_resultado_tcs + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.0: English language TCs processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v0.8\n",
    "# Load source file:\n",
    "df_pre_en = joblib.load('./output/df_pre_en_2020-08-23_v08.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "#pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO)\\s{0,}\\n?'\n",
    "pattern_es_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objetivos? y Justificación((\\s?de (la\\s)?(CT\\:?|Cooperación Técnica|TC)\\.?)|(\\sdel Proyecto)|\\:| de la Cooperación Técnica \\(CT\\))?|Objetivos y justificación de la CT|Objetivos y justificación de la Cooperación Técnica|Justificación y Objetivos de la CT|Justificación y Objetivo|Problema\\, Objetivos y Justificación de la CT\\.?|OBJETIVO Y JUSTIFICACIÓN DE LA CT|OBJETIVOS Y JUSTIFICACIÓN DE LA OPERACIÓN DE COOPERACIÓN TÉCNICA|JUSTIFICACIÓN Y OBJETIVOS DE LA CT|OBJETIVOS Y JUSTIFICACIÓN|Justificaci6n y objetivo|Objetivos y Justificación de la CT \\(estimado\\: 1 página\\)|Objetivo y justificación|Objetivos y justificación|DESCRIPCIÓN DEL PRÉSTAMO\\/GARANTÍA ASOCIADO|Descripción del préstamo\\/garantía asociado)\\s{0,}\\n?'\n",
    "pattern_en_1 = r'\\n?\\s?\\n?\\s?[2IV31lI]+\\.?\\s{0,}(Objectives?\\s+and\\s+(J|j)ustification( of the TC)?|OBJECTIVES? AND JUSTIFICATION( OF THE TC)?|JUSTIFICATION AND OBJECTIVE|(TC|TECHNICAL COOPERATION) OBJECTIVES AND RATIONALE|Justification and Objectives of the TC|Description of the Associated Loan|JUSTIFICATION|Background\\, Objectives and Justification of the TC|OBJECTIVE AND RATIONALE OF THE TC|OBJECTIVES AND RATIONALE OF THE TECHNICAL COOPERATION OPERATION)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en[df_pre_en.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-doing for english docs title_inicial and title_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English:\n",
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_en['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_pre_en['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_en.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Englishh:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_en[df_pre_en.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_en['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_en['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_pre_en['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_en.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    page_ini = df_pre_en.title_inicial[index][1]\n",
    "    page_fin = df_pre_en.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_en['title_inicial'][index], df_pre_en['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_en['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_en['title_inicial'][index], df_pre_en['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_en.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_en.title_inicial[index][1]\n",
    "    page_fin = df_pre_en.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_en['Document_Content'][index][df_pre_en['title_inicial'][index][1]][re.search(df_pre_en['title_inicial'][index][0], df_pre_en['Document_Content'][index][df_pre_en['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_en['Document_Content'][index][page_fin][:df_pre_en['Document_Content'][index][page_fin].find(df_pre_en['title_final'][index][0])+len(df_pre_en['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_en['Document_Content'][index][df_pre_en['title_inicial'][index][1]][re.search(df_pre_en['title_inicial'][index][0], df_pre_en['Document_Content'][index][df_pre_en['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_en['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_en['Document_Content'][index][page_fin][:df_pre_en['Document_Content'][index][page_fin].find(df_pre_en['title_final'][index][0])+len(df_pre_en['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_en.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.2 - 08/30/2020)\n",
    "\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    #print(df_pre_en['lista_paginas'][index])\n",
    "    longitud = len(df_pre_en['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_en['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_en.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index, row in df_pre_en.iterrows():\n",
    "    #if (len(df_pre_en['extracted'][index]) < len(df_pre_en['extracted_v2'][index])):\n",
    "        #print(True, str((len(df_pre_en['extracted_v2'][index]) - len(df_pre_en['extracted'][index]))))\n",
    "    #    print()\n",
    "    if not (len(df_pre_en['extracted'][index]) < len(df_pre_en['extracted_v2'][index])) and (len(df_pre_en['extracted'][index]) - len(df_pre_en['extracted_v2'][index]) > 500):\n",
    "        print(\"!!!! Alert on\", str(index), str(len(df_pre_en['extracted'][index]) - len(df_pre_en['extracted_v2'][index])))\n",
    "        count += 1\n",
    "print(str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df_pre_en['extracted'][616])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(df_pre_en['extracted_v2'][616])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_en.iterrows():\n",
    "    texto = df_pre_en['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_en.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pre_en['extracted_cleaned_v2'][21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2.0 - NULL_URL TCs processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load joblib from v1.01: NULL_URL TC documents, both languages\n",
    "# Load source file:\n",
    "df_pre_null = joblib.load('./output/df_pre_null_2020-08-25_v101.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Objetivos y justificación' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish documents:\n",
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English documents:\n",
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'Agencia Ejecutora y estructura de ejecución' - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2.0\n",
    "pattern_es_3 = r'\\n?\\s?\\n?\\s?[IV\\.5]+\\s{0,}(((4\\.1\\s+)?Agencia Ejecutora(\\s+\\(AE\\))?|Organismo [Ee]jecutor|Unidad Ejecutora|Entidad Ejecutora) y [Ee]structura de [Ee]jecución|Estructura del Organismo Ejecutor\\s?\\(?O?E?\\)?|Organismo de Ejecución y Estructura de Implementación|(Agencia|Unidad) [Ee]jecutora y [Ee]structura de [Ee]jecución|ORGANISMO EJECUTOR Y ESTRUCTURA DE IMPLEMENTACIÓN|AGENCIA EJECUTORA Y ESTRUCTURA DE EJECUCIÓN|ORGANISMO EJECUTOR Y ESTRUCTURA DE EJECUCIÓN|Agencia ejecutora y justificación de la estructura de ejecución|Organismo Ejecutor|Estructura de ejecución|Agencia Ejecutora|Mecanismo de Ejecución)\\s{0,}\\n?'\n",
    "pattern_en_3 = r'\\n?\\s?\\n?\\s?[IV\\.54]+\\s{0,}(4\\.1\\s+)?(Executing [Aa]gency and [Ee]xecution [Ss]tructure|EXECUTING AGENCY( AND EXECUTION STRUCTURE)?|Executing agency and execution|Executing Agency \\(EA\\) and execution structure|Executing Agency and Executing Structure|Executing agency and execution structure|EA AND EXECUTION STRUCTURE)\\s{0,}\\n?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_es_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_en_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-doing: docs title_inicial and title_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title inicial - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spanish:\n",
    "index_es_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_null[df_pre_null.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_es_1, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_es_1, df_pre_null['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_null.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_es_to_check.append(index)\n",
    "print('Index to check', index_es_to_check)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English:\n",
    "index_en_to_check = []\n",
    "# identify 1st title location:\n",
    "for index, row in df_pre_null[df_pre_null.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_pre_null['doc_identifier'][index][1] # starting page\n",
    "    \n",
    "    for i in range(0,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_en_1, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(pattern_en_1, df_pre_null['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_pre_null.at[index, 'title_inicial'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_en_to_check.append(index)\n",
    "print('Index to check', index_en_to_check)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### title final - v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spanish:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'es'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_null['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_es_3, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_es_3, df_pre_null['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_null.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#English:\n",
    "index_null_to_check = []\n",
    "\n",
    "for index, row in df_pre_null[df_pre_null.language == 'en'].iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    #page base is the first title:\n",
    "    page_base = df_pre_null['title_inicial'][index][1] + 1 # starting page\n",
    "\n",
    "    for i in range(page_base,len(df_pre_null['Document_Content'][index])):\n",
    "        if re.search(pattern_en_3, df_pre_null['Document_Content'][index][i]) != None: # pattern found\n",
    "            print('* pattern found at document page:', str(i))\n",
    "            \n",
    "            ## storing:\n",
    "            final_match_title = re.search(pattern_en_3, df_pre_null['Document_Content'][index][i]).group()\n",
    "            final_match_page = i\n",
    "            df_pre_null.at[index, 'title_final'] = (final_match_title, final_match_page)\n",
    "            ##\n",
    "            \n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break       \n",
    "        \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        index_null_to_check.append(index)\n",
    "print('Index to check', index_null_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for those cases where the page distance is <2:\n",
    "for index, row in df_pre_null.iterrows():\n",
    "    page_ini = df_pre_null.title_inicial[index][1]\n",
    "    page_fin = df_pre_null.title_final[index][1]\n",
    "    #print(str(index), page_fin - page_ini, df_pre_null['title_inicial'][index], df_pre_null['title_final'][index])\n",
    "    if (page_fin - page_ini) < 2:\n",
    "        print('Alert index', str(index), \"---\", \"Lenght:\", (page_fin - page_ini), df_pre_null['Document_Name'][index])\n",
    "        print(\"        \", str(index), page_fin - page_ini, df_pre_null['title_inicial'][index], df_pre_null['title_final'][index])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by title_inicial y title_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null['lista_paginas'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_pre_null.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_pre_null.title_inicial[index][1]\n",
    "    page_fin = df_pre_null.title_final[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_pre_null['Document_Content'][index][df_pre_null['title_inicial'][index][1]][re.search(df_pre_null['title_inicial'][index][0], df_pre_null['Document_Content'][index][df_pre_null['title_inicial'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_pre_null['Document_Content'][index][page_fin][:df_pre_null['Document_Content'][index][page_fin].find(df_pre_null['title_final'][index][0])+len(df_pre_null['title_final'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_pre_null['Document_Content'][index][df_pre_null['title_inicial'][index][1]][re.search(df_pre_null['title_inicial'][index][0], df_pre_null['Document_Content'][index][df_pre_null['title_inicial'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_pre_null['Document_Content'][index][j])\n",
    "        lista_pages.append(df_pre_null['Document_Content'][index][page_fin][:df_pre_null['Document_Content'][index][page_fin].find(df_pre_null['title_final'][index][0])+len(df_pre_null['title_final'][index][0])])\n",
    "    \n",
    "    df_pre_null.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.2 - 08/30/2020)\n",
    "\n",
    "for index, row in df_pre_null.iterrows():\n",
    "    #print(df_pre_null['lista_paginas'][index])\n",
    "    longitud = len(df_pre_null['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_pre_null['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n\\n[1-9]\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Aug|Sep|Set|Oct|Nov|Dic|IDB|months|Budget|Development)([A-Z\\¿]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;\\,]+', ' ', texto)\n",
    "    \n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_pre_null.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_pre_null['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_pre_null.iterrows():\n",
    "    texto = df_pre_null['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_pre_null.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all the results in one single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_pre_null = joblib.load('./output/df_pre_null_2020-08-31_v20.joblib.bz2')\n",
    "df_pre_es = joblib.load('./output/df_pre_es_2020-08-31_v20.joblib.bz2')\n",
    "df_pre_en = joblib.load('./output/df_pre_en_2020-08-31_v20.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_null.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_es.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_en.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_tcs = pd.concat([df_pre_es[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name', 'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']], df_pre_en[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name', 'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']], df_pre_null[['doc_type', 'language', 'FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name', 'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "resultado_tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#v2.0: all documents \n",
    "resultado_tcs = joblib.load('./output/df_resultado_tcs_2020-08-31_v20.joblib.bz2')\n",
    "resultado_tcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 2.2           01/14/2021        Emiliano Colina    - Includes operations from Nov to Dec, in both languages     \n",
    "\n",
    "#   - 2.1           10/17/2020        Emiliano Colina    - Includes July to September TCs, both languages     \n",
    "                                                                                                                  \n",
    "#   - 2.0           08/31/2020        Emiliano Colina    - Initial version, starting with already read TCs from     \n",
    "#                                                        notebook \"Digital Transformation Advisory - 02.0 - Document Processing TCs\"\n",
    "                                                       \n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
