{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.1 - Document Processing - Loan Proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 02.1 - Document Processing - Loan Proposals\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): pdf files\n",
    "#-- Description:  \n",
    "#                - Loan Proposals processing\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  0.7\n",
    "#-- Last Update: 01/14/2021\n",
    "#-- Last Revision Date: 07/19/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loans from Nov and Dec 2020 - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Loans-Doc_Collection_2021-01-12_v10_.joblib.bz2')\n",
    "print(data_pre.shape)\n",
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Reading - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.7: Blank pages statistics\n",
    "\n",
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100\n",
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index page and Language identification - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify & get the index page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the index language\n",
    "df_filtered_2['language'] = ''\n",
    "# stores the index page\n",
    "df_filtered_2['index_page'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:\n",
    "#CONTENTS \\n\\n\\nPROJECT SUMMARY\n",
    "#re.search(r'(^(\\-(\\s+)?i?i\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+|- i - \\n\\n\\nCONTENTS \\n\\n\\nPROJECT SUMMARY)', df_filtered_2.Document_Content[72][3], re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review = []\n",
    "loan_count = 0\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    is_loan = False\n",
    "    for page in range(0,len(df_filtered_2.Document_Content[index])):\n",
    "        if re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+|- i - \\n\\n\\nCONTENTS \\n\\n\\nPROJECT SUMMARY)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('English - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'en'\n",
    "            match_title_type = re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+|- i - \\n\\n\\nCONTENTS \\n\\n\\nPROJECT SUMMARY)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "            \n",
    "        elif re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('Spanish - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'es'\n",
    "            match_title_type = re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "        \n",
    "        \n",
    "    if not is_loan: \n",
    "        print('check regex on:', str(index))\n",
    "        #df_filtered_2.at[index, 'doc_type'] = 'other'\n",
    "        #df_filtered_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('Loans identified:', str(loan_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(df_filtered_2.OPERATION_NUMBER.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index titles - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store key index titles: \n",
    "df_filtered_2['index_titles'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_review = []\n",
    "# key titles are extracted along with their respective page number: \n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('*Processing index:', str(index))\n",
    "    key_titles = re.findall(r'[IV\\.]{1,5}\\s+[A-ZÁÉÍÓÚ\\s\\,\\n]+[\\.\\s\\-\\…]{0,200}\\d\\d?', df_filtered_2.Document_Content[index][df_filtered_2.index_page[index]])\n",
    "    print(key_titles)\n",
    "    if key_titles == []:\n",
    "        print('Found empty list on:', str(index))\n",
    "        #to_review.append(index)\n",
    "    else:\n",
    "        df_filtered_2.at[index, 'index_titles'] = key_titles\n",
    "        \n",
    "    print(\"~~~\")\n",
    "    print()\n",
    "#print(to_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['index_title_I'] = ''\n",
    "df_filtered_2['index_title_II'] = ''\n",
    "df_filtered_2['index_title_III'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the index titles and get main titles and pages:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('* Processing index:', str(index))\n",
    "    for i in range(0,len(df_filtered_2.index_titles[index])):\n",
    "        resultado = tuple(re.findall(r'[A-ZÁÉÍÓÚ\\.\\s\\-\\…\\,\\n]+|\\d+', df_filtered_2.index_titles[index][i]))\n",
    "        #ini:\n",
    "        if (resultado[0].startswith('I.') or 'DESCRIP' in resultado[0]):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][2:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_I'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #medio:\n",
    "        elif (resultado[0].startswith('II.') or resultado[0].startswith('II ') or resultado[0].startswith('..... ESTRUCTURA')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_II'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #fin:\n",
    "        elif (resultado[0].startswith('III') or resultado[0].startswith('.... PLAN')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_III'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        else:\n",
    "            # do nothing\n",
    "            print('nothing')\n",
    "        \n",
    "    #del aux\n",
    "    print()\n",
    "    print('~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_I (v0.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.Document_Content[46][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2['index_title_I'][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0.7: adjustment required on index 46, since an invalid value was found: ('PROJECT DESCRIPTION AND RESULTS MNITORING', '2'):\n",
    "df_filtered_2.at[46, 'index_title_I'] = ('PROJECT DESCRIPTION AND RESULTS MONITORING', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2['index_title_I'][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_II (v0.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_III (v0.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Title check - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['true_title_I'] = ''\n",
    "df_filtered_2['true_title_II'] = ''\n",
    "df_filtered_2['true_title_III'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_I location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_I'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_II location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_I'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_II'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_III location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_II'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_III'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for crossed titles - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered_2.iterrows():\n",
    "    if (df_filtered_2.true_title_I[index][1] < df_filtered_2.true_title_II[index][1] < df_filtered_2.true_title_III[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_filtered_2.true_title_III[index][1]> df_filtered_2.true_title_I[index][1] > df_filtered_2.true_title_II[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        \n",
    "    #if (df_filtered_2.true_title_III[index][1] - df_filtered_2.true_title_I[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "    #    print('File to check due to extension between titles:', df_filtered_2['Document_Name'][index])\n",
    "    #    print((df_filtered_2.true_title_I[index][0], df_filtered_2.true_title_I[index][1]), (df_filtered_2.true_title_II[index][0], df_filtered_2.true_title_II[index][1]), (df_filtered_2.true_title_III[index][0], df_filtered_2.true_title_III[index][1]))\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the list of pages, delimited by true_title_I y true_title_II - v0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2['lista_paginas'] = ''\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_filtered_2.true_title_I[index][1]\n",
    "    page_fin = df_filtered_2.true_title_II[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_filtered_2['Document_Content'][index][j])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "    \n",
    "    df_filtered_2.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments (v0.7)\n",
    "df_filtered_2['doc_type'] = 'loan'\n",
    "\n",
    "df_filtered_2.rename(columns={'true_title_I':'title_inicial', 'true_title_II': 'title_final'}, inplace=True)\n",
    "\n",
    "df_filtered_1 = df_filtered_2[['doc_type','language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'Document_Content', 'title_inicial', 'title_final', 'lista_paginas']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.7 - Load Previous Results and Generate the list of pages, delimited by true_title_I y true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load previous results (from v0.6), since an adjusted extraction process is performed\n",
    "df_previous = joblib.load('./output/df_resultado_loans_2020-11-04_v06.joblib.bz2')\n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these columns:\n",
    "df_previous.drop(['extracted_v2', 'extracted_cleaned_v2'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.7 - Merge Loans from current version with previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge Loans from version 0.6 with previous results:\n",
    "df_loans = pd.concat([df_previous, df_filtered_1], ignore_index=True)\n",
    "\n",
    "df_loans.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.7 - Text extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_loans['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.3 - 10/17/2020)\n",
    "\n",
    "for index, row in df_loans.iterrows():\n",
    "    #print(df_loans['lista_paginas'][index])\n",
    "    longitud = len(df_loans['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_loans['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_loans.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_loans['extracted_v2'][155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.7 - supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_loans['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_loans.iterrows():\n",
    "    texto = df_loans['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_loans.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.6 - Loans from October 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Loans-Doc_Collection_2020-11-04_v09_.joblib.bz2')\n",
    "print(data_pre.shape)\n",
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.6 - Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100\n",
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.6 -  Index page and Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify & get the index page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the index language\n",
    "df_filtered_2['language'] = ''\n",
    "# stores the index page\n",
    "df_filtered_2['index_page'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review = []\n",
    "loan_count = 0\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    is_loan = False\n",
    "    for page in range(0,len(df_filtered_2.Document_Content[index])):\n",
    "        if re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('English - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'en'\n",
    "            match_title_type = re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "            \n",
    "        elif re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('Spanish - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'es'\n",
    "            match_title_type = re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "        \n",
    "        \n",
    "    if not is_loan: \n",
    "        print('check regex on:', str(index))\n",
    "        #df_filtered_2.at[index, 'doc_type'] = 'other'\n",
    "        #df_filtered_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('Loans identified:', str(loan_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(df_filtered_2.OPERATION_NUMBER.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.6 - Index titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store key index titles: \n",
    "df_filtered_2['index_titles'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_review = []\n",
    "# key titles are extracted along with their respective page number: \n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('*Processing index:', str(index))\n",
    "    key_titles = re.findall(r'[IV\\.]{1,5}\\s+[A-ZÁÉÍÓÚ\\s\\,\\n]+[\\.\\s\\-\\…]{0,200}\\d\\d?', df_filtered_2.Document_Content[index][df_filtered_2.index_page[index]])\n",
    "    print(key_titles)\n",
    "    if key_titles == []:\n",
    "        print('Found empty list on:', str(index))\n",
    "        #to_review.append(index)\n",
    "    else:\n",
    "        df_filtered_2.at[index, 'index_titles'] = key_titles\n",
    "        \n",
    "    print(\"~~~\")\n",
    "    print()\n",
    "#print(to_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['index_title_I'] = ''\n",
    "df_filtered_2['index_title_II'] = ''\n",
    "df_filtered_2['index_title_III'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the index titles and get main titles and pages:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('* Processing index:', str(index))\n",
    "    for i in range(0,len(df_filtered_2.index_titles[index])):\n",
    "        resultado = tuple(re.findall(r'[A-ZÁÉÍÓÚ\\.\\s\\-\\…\\,\\n]+|\\d+', df_filtered_2.index_titles[index][i]))\n",
    "        #ini:\n",
    "        if (resultado[0].startswith('I.') or 'DESCRIP' in resultado[0]):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][2:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_I'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #medio:\n",
    "        elif (resultado[0].startswith('II.') or resultado[0].startswith('II ') or resultado[0].startswith('..... ESTRUCTURA')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_II'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #fin:\n",
    "        elif (resultado[0].startswith('III') or resultado[0].startswith('.... PLAN')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_III'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        else:\n",
    "            # do nothing\n",
    "            print('nothing')\n",
    "        \n",
    "    #del aux\n",
    "    print()\n",
    "    print('~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0.6 - True Title check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['true_title_I'] = ''\n",
    "df_filtered_2['true_title_II'] = ''\n",
    "df_filtered_2['true_title_III'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_I location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_I'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_II location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_I'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_II'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_III location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_II'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_III'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - Check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered_2.iterrows():\n",
    "    if (df_filtered_2.true_title_I[index][1] < df_filtered_2.true_title_II[index][1] < df_filtered_2.true_title_III[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_filtered_2.true_title_III[index][1]> df_filtered_2.true_title_I[index][1] > df_filtered_2.true_title_II[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        \n",
    "    #if (df_filtered_2.true_title_III[index][1] - df_filtered_2.true_title_I[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "    #    print('File to check due to extension between titles:', df_filtered_2['Document_Name'][index])\n",
    "    #    print((df_filtered_2.true_title_I[index][0], df_filtered_2.true_title_I[index][1]), (df_filtered_2.true_title_II[index][0], df_filtered_2.true_title_II[index][1]), (df_filtered_2.true_title_III[index][0], df_filtered_2.true_title_III[index][1]))\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - Generate the list of pages, delimited by true_title_I y true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2['lista_paginas'] = ''\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_filtered_2.true_title_I[index][1]\n",
    "    page_fin = df_filtered_2.true_title_II[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_filtered_2['Document_Content'][index][j])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "    \n",
    "    df_filtered_2.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments (v0.6)\n",
    "df_filtered_2['doc_type'] = 'loan'\n",
    "\n",
    "df_filtered_2.rename(columns={'true_title_I':'title_inicial', 'true_title_II': 'title_final'}, inplace=True)\n",
    "\n",
    "df_filtered_1 = df_filtered_2[['doc_type','language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'Document_Content', 'title_inicial', 'title_final', 'lista_paginas']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - Load Previous Results and Generate the list of pages, delimited by true_title_I y true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load previous results (from v0.4), since an adjusted extraction process is performed\n",
    "df_previous = joblib.load('./output/df_resultado_loans_2020-10-19_v05.joblib.bz2')\n",
    "df_previous.drop(['extracted_v2', 'extracted_cleaned_v2'], axis=1, inplace=True)\n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - Merge Loans from current version with previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge Loans from version 0.5 with previous results:\n",
    "df_loans = pd.concat([df_previous, df_filtered_1], ignore_index=True)\n",
    "\n",
    "df_loans.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - Text extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_loans['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.3 - 10/17/2020)\n",
    "\n",
    "for index, row in df_loans.iterrows():\n",
    "    #print(df_loans['lista_paginas'][index])\n",
    "    longitud = len(df_loans['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_loans['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_loans.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_loans['extracted_v2'][155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.6 - supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_loans['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_loans.iterrows():\n",
    "    texto = df_loans['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_loans.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.5 - Loans from July to September 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = joblib.load('./output/Loans-Doc_Collection_2020-10-16_v08_.joblib.bz2')\n",
    "print(data_pre.shape)\n",
    "data_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.5 - Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals_cont\"\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()\n",
    "data['Document_Content'] = ''\n",
    "#data.head()\n",
    "print(data.Document_Status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in data.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + data.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data_ = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data_['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        data.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        data.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['blank_pages'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = data['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    data.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100\n",
    "data.blank_pages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_count'] = data['Document_Content'].apply(lambda x: len(x))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.5 -  Index page and Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify & get the index page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the index language\n",
    "df_filtered_2['language'] = ''\n",
    "# stores the index page\n",
    "df_filtered_2['index_page'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review = []\n",
    "loan_count = 0\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    is_loan = False\n",
    "    for page in range(0,len(df_filtered_2.Document_Content[index])):\n",
    "        if re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('English - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'en'\n",
    "            match_title_type = re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "            \n",
    "        elif re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('Spanish - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'es'\n",
    "            match_title_type = re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "        \n",
    "        \n",
    "    if not is_loan: \n",
    "        print('check regex on:', str(index))\n",
    "        #df_filtered_2.at[index, 'doc_type'] = 'other'\n",
    "        #df_filtered_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('Loans identified:', str(loan_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(df_filtered_2.OPERATION_NUMBER.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0.5 - Index titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store key index titles: \n",
    "df_filtered_2['index_titles'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_review = []\n",
    "# key titles are extracted along with their respective page number: \n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('*Processing index:', str(index))\n",
    "    key_titles = re.findall(r'[IV\\.]{1,5}\\s+[A-ZÁÉÍÓÚ\\s\\,\\n]+[\\.\\s\\-\\…]{0,200}\\d\\d?', df_filtered_2.Document_Content[index][df_filtered_2.index_page[index]])\n",
    "    print(key_titles)\n",
    "    if key_titles == []:\n",
    "        print('Found empty list on:', str(index))\n",
    "        #to_review.append(index)\n",
    "    else:\n",
    "        df_filtered_2.at[index, 'index_titles'] = key_titles\n",
    "        \n",
    "    print(\"~~~\")\n",
    "    print()\n",
    "#print(to_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually adjusted index 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered_2.Document_Content[127][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.at[127, 'index_titles'] = ['I. DESCRIPCIÓN DEL PROYECTO Y MONITOREO DE RESULTADOS .......................................... 2', \\\n",
    "                                         'II. ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS .......................................... 12', 'III. PLAN DE IMPLEMENTACIÓN Y GESTIÓN .......................................... 14']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['index_title_I'] = ''\n",
    "df_filtered_2['index_title_II'] = ''\n",
    "df_filtered_2['index_title_III'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the index titles and get main titles and pages:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('* Processing index:', str(index))\n",
    "    for i in range(0,len(df_filtered_2.index_titles[index])):\n",
    "        resultado = tuple(re.findall(r'[A-ZÁÉÍÓÚ\\.\\s\\-\\…\\,\\n]+|\\d+', df_filtered_2.index_titles[index][i]))\n",
    "        #ini:\n",
    "        if (resultado[0].startswith('I.') or 'DESCRIP' in resultado[0]):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][2:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_I'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #medio:\n",
    "        elif (resultado[0].startswith('II.') or resultado[0].startswith('II ') or resultado[0].startswith('..... ESTRUCTURA')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_II'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #fin:\n",
    "        elif (resultado[0].startswith('III') or resultado[0].startswith('.... PLAN')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_III'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        else:\n",
    "            # do nothing\n",
    "            print('nothing')\n",
    "        \n",
    "    #del aux\n",
    "    print()\n",
    "    print('~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 0.5 - True Title check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['true_title_I'] = ''\n",
    "df_filtered_2['true_title_II'] = ''\n",
    "df_filtered_2['true_title_III'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_I location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_I'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_II location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_I'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_II'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_III location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_II'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_III'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - Check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered_2.iterrows():\n",
    "    if (df_filtered_2.true_title_I[index][1] < df_filtered_2.true_title_II[index][1] < df_filtered_2.true_title_III[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_filtered_2.true_title_III[index][1]> df_filtered_2.true_title_I[index][1] > df_filtered_2.true_title_II[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        \n",
    "    #if (df_filtered_2.true_title_III[index][1] - df_filtered_2.true_title_I[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "    #    print('File to check due to extension between titles:', df_filtered_2['Document_Name'][index])\n",
    "    #    print((df_filtered_2.true_title_I[index][0], df_filtered_2.true_title_I[index][1]), (df_filtered_2.true_title_II[index][0], df_filtered_2.true_title_II[index][1]), (df_filtered_2.true_title_III[index][0], df_filtered_2.true_title_III[index][1]))\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - Generate the list of pages, delimited by true_title_I y true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_2['lista_paginas'] = ''\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_filtered_2.true_title_I[index][1]\n",
    "    page_fin = df_filtered_2.true_title_II[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]][re.search(df_filtered_2['true_title_I'][index][0], df_filtered_2['Document_Content'][index][df_filtered_2['true_title_I'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_filtered_2['Document_Content'][index][j])\n",
    "        lista_pages.append(df_filtered_2['Document_Content'][index][page_fin][:df_filtered_2['Document_Content'][index][page_fin].find(df_filtered_2['true_title_II'][index][0])+len(df_filtered_2['true_title_II'][index][0])])\n",
    "    \n",
    "    df_filtered_2.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - Load Previous Results and Generate the list of pages, delimited by true_title_I y true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load previous results (from v0.4), since an adjusted extraction process is performed\n",
    "df_previous = joblib.load('./output/df_filtered_2_loans_2020-07-21_v04_Content_extracted_cleaned.joblib.bz2')\n",
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.drop(['extracted', 'extracted_cleaned'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous['lista_paginas'] = ''\n",
    "\n",
    "for index, row in df_previous.iterrows():\n",
    "    print('processing index', str(index))\n",
    "    lista_pages = []\n",
    "    page_ini = df_previous.true_title_I[index][1]\n",
    "    page_fin = df_previous.true_title_II[index][1]\n",
    "    if (page_fin - page_ini) < 2: \n",
    "        lista_pages.append(df_previous['Document_Content'][index][df_previous['true_title_I'][index][1]][re.search(df_previous['true_title_I'][index][0], df_previous['Document_Content'][index][df_previous['true_title_I'][index][1]]).span()[0]:])\n",
    "        lista_pages.append(df_previous['Document_Content'][index][page_fin][:df_previous['Document_Content'][index][page_fin].find(df_previous['true_title_II'][index][0])+len(df_previous['true_title_II'][index][0])])\n",
    "\n",
    "    else: \n",
    "        lista_pages.append(df_previous['Document_Content'][index][df_previous['true_title_I'][index][1]][re.search(df_previous['true_title_I'][index][0], df_previous['Document_Content'][index][df_previous['true_title_I'][index][1]]).span()[0]:])\n",
    "        for j in range(page_ini+1,page_fin): \n",
    "            lista_pages.append(df_previous['Document_Content'][index][j])\n",
    "        lista_pages.append(df_previous['Document_Content'][index][page_fin][:df_previous['Document_Content'][index][page_fin].find(df_previous['true_title_II'][index][0])+len(df_previous['true_title_II'][index][0])])\n",
    "    \n",
    "    df_previous.at[index, 'lista_paginas'] = lista_pages\n",
    "    del lista_pages\n",
    "    del page_ini\n",
    "    del page_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - Merge Loans from current version with previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge Loans from version 0.5 with previous results:\n",
    "df_loans = pd.concat([df_previous, df_filtered_2], ignore_index=True)\n",
    "\n",
    "df_loans.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - Text extraction and clean-up routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_loans['extracted_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text_extraction and clean-up routine (v2.3 - 10/17/2020)\n",
    "\n",
    "for index, row in df_loans.iterrows():\n",
    "    #print(df_loans['lista_paginas'][index])\n",
    "    longitud = len(df_loans['lista_paginas'][index])\n",
    "    print('### Processing index: ', str(index), ' - page range:', str(longitud))\n",
    "    texto = ''\n",
    "    for j in range(0,longitud):\n",
    "\n",
    "        page = df_loans['lista_paginas'][index][j]\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'(^\\s?\\-\\s{0,3}[1-9]\\d?\\s{0,3}\\-|^\\-\\s{5,9})', ' \\n ', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+([A-Z]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|Mar|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep|IDB|months|Budget|Development)([A-Z\\¿\\“]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean) + ' \\n '\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page) + ' '\n",
    "            \n",
    "    # Additional clean-up\n",
    "    # - remove urls:\n",
    "    texto = re.sub(r'https?://\\S+', '', texto)\n",
    "    \n",
    "    #print(texto)\n",
    "    \n",
    "    df_loans.at[index, 'extracted_v2'] = texto.strip()\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('#-#-#-#')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_loans['extracted_v2'][155])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 0.5 - supra-indexes removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_loans['extracted_cleaned_v2'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_loans.iterrows():\n",
    "    texto = df_loans['extracted_v2'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\)\\”\\\"]+(\\d{1,3}|[\\¹\\²\\³\\⁴\\⁵\\⁶\\⁷\\⁸\\⁹\\⁰]+)[\\.\\,\\;\\:]?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_loans.at[index, 'extracted_cleaned_v2'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['doc_type'] = 'loan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans.rename(columns={'true_title_I':'title_inicial', 'true_title_II': 'title_final'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = df_loans[['doc_type','language', 'FK_OPERATION_ID', 'OPERATION_NUMBER',\n",
    "       'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'Document_Content', 'title_inicial', 'title_final', 'lista_paginas',\n",
    "       'extracted_v2', 'extracted_cleaned_v2']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataframe containing Loans from `Digital Transformation Advisory - 01 - Document Collection` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_base_pre = joblib.load('./output/Loans-Doc_Collection_2020-07-15_v07_.joblib.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Loan Document` Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Document Location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Loans_Approvals\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataframe for text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df_base_pre[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'DOCUMENT_NAME', 'Document_Name', 'Document_Status']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['Document_Content'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the documents and store the content in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_count = 0\n",
    "indexes_to_remove = []\n",
    "for index, row in df_base.iterrows():\n",
    "    print(\"## Processing item:\", str(index))\n",
    "    filename = file_dir + '\\\\' + df_base.Document_Name[index]\n",
    "    pages_txt = []\n",
    "    \n",
    "    if (not(str(filename).endswith('found')) | (str(filename).endswith('downloaded'))):\n",
    " \n",
    "        # Read PDF file\n",
    "        data = parser.from_file(filename, xmlContent=True)\n",
    "        xhtml_data = BeautifulSoup(data['content'])\n",
    "        for i, content in enumerate(xhtml_data.find_all('div', attrs={'class': 'page'})):\n",
    "            # Parse PDF data using TIKA (xml/html)\n",
    "            # It's faster and safer to create a new buffer than truncating it\n",
    "            # https://stackoverflow.com/questions/4330812/how-do-i-clear-a-stringio-object\n",
    "            _buffer = StringIO()\n",
    "            _buffer.write(str(content))\n",
    "            parsed_content = parser.from_buffer(_buffer.getvalue())\n",
    "        \n",
    "            # Add pages\n",
    "            if parsed_content['content'] != None:    # page is not blank page\n",
    "                text = parsed_content['content'].strip()\n",
    "            else: \n",
    "                text = ''\n",
    "            \n",
    "            pages_txt.append(text)\n",
    "            \n",
    "            \n",
    "        # save results and report status:\n",
    "        df_base.at[index, 'Document_Content'] = pages_txt\n",
    "        doc_count += 1\n",
    "        print()\n",
    "        print(\"Completed doc index:\", str(index), \"Document number:\", str(doc_count))\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"Document not available\")\n",
    "        df_base.at[index, 'Document_Content'] = 'not available'\n",
    "        del pages_txt\n",
    "        del filename\n",
    "        print('------')\n",
    "        print()\n",
    "        indexes_to_remove.append(int(index))\n",
    "\n",
    "print()\n",
    "print('-------')\n",
    "print('Indexes to remove:', str(indexes_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.Document_Content[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.3 - Store content\n",
    "f_base = 'Loans_Documents-full_content_v01_2020-07-15.joblib'\n",
    "joblib.dump(df_base, './output/' + f_base + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['blank_pages'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.loc[indexes_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following indexes are removed from the dataframe: \n",
    "\n",
    "- [119, 120, 240, 249, 299, 300, 501, 542, 650, 651]\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes_to_remove since no document was downloaded:\n",
    "df_base.drop([119, 120, 240, 249, 299, 300, 501, 542, 650, 651], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional indexes to remove: \n",
    "\n",
    "- 301, 'DR-L1125_Contingent Loan for Natural Disaster... ' is just a one page resolution.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.drop([301], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Blank Pages (%) calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in df_base.iterrows():\n",
    "    print('## Processing index', str(index))\n",
    "    lista = df_base['Document_Content'][index]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == '':\n",
    "            count += 1\n",
    "    \n",
    "    df_base.at[index, 'blank_pages'] = format(count/len(lista)*100, '.4g')\n",
    "    print(str(count))\n",
    "    print('')\n",
    "    #count/len(lista)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Page count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['page_count'] = df_base['Document_Content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the document length distribution:\n",
    "\n",
    "P.figure()\n",
    "# the histogram of the data with histtype='step'\n",
    "n, bins, patches = P.hist(df_base.page_count.to_list(), bins, histtype='bar', rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_pages_per_document = df_base.blank_pages.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the document blank pages (%) distribution:\n",
    "\n",
    "P.figure()\n",
    "# the histogram of the data with histtype='step'\n",
    "bins = [0, 20, 40, 60, 80, 100]\n",
    "n, bins, patches = P.hist(df_base.blank_pages.to_list(), bins, histtype='bar', rwidth=0.8, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering \n",
    "\n",
    "#### Step_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 07/20, <b>consider</b> files under the following conditions:\n",
    "* documents with page_count > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_base[(df_base.page_count > 10)].copy()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the new results:\n",
    "P.figure()\n",
    "# the histogram of the data with histtype='step'\n",
    "n, bins, patches = P.hist(df_filtered.page_count.to_list(), bins, histtype='bar', rwidth=0.8, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_filtered.page_count, color='g').set_title('Document length distribution (in pages)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.8,6))\n",
    "sns.distplot(df_filtered.blank_pages).set_title('Blank Pages distribution (% of blank pages)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the previous result to work with:\n",
    "df_filtered_2 = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify & get the index page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the index language\n",
    "df_filtered_2['language'] = ''\n",
    "# stores the index page\n",
    "df_filtered_2['index_page'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review = []\n",
    "loan_count = 0\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    is_loan = False\n",
    "    for page in range(0,len(df_filtered_2.Document_Content[index])):\n",
    "        if re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('English - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'en'\n",
    "            match_title_type = re.search(r'(^(\\-(\\s+)?ii\\s+\\-s+)?CONTENTS?\\s+(PROJECT|PROGRAM) SUMMARY|^(\\-\\s+ii\\s+\\-s+)?CONTENTS?\\s+I\\.|^\\-(\\s+)?ii(\\s+)?\\-\\s+CONTENTS\\s+PROJECT\\s+SUMMARY\\s+)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "            \n",
    "        elif re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE):\n",
    "            print('index', str(index))\n",
    "            print('Spanish - index page found at page:', str(page))\n",
    "            loan_count += 1\n",
    "            is_loan = True\n",
    "            df_filtered_2.at[index, 'language'] = 'es'\n",
    "            match_title_type = re.search(r'((Í|I)NDICE\\s+RESUM\\s?EN (DEL? (PROYECTO|PROGRAMA)|EJECUTIVO)(\\.?\\…+|\\s+|\\.+)|ÍNDICE\\s+(\\d\\s+)?I\\.)', df_filtered_2.Document_Content[index][page], re.IGNORECASE).group()\n",
    "            df_filtered_2.at[index, 'index_page'] = page\n",
    "            print(match_title_type, page)\n",
    "            print('~ ~ ~')\n",
    "            break\n",
    "        \n",
    "        \n",
    "    if not is_loan: \n",
    "        print('check regex on:', str(index))\n",
    "        #df_filtered_2.at[index, 'doc_type'] = 'other'\n",
    "        #df_filtered_2.at[index, 'doc_identifier'] = ('na', 'na')\n",
    "        to_review.append(index)\n",
    "\n",
    "print('Loans identified:', str(loan_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes of documents to review:\n",
    "#len(to_review)\n",
    "\n",
    "df_filtered_2.loc[to_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_filtered_2.OPERATION_NUMBER.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing -intermediate- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.7: all loans\n",
    "f_df_resultado_loans = 'df_resultado_loans_2021-01-14_v07.joblib'\n",
    "joblib.dump(df_loans, './output/' + f_df_resultado_loans + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.7: all documents to Excel:\n",
    "df_loans.to_excel('loans_docs_2021-01-14_v07.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ~ ~ ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.6: all loans\n",
    "f_df_resultado_loans = 'df_resultado_loans_2020-11-04_v06.joblib'\n",
    "joblib.dump(df_loans, './output/' + f_df_resultado_loans + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.6: all documents to Excel:\n",
    "df_loans.to_excel('loans_docs_2020-11-04_v06.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ~ ~ ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.5: all loans\n",
    "f_df_resultado_loans = 'df_resultado_loans_2020-10-19_v05.joblib'\n",
    "joblib.dump(resultado, './output/' + f_df_resultado_loans + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.5: all documents to Excel:\n",
    "resultado.to_excel('loans_docs_2020-10-19_v05.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ~ ~ ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.4: Extracted and cleaned 1st part\n",
    "f_df_filtered_2_v04 = 'df_filtered_2_loans_2020-07-21_v04_Content_extracted_cleaned.joblib'\n",
    "\n",
    "joblib.dump(df_filtered_2, './output/' + f_df_filtered_2_v04 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ~ ~ ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.3: completed titles recognition from the index page - (some documents had titles modification in order to have a matching condition)\n",
    "f_df_filtered_2_v03 = 'df_filtered_2_loans_2020-07-21_v03.joblib'\n",
    "\n",
    "joblib.dump(df_filtered_2, './output/' + f_df_filtered_2_v03 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.2: loan read, index page and language detection\n",
    "f_df_filtered_2_v02 = 'df_filtered_2_loans_2020-07-21_v02.joblib'\n",
    "\n",
    "joblib.dump(df_filtered_2, './output/' + f_df_filtered_2_v02 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v0.1: loan read, index page and language detection\n",
    "f_df_filtered_2_v01 = 'df_filtered_2_loans_2020-07-16_v01.joblib'\n",
    "\n",
    "joblib.dump(df_filtered_2, './output/' + f_df_filtered_2_v01 + '.bz2', compress=('bz2', 3))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***********************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load previous/intermediate results (v02):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source file:\n",
    "df_filtered_2 = joblib.load('./output/df_filtered_2_loans_2020-07-21_v02.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read index page and Titles Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually adjusted: \n",
    "##### index 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.Document_Name[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of index page in document `ME-L1258_México. Propuesta de préstamo para el proyecto “Fortalecimiento de la Gestión de las Políticas de Promoción al Empleo”.pdf`: \n",
    "lista_aux = df_filtered_2['Document_Content'][85]\n",
    "lista_aux[3] = df_filtered_2['Document_Content'][85][3].replace('DESCRIPCIÓN DEL PROYECTO Y MONITOREO DE RESULTADOS', 'DESCRIPCIÓN DEL PROYECTO Y MONITOREO DE RESULTADOS 2')\n",
    "lista_aux[3] = df_filtered_2['Document_Content'][85][3].replace('ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS 16')\n",
    "lista_aux[3] = df_filtered_2['Document_Content'][85][3].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE IMPLEMENTACIÓN Y GESTIÓN 18')\n",
    "df_filtered_2.at[85, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_filtered_2.Document_Content[85][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store key index titles: \n",
    "df_filtered_2['index_titles'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_review = []\n",
    "# key titles are extracted along with their respective page number: \n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('*Processing index:', str(index))\n",
    "    key_titles = re.findall(r'[IV\\.]{1,5}\\s+[A-ZÁÉÍÓÚ\\s\\,\\n]+[\\.\\s\\-\\…]{0,200}\\d\\d?', df_filtered_2.Document_Content[index][df_filtered_2.index_page[index]])\n",
    "    print(key_titles)\n",
    "    if key_titles == []:\n",
    "        print('Found empty list on:', str(index))\n",
    "        #to_review.append(index)\n",
    "    else:\n",
    "        df_filtered_2.at[index, 'index_titles'] = key_titles\n",
    "        \n",
    "    print(\"~~~\")\n",
    "    print()\n",
    "#print(to_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['index_title_I'] = ''\n",
    "df_filtered_2['index_title_II'] = ''\n",
    "df_filtered_2['index_title_III'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the index titles and get main titles and pages:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print('* Processing index:', str(index))\n",
    "    for i in range(0,len(df_filtered_2.index_titles[index])):\n",
    "        resultado = tuple(re.findall(r'[A-ZÁÉÍÓÚ\\.\\s\\-\\…\\,\\n]+|\\d+', df_filtered_2.index_titles[index][i]))\n",
    "        #ini:\n",
    "        if (resultado[0].startswith('I.') or 'DESCRIP' in resultado[0]):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][2:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_I'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #medio:\n",
    "        elif (resultado[0].startswith('II.') or resultado[0].startswith('II ') or resultado[0].startswith('..... ESTRUCTURA')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_II'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        #fin:\n",
    "        elif (resultado[0].startswith('III') or resultado[0].startswith('.... PLAN')):\n",
    "            aux = (re.search(r'[A-ZÁÉÍÓÚ\\s\\,\\n]+', resultado[0][3:]).group().strip(),resultado[1])\n",
    "            print(aux)\n",
    "            df_filtered_2.at[index, 'index_title_III'] = aux\n",
    "            del aux\n",
    "            del resultado\n",
    "        else:\n",
    "            # do nothing\n",
    "            print('nothing')\n",
    "        \n",
    "    #del aux\n",
    "    print()\n",
    "    print('~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_I:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test looking for the page of title_III:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Indexes to check', indexes_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 263\n",
    "print(df_filtered_2['index_titles'][index])\n",
    "df_filtered_2['Document_Content'][index][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(df_filtered_2['Document_Content'][index][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document adjustments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Found the following documents which the index page titles do not had matches within the document contents:\n",
    "\n",
    "(Manually replaced content where indicated)\n",
    "\n",
    "Due to `title_I`:\n",
    "\n",
    "- index 99: 'GU-L1163_Guatemala. Loan proposal for the “Program to Strengthen the Institutional Healthcare Service Network (PRORISS).pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "- index 336: 'BL-L1029_Belize. Propuesta de préstamo para un “Financiamiento Adicional para el Proyecto de Rehabilitación de la Carretera George Price”.pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "- index 357: 'EC-L1235_Ecuador. Proposal for a loan for the project “Investment in The Quality of Child Development Services”.pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "- index 368: 'BO-L1198_Bolivia. Propuesta de préstamo para el “Programa de Mejora en la Accesibilidad a los.pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "- index 475: 'JA-L1085_Jamaica. Propuesta de Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) para el “Programa de Impulso a la Innovación, el Crecimiento y los Ecosistemas Empresariales”~s Empresariales”.pdf' - <b>Solution: </b> replaced title in content at page 8\n",
    "- index 509: 'UR-L1153_Uruguay. Propuesta de Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) para el “Programa de Mejora de Corredores Viales de Uso Agroindustrial y Forestal” y primer p~rial y Forestal”.pdf' - <b>Solution: </b> replaced title in content at page 8\n",
    "- index 525: 'BH-L1046_Bahamas. Propuesta de préstamo para el “Programa para Reforzar la Calidad Crediticia de Micro, Pequeñas y Medianas Empresas”.pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "- index 643: 'TT-L1058_Trinidad y Tobago. Propuesta de préstamo para el “Programa de Fortalecimiento de la Política Pública y la Gestión Fiscal para la Atención de la Crisis Sanitaria y Económica Causada~inidad y Tobago”.pdf' - <b>Solution: </b> replaced title in content at page 7\n",
    "\n",
    "<br>\n",
    "Due to `title_II`:\n",
    "indexes to review/modify: [15, 83, 85, 336, 473, 475, 509, 561]\n",
    "\n",
    "\n",
    "<br>\n",
    "Due to `title_II`:\n",
    "indexes to review/modify: [53, 132, 146, 148, 158, 182, 336, 432, 475, 525]\n",
    "\n",
    "\n",
    "<br>\n",
    "Also, removed from the dataframe: \n",
    "- index 304: 'CR-L1135_LP - Paquete aprobado - CR-L1135.pdf'\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to `title_III`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 525 # index of document to modify\n",
    "page = 25  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE EJECUCIÓN Y ADMINISTRACIÓN DEL PROGRAMA')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 475 # index of document to modify\n",
    "page = 27  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE EJECUCIÓN Y ADMINISTRACIÓN', 'PLAN DE EJECUCIÓN Y ADMINISTRACIÓN DEL PROYECTO')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 432 # index of document to modify\n",
    "page = 24  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE IMPLEM ENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 336 # index of document to modify\n",
    "page = 26  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE EJECUCIÓN Y ADMINISTRACIÓN', 'PLAN DE IMPLEMENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 182 # index of document to modify\n",
    "page = 25  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('Plan de Implementación y Gestión', 'PLAN DE IMPLEMENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 158 # index of document to modify\n",
    "page = 27  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE IMPLEM ENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 148 # index of document to modify\n",
    "page = 20  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE IMPLEM ENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 146 # index of document to modify\n",
    "page = 26  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'IMPLEMENTACIÓN Y PLAN DE GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 132 # index of document to modify\n",
    "page = 23  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('PLAN DE IMPLEMENTACIÓN Y GESTIÓN', 'PLAN DE IMPLEM ENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 53 # index of document to modify\n",
    "page = 21  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('Plan de Implementación y Gestión', 'PLAN DE IMPLEMENTACIÓN Y GESTIÓN')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to `title_II`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 561 # index of document to modify\n",
    "page = 19  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('Estructura de Financiamiento y Principales Riesgos', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 509 # index of document to modify\n",
    "page = 16  # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA FINANCIERA Y PRINCIPALES RIESGOS', 'ESTRUCTURA FINANCIERA Y RIESGOS PRINCIPALES')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 475 # index of document to modify\n",
    "page =  23 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y RIESGOS PRINCIPALES')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 473 # index of document to modify\n",
    "page =  27 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DE FINANCIAMIENTO Y RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 336 # index of document to modify\n",
    "page =  23 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DE FINANCIAMIENTO Y RIESGOS PRINCIPALES', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 85 # index of document to modify\n",
    "page =  22 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DEL PRÉSTAMO Y PRINCIPALES RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 83 # index of document to modify\n",
    "page =  29 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DE FINANCIAMIENTO Y RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15 # index of document to modify\n",
    "page =  18 # location of the title to be modified\n",
    "lista_aux = df_filtered_2['Document_Content'][index]\n",
    "lista_aux[page] = df_filtered_2['Document_Content'][index][page].replace('ESTRUCTURA DEL PRÉSTAMO Y PRINCIPALES RIESGOS', 'ESTRUCTURA DE FINANCIAMIENTO Y PRINCIPALES RIESGOS')\n",
    "df_filtered_2.at[index, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to `title_I`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of index page in document 'GU-L1163_Guatemala. Loan proposal for the “Program to Strengthen the Institutional Healthcare Service Network (PRORISS).pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][99]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][99][7].replace('PROJECT DESCRIPTION AND RESULTS MONITORING', 'PROGRAM DESCRIPTION AND RESULTS MONITORING')\n",
    "df_filtered_2.at[99, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'BL-L1029_Belize. Propuesta de préstamo para un “Financiamiento Adicional para el Proyecto de Rehabilitación de la Carretera George Price”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][336]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][336][7].replace('DESCRIPCIÓN Y SEGUIMIENTO DE RESULTADOS', 'DESCRIPCIÓN Y SUPERVISIÓN DE RESULTADOS')\n",
    "df_filtered_2.at[336, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'EC-L1235_Ecuador. Proposal for a loan for the project “Investment in The Quality of Child Development Services”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][357]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][357][7].replace('DESCRIPTION AND RESULTS MONITORING', 'PROJECT DESCRIPTION AND RESULTS MONITORING')\n",
    "df_filtered_2.at[357, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'BO-L1198_Bolivia. Propuesta de préstamo para el “Programa de Mejora en la Accesibilidad a los.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][368]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][368][7].replace('DESCRIPCIÓN DEL PROGRAMA Y MONITOREO DE RESULTADOS', 'DESCRIPCIÓN DEL PROYECTO Y MONITOREO DE RESULTADOS')\n",
    "df_filtered_2.at[368, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'JA-L1085_Jamaica. Propuesta de Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) para el “Programa de Impulso a la Innovación, el Crecimiento y los Ecosistemas Empresariales”~s Empresariales”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][475]\n",
    "lista_aux[8] = df_filtered_2['Document_Content'][475][8].replace('DESCRIPCIÓN Y SEGUIMIENTO DE RESULTADOS', 'DESCRIPCIÓN Y SEGUIMIENTO DE LOS RESULTADOS DEL PROGRAMA')\n",
    "df_filtered_2.at[475, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'UR-L1153_Uruguay. Propuesta de Línea de Crédito Condicional para Proyectos de Inversión (CCLIP) para el “Programa de Mejora de Corredores Viales de Uso Agroindustrial y Forestal” y primer p~rial y Forestal”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][509]\n",
    "lista_aux[8] = df_filtered_2['Document_Content'][509][8].replace('DESCRIPCIÓN DEL PROYECTO Y MONITOREO DE RESULTADOS', 'DESCRIPCIÓN Y MONITOREO DE RESULTADOS')\n",
    "df_filtered_2.at[509, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'BH-L1046_Bahamas. Propuesta de préstamo para el “Programa para Reforzar la Calidad Crediticia de Micro, Pequeñas y Medianas Empresas”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][525]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][525][7].replace('DESCRIPCIÓN DEL PROYECTO Y SEGUIMIENTO DE RESULTADOS', 'DESCRIPCIÓN Y SEGUIMIENTO DE LOS RESULTADOS DEL PROYECTO')\n",
    "df_filtered_2.at[525, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change of title based on index page in document 'TT-L1058_Trinidad y Tobago. Propuesta de préstamo para el “Programa de Fortalecimiento de la Política Pública y la Gestión Fiscal para la Atención de la Crisis Sanitaria y Económica Causada~inidad y Tobago”.pdf': \n",
    "lista_aux = df_filtered_2['Document_Content'][643]\n",
    "lista_aux[7] = df_filtered_2['Document_Content'][643][7].replace('DESCRIPCIÓN Y SEGUIMIENTO DE RESULTADOS DEL PROYECTO', 'OBJETIVOS, DESCRIPCIÓN Y SEGUIMIENTO DE RESULTADOS')\n",
    "df_filtered_2.at[643, 'Document_Content'] = lista_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed from dataframe index 304: 'CR-L1135_LP - Paquete aprobado - CR-L1135.pdf', referring to Nate Storm: document has different format\n",
    "df_filtered_2.drop([304], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the results:\n",
    "df_filtered_2['true_title_I'] = ''\n",
    "df_filtered_2['true_title_II'] = ''\n",
    "df_filtered_2['true_title_III'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_I location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = int(df_filtered_2['index_page'][index]) + 1 # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_I'][index][0][:-1], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_I'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2['true_title_I'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_II location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_I'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_II'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_II'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### true_title_III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexes_to_check = []\n",
    "\n",
    "# identify true_title_III location:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    print()\n",
    "    pattern_found = False\n",
    "    print('Processsing index:', str(index))\n",
    "    \n",
    "    page_base = df_filtered_2['true_title_II'][index][1]  # starting page\n",
    "    \n",
    "    for i in range(page_base,len(df_filtered_2['Document_Content'][index])):\n",
    "        if re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]) != None: # pattern found\n",
    "            \n",
    "            print('* pattern found at document page:', str(i))\n",
    "            ## storing:\n",
    "            inicial_match_title = re.search(df_filtered_2['index_title_III'][index][0], df_filtered_2['Document_Content'][index][i]).group()\n",
    "            inicial_match_page = i\n",
    "            df_filtered_2.at[index, 'true_title_III'] = (inicial_match_title, inicial_match_page)\n",
    "            ##\n",
    "            print('-----------------    -----------------')\n",
    "            pattern_found = True\n",
    "            break\n",
    "            \n",
    "    if not pattern_found: \n",
    "        print('check regex on:', str(index))\n",
    "        indexes_to_check.append(index)\n",
    "print('Index to check', indexes_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for crossed titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered_2.iterrows():\n",
    "    if (df_filtered_2.true_title_I[index][1] < df_filtered_2.true_title_II[index][1] < df_filtered_2.true_title_III[index][1]):\n",
    "        print('Sequence OK for index:', str(index))\n",
    "    \n",
    "    elif (df_filtered_2.true_title_III[index][1]> df_filtered_2.true_title_I[index][1] > df_filtered_2.true_title_II[index][1]):\n",
    "        print('middle title before the first title on index:', str(index))\n",
    "        \n",
    "    else: \n",
    "        print('other case on:', str(index))\n",
    "        \n",
    "    #if (df_filtered_2.true_title_III[index][1] - df_filtered_2.true_title_I[index][1]) > 10: # alert on cases where extension between titles is greater than 10\n",
    "    #    print('File to check due to extension between titles:', df_filtered_2['Document_Name'][index])\n",
    "    #    print((df_filtered_2.true_title_I[index][0], df_filtered_2.true_title_I[index][1]), (df_filtered_2.true_title_II[index][0], df_filtered_2.true_title_II[index][1]), (df_filtered_2.true_title_III[index][0], df_filtered_2.true_title_III[index][1]))\n",
    "    #    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.loc[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OK - save v0.3 07/21/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### footer and header clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filtered_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the extracted content:\n",
    "df_filtered_2['extracted'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing section (pre v1.0)\n",
    "for index in [669]:\n",
    "#for index, row in df_filtered_2.iterrows():\n",
    "    page_ini = df_filtered_2.true_title_I[index][1]\n",
    "    page_fin = df_filtered_2.true_title_II[index][1]\n",
    "    \n",
    "    print('*** Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_filtered_2['Document_Content'][index][j]\n",
    "        \n",
    "        print('+ - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + ')\n",
    "        print(repr(page))\n",
    "        print('+ - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + ')\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}\\d\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep)([A-Z\\¿\\“]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n?\\n?\\n\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep)([A-Z\\¿\\“]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    ini = re.search(df_filtered_2['index_title_I'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    texto = texto[ini:fin].strip()[:-3]\n",
    "    print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_filtered_2.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('~~~ *** ~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean-up routine (v1.0)\n",
    "#for index in [30]:\n",
    "for index, row in df_filtered_2.iterrows():\n",
    "    page_ini = df_filtered_2.true_title_I[index][1]\n",
    "    page_fin = df_filtered_2.true_title_II[index][1]\n",
    "    \n",
    "    print('*** Processing index: ', str(index), ' - page range:', str(page_ini),str(page_fin))\n",
    "    texto = ''\n",
    "    for j in range(page_ini,page_fin+1):\n",
    "\n",
    "        page = df_filtered_2['Document_Content'][index][j]\n",
    "        \n",
    "        print('+ - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + ')\n",
    "        print(repr(page))\n",
    "        #print('+ - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + - - - + ')\n",
    "        \n",
    "        # header cleanup:\n",
    "        page = re.sub(r'^\\s?\\-\\s{0,3}\\d\\d?\\s{0,3}\\-', '', page)\n",
    "        \n",
    "        # check for footnote and remove:\n",
    "        if re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page) != None:    # 1st type of footnote found!\n",
    "            print('* Footnote pattern 1: \\'30+ blanks + digit\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\s{30,}\\d{1,2}\\s+[A-Z]', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "                       \n",
    "        # footnotes - pending\n",
    "        elif re.search(r'\\n\\n?\\n?\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep)([A-Z\\¿\\“]|http)', page) != None: #  2nd type of footnote found!\n",
    "            print('* Footnote 2: \\'2 or 3 blanks + 1 or 2 digits\\' at:', str(j))\n",
    "            # cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n\\n?\\n?\\d\\d?\\s{1,2}(?!Información\\s|Objetivos\\s|Descripción\\s|Presupuesto\\s|May|Jun|Jul|Ago|Sep|Set|Oct|Nov|Dic|PMRep)([A-Z\\¿\\“]|http)', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        elif re.search(r'\\n+\\xa0+\\n\\d', page) != None: # 3rd type of footnote found!\n",
    "            print('* Footnote 3: \\'xa0 type\\' at:', str(j))\n",
    "            #  cut footnote area:\n",
    "            page_clean = page[:re.search(r'\\n+\\xa0+\\n\\d', page).span()[0]]\n",
    "            texto = texto + ''.join(page_clean)\n",
    "\n",
    "        else: \n",
    "            texto = texto + ''.join(page)\n",
    "            \n",
    "    texto = re.sub(r'https?[\\:\\/a-zA-Z0-9\\.\\?\\=\\-\\_\\%\\&\\;]+', ' ', texto)\n",
    "    \n",
    "    # cutting sections based on titles\n",
    "    ini = re.search(df_filtered_2['index_title_I'][index][0][:-1], texto).span()[0]\n",
    "    \n",
    "\n",
    "    #if re.search(r'Presupuesto (I|i)ndicativo', texto) != None:  # search for 'Presupuesto Indicativo'\n",
    "    #    fin = re.search(r'Presupuesto (I|i)ndicativo', texto).span()[0]\n",
    "    #    \n",
    "    #else:   # search for pattern_3, as border condition\n",
    "    #    fin = re.search(pattern_es_3, texto, re.IGNORECASE).span()[0]\n",
    "    \n",
    "    fin = re.search(df_filtered_2['index_title_II'][index][0], texto).span()[0]\n",
    "    texto = texto[ini:fin].strip()[:-3]\n",
    "    print(texto)\n",
    "    \n",
    "    # store extracted content in dataframe\n",
    "    df_filtered_2.at[index, 'extracted'] = texto\n",
    "    \n",
    "    del texto\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('~~~ *** ~~~')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (store results as v0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### supra-indexes and extra-blank spaces removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered_2['extracted'][669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cleaned content storing:\n",
    "df_filtered_2['extracted_cleaned'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_filtered_2.iterrows():\n",
    "    texto = df_filtered_2['extracted'][index].split()\n",
    "    resultado = [\"\".join(filter(lambda x: not x.isdigit(), word)) if re.search(r'[A-Za-záéíóú\\-\\)\\”]+\\d{1,2}\\.?$', word) else word for word in texto]\n",
    "    res_clean = ' '.join(resultado)\n",
    "    df_filtered_2.at[index, 'extracted_cleaned'] = res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.extracted_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(stored as v0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID',\n",
    "       'DOCUMENT_REFERENCE', 'DESCRIPTION', 'Document_Name',\n",
    "       'Document_Status', 'blank_pages', 'page_count',\n",
    "       'language', 'index_titles', 'true_title_I', 'true_title_II',\n",
    "       'extracted', 'extracted_cleaned']].to_excel('Loans_Docs_Collection_Processed_2020-07-21_v04.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 0.7           01/14/2021        Emiliano Colina    - processed Loans from Nov & Dec 2020\n",
    "#\n",
    "\n",
    "#   - 0.6           11/04/2020        Emiliano Colina    - processed Loans approved in October 2020\n",
    "#\n",
    "\n",
    "#   - 0.5           10/19/2020        Emiliano Colina    - processed Loans approved between July and Sept 2020\n",
    "# \n",
    "\n",
    "#   - 0.4           07/21/2020        Emiliano Colina    - content extracted and cleaned\n",
    "#                                                        \n",
    "                                                        \n",
    "#   - 0.3           07/21/2020        Emiliano Colina    - title search on documents and content extracted\n",
    "\n",
    "#   - 0.2           07/20/2020        Emiliano Colina    - index titles recognition\n",
    "                                                                                                                  \n",
    "#   - 0.1           07/16/2020        Emiliano Colina    - Initial version, Loan documents read, index page and      \n",
    "#                                                        language detection\n",
    "\n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
