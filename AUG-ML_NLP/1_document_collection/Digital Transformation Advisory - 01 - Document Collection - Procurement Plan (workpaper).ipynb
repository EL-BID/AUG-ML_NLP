{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Document Collection - Procurement Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: to download the procurement plans/plan de adquisiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 01 - Document Collection - Procurement Plan\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): Documents stored at IDBDocs, IDB SharePoint & ezShare\n",
    "#-- Description: Download to a local folder all the selected documents:\n",
    "#                - Procurement Plan\n",
    "#                - \n",
    "#\n",
    "#                                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  0.1\n",
    "#-- Last Update: 10/25/2020\n",
    "#-- Last Revision Date: 10/25/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######       \n",
    "file_extension = re.compile('\\.[a-zA-Z]{3}[a-zA-Z]?$')  # regular expression corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_check(url):\n",
    "    '''\n",
    "    Description: Checks how to connect to a sharepoint/ezshare or idbdocs repository to download a file.\n",
    "    (based on the pcr_validate() function)\n",
    "    @author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - initial version (07/07/2020)\n",
    "    '''\n",
    "    \n",
    "    url = url.strip() # remove trailing white spaces\n",
    "    \n",
    "    if ('idbdocs' in url) or ('ezws' in url):\n",
    "        \n",
    "        # protocol and host adjustment\n",
    "        if url.startswith('http:'): # replace http with https\n",
    "            url = url.replace('http', 'https')\n",
    "        elif url.startswith('https://ezws'): # replace the ezws host with idbdocs\n",
    "            url = url.replace('https://ezws', 'https://idbdocs')\n",
    "\n",
    "        if 'EZSHARE' in url: \n",
    "            # Connect once using the cookie_2 (from idbdocs) to get the last url (in sharepoint), and then\n",
    "            # connect using the sharepoint cookie\n",
    "            return('connect_twice',url)\n",
    "\n",
    "        else: \n",
    "            # connect using the cookie_2 (from idbdocs)\n",
    "            return('idbdocs_directly',url)\n",
    "            #r = requests.get(df_2['Link Descarga'][index], headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    else:\n",
    "        if 'www.iadb.org' in url:\n",
    "            if 'EZSHARE' in url:\n",
    "                url = url.replace('https://www.iadb.org/Document.cfm?id=', 'https://idbdocs.iadb.org/wsdocs/getdocument.aspx?docnum=')\n",
    "                return('connect_twice',url)\n",
    "            \n",
    "        else: \n",
    "            if 'sharepoint' in url:\n",
    "                \n",
    "                return('sharepoint_directly', sharepoint_adjust(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharepoint_adjust(original):\n",
    "    '''\n",
    "    Description: Adjusts a sharepoint url to download the file it points to.\n",
    "    @author: camilode; emilianoco\n",
    "    \n",
    "    Version: \n",
    "        - v0.3 - added control for trailing parameters in URL (07/04/2020)\n",
    "        - v0.2 - Added control for url with path 'WopiFrame.aspx' \n",
    "        - v0.1 - (01/09/2020)\n",
    "    '''\n",
    "    #posicion_corte = 0\n",
    "    #del posicion_corte\n",
    "    if '{' in original:\n",
    "        original = original.replace('{','%7B')\n",
    "    if '}' in original:\n",
    "        original = original.replace('}', '%7D')\n",
    "    \n",
    "    if '%7D' in original:\n",
    "        posicion_corte = original.find('%7D')\n",
    "        original = original[:posicion_corte]\n",
    "    #print(original)\n",
    "\n",
    "    if 'Doc.aspx' in original: \n",
    "        original = original.replace('Doc.aspx?sourcedoc=%7B','download.aspx?UniqueId=')\n",
    "    \n",
    "    if 'WopiFrame.aspx' in original:\n",
    "        original = original.replace('WopiFrame.aspx?sourcedoc=%7B','download.aspx?UniqueId=')\n",
    "\n",
    "    return(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content(req):\n",
    "    '''\n",
    "    Description: Checks for certain messages/errors in a request content\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - initial version - (07/07/2020)\n",
    "    '''\n",
    "    if 'could not be found in Sharepoint EzShare' in str(req.content):\n",
    "        return('not found')\n",
    "    elif ('AccessDenied.aspx' in str(req.content)) or ('does not have permissions to access this resource' in str(req.content)): \n",
    "        return('access denied')\n",
    "    else:\n",
    "        return('content undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_dir, req, name_prefix=''):\n",
    "    '''\n",
    "    Description: Saves to 'file_dir' the file under 'req' as 'file_name', obtained either from the URL or \n",
    "    the HTTP response. Optional parameter added to insert a prefix to the file_name.\n",
    "    \n",
    "    The function also controls 'file_dir' + 'file_name' lengths to avoid OS constraints.\n",
    "    \n",
    "    To control duplicates and not overwrite already downloaded files, the function iterates over\n",
    "    the destination folder and adds a counter if the 'file_name' is already present.\n",
    "    \n",
    "    @ author: emilianoco\n",
    "    \n",
    "    Version:\n",
    "        - v0.4 - optional parameter (07/07/2020)\n",
    "        - v0.3 - (06/17/2020)\n",
    "        - v0.2 - (06/16/2020)\n",
    "        - v0.1 - (01/02/2020)\n",
    "    '''\n",
    "    \n",
    "    if req.headers.get('Content-Disposition') == None: \n",
    "        #file_name not in 'Content-Disposition' but in in url - usually sharepoint\n",
    "        if '&file=' in requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace'):\n",
    "            \n",
    "            # case where the file_name is defined in parameter &file, usually a 'docx' file\n",
    "            file_name_orig = requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace').split('&file=')[-1].split('&')[0]\n",
    "            \n",
    "            # the request url needs to be re-written and a new connection is required:\n",
    "            req = requests.get(sharepoint_adjust(req.url), headers = h_sharepoint, allow_redirects = True) \n",
    "            \n",
    "        else:\n",
    "            file_name_orig = requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace')\n",
    "    else:\n",
    "        #file_name extracted from the content - usually idbdocs\n",
    "        file_name_orig = requests.utils.unquote(req.headers['Content-Disposition'].split('filename=')[-1].encode('latin-1').decode('utf-8')).replace('\"', '')\n",
    "    \n",
    "    \n",
    "    # Set name_prefix (v0.4)\n",
    "    if name_prefix != '':\n",
    "        #not empty\n",
    "        name_prefix = name_prefix + '_'\n",
    "    \n",
    "    \n",
    "    # Check file_name length (v0.4)\n",
    "    if len(file_dir + name_prefix + file_name_orig) > 240: \n",
    "        file_name_ini = file_name_orig[0:180]\n",
    "        file_name_fin = file_name_orig[-20:]\n",
    "        file_name = name_prefix + file_name_ini + '~' + file_name_fin\n",
    "    else:\n",
    "        file_name = name_prefix + file_name_orig\n",
    "    \n",
    "\n",
    "    # Check if file_name already present in destination folder (v0.4)\n",
    "    if file_name in os.listdir(file_dir):\n",
    "        file_name = ''.join(file_name.split('.')[:-1]) + '_' + '%s' + str('.') + file_name.split('.')[-1]\n",
    "        i = 1\n",
    "        while os.path.exists(file_dir + '\\\\' + file_name %i):\n",
    "            i += 1\n",
    "    \n",
    "        # Save the file\n",
    "        with open(file_dir + '\\\\' + file_name %i, 'wb') as f:\n",
    "            f.write(req.content)\n",
    "        print('Downloaded: ' + file_name %i)   #v0.3\n",
    "        return file_name %i \n",
    "    \n",
    "    else:\n",
    "        with open(file_dir + '\\\\' + file_name, 'wb') as f:\n",
    "            f.write(req.content)\n",
    "        print('Downloaded: ' + file_name)\n",
    "        return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headers configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following variables must be set! ###\n",
    "cookie_idbdocs = 'XXXXX'    ## <----\n",
    "cookie_sharepoint = 'YYYYY' ## <----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idbdocs\n",
    "h_idbdocs = {\n",
    "    'method': 'GET',\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'cookie': cookie_idbdocs,\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36', #Chrome\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharepoint\n",
    "h_sharepoint = {\n",
    "    'method': 'GET',\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'cookie': cookie_sharepoint,\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36', #Chrome\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cookies & headers clean-up:\n",
    "#del cookie_idbdocs\n",
    "#del cookie_sharepoint\n",
    "#del h_idbdocs\n",
    "#del h_sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procurement Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source xlsx file:\n",
    "data_ = pd.read_excel(r\"./input/Data-30 Sep 2020-All documents.xlsx\", sheet_name='procurement_plan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_.columns)\n",
    "data = data_[['OPERATION_ID', 'operation_number', 'Oper_type', 'Country', 'Region', 'Sector', 'Sector_Subsector', 'OPERATION_NAME_ES', \\\n",
    "              'OPERATION_YEAR', 'APPROVAL_DATE', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procurement Documents Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the filtered dataframe, and we'll add additional columns for storing the results:\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the document's name and its status, i.e.: 'downloaded', exception message \n",
    "df['Document_Name'] = '' #\n",
    "df['Document_Status'] = '' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document_URL'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are operations with 'NULL' value in the URL.\n",
    "<br>\n",
    "Using the EzShare code, fill in the `URL_2` field by including the common idbdocs url to request as:\n",
    "URL base: `\"https://idbdocs.iadb.org/wsdocs/getDocument.aspx?DOCNUM=\"` + `\"Codigo_EZSHARE\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_base = 'https://idbdocs.iadb.org/wsdocs/getDocument.aspx?DOCNUM='\n",
    "\n",
    "# new column to store idbdocs_url + ezshare_code:\n",
    "df['URL_2'] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print('Processing index:', str(index))\n",
    "    url = url_base + df['DOCUMENT_REFERENCE'][index].strip()\n",
    "    df.at[index, 'URL_2'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination folder setup: all files will be downloaded to 'file_dir'\n",
    "\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\procur_plans\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'operation_number':'OPERATION_NUMBER'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Documents collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df['URL_2'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir, r, df['OPERATION_NUMBER'][index]) \n",
    "        df.at[index, 'Document_Name'] = file_name\n",
    "        df.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir, s, df['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df.at[index, 'Document_Name'] = file_name\n",
    "                                df.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pending_1 = df[df.Document_Status.str.contains('Errno 2')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Document_Status == 'not found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['URL_2'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset=['OPERATION_NUMBER'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sort_values(['OPERATION_NUMBER', 'DOCUMENT_ID']).duplicated(subset=['OPERATION_NUMBER'], keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.sort_values(['OPERATION_NUMBER', 'DOCUMENT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to excel:\n",
    "df_test[~df_test.duplicated(subset=['OPERATION_NUMBER'], keep='last')].to_excel('./output/Procurement_Plan_doc_collection_filtered.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save completed df:\n",
    "# Store results: \n",
    "# v0.1 - 10/25: \n",
    "joblib.dump(df, './output/' + 'Procurment_Plan_Doc_Collection_2020-10-25_all.joblib' + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,\n",
    "                       truncate_sheet=False, \n",
    "                       **to_excel_kwargs):\n",
    "    \"\"\"\n",
    "    Append a DataFrame [df] to existing Excel file [filename]\n",
    "    into [sheet_name] Sheet.\n",
    "    If [filename] doesn't exist, then this function will create it.\n",
    "\n",
    "    Parameters:\n",
    "      filename : File path or existing ExcelWriter\n",
    "                 (Example: '/path/to/file.xlsx')\n",
    "      df : dataframe to save to workbook\n",
    "      sheet_name : Name of sheet which will contain DataFrame.\n",
    "                   (default: 'Sheet1')\n",
    "      startrow : upper left cell row to dump data frame.\n",
    "                 Per default (startrow=None) calculate the last row\n",
    "                 in the existing DF and write to the next row...\n",
    "      truncate_sheet : truncate (remove and recreate) [sheet_name]\n",
    "                       before writing DataFrame to Excel file\n",
    "      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n",
    "                        [can be dictionary]\n",
    "\n",
    "    Returns: None\n",
    "\n",
    "    (c) [MaxU](https://stackoverflow.com/users/5741205/maxu?tab=profile)\n",
    "    \"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    # Python 2.x: define [FileNotFoundError] exception if it doesn't exist \n",
    "    try:\n",
    "        FileNotFoundError\n",
    "    except NameError:\n",
    "        FileNotFoundError = IOError\n",
    "\n",
    "\n",
    "    try:\n",
    "        # try to open an existing workbook\n",
    "        writer.book = load_workbook(filename)\n",
    "        \n",
    "        # get the last row in the existing Excel sheet\n",
    "        # if it was not specified explicitly\n",
    "        if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "            startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "        # truncate sheet\n",
    "        if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "            # index of [sheet_name] sheet\n",
    "            idx = writer.book.sheetnames.index(sheet_name)\n",
    "            # remove [sheet_name]\n",
    "            writer.book.remove(writer.book.worksheets[idx])\n",
    "            # create an empty sheet [sheet_name] using old index\n",
    "            writer.book.create_sheet(sheet_name, idx)\n",
    "        \n",
    "        # copy existing sheets\n",
    "        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    except FileNotFoundError:\n",
    "        # file does not exist yet, we will create it\n",
    "        pass\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_df_to_excel('./input/Data-30 Sep 2020-All documents - original.xlsx', df, sheet_name='documents_status', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_df_to_excel('./input/Data-30 Sep 2020-All documents - original.xlsx', df_test[~df_test.duplicated(subset=['OPERATION_NUMBER'], keep='last')], sheet_name='duplicates_filtered', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_file_name = './input/Data-30 Sep 2020-All documents.xlsx' # file name\n",
    "### Output to new Excel containing each test on a different sheet\n",
    "\n",
    "#with pd.ExcelWriter(output_file_name) as writer:\n",
    "#    df.to_excel(writer, sheet_name='documents_status')\n",
    "#    df_test[~df_test.duplicated(subset=['OPERATION_NUMBER'], keep='last')].to_excel(writer, sheet_name='duplicates_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[~df_test.duplicated(subset=['OPERATION_NUMBER'], keep='last')][df_test.Document_Status != 'not found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# ******************************************************************************************************************** #\n",
    "# *************************************************  Version Control  ************************************************ #\n",
    "# ******************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                    Change:                                          #                                                          \n",
    "\n",
    "#   - 0.1           10/25/2020        Emiliano Colina       - Initial version, including filtering duplicates\n",
    "#                                                            \n",
    "\n",
    "#\n",
    "# ******************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
