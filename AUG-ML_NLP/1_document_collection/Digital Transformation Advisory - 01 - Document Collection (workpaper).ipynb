{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Transformation Advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: to download a defined set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 01 - Document Collection\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): Documents stored at IDBDocs, IDB SharePoint & ezShare\n",
    "#-- Description: Download to a local folder all the selected documents:\n",
    "#                - TCs 'Approval Registry' (excluded on 06/18)\n",
    "#                - TCs 'Approval Documents' and 'NULL' URL ones\n",
    "#                - Loans\n",
    "#                - Grants\n",
    "#                                \n",
    "#\n",
    "#-- @author:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  0.7\n",
    "#-- Last Update: 07/15/2020\n",
    "#-- Last Revision Date: 07/15/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######       \n",
    "file_extension = re.compile('\\.[a-zA-Z]{3}[a-zA-Z]?$')  # regular expression corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_check(url):\n",
    "    '''\n",
    "    Description: Checks how to connect to a sharepoint/ezshare or idbdocs repository to download a file.\n",
    "    (based on the pcr_validate() function)\n",
    "    @author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - initial version (07/07/2020)\n",
    "    '''\n",
    "    \n",
    "    url = url.strip() # remove trailing white spaces\n",
    "    \n",
    "    if ('idbdocs' in url) or ('ezws' in url):\n",
    "        \n",
    "        # protocol and host adjustment\n",
    "        if url.startswith('http:'): # replace http with https\n",
    "            url = url.replace('http', 'https')\n",
    "        elif url.startswith('https://ezws'): # replace the ezws host with idbdocs\n",
    "            url = url.replace('https://ezws', 'https://idbdocs')\n",
    "\n",
    "        if 'EZSHARE' in url: \n",
    "            # Connect once using the cookie_2 (from idbdocs) to get the last url (in sharepoint), and then\n",
    "            # connect using the sharepoint cookie\n",
    "            return('connect_twice',url)\n",
    "\n",
    "        else: \n",
    "            # connect using the cookie_2 (from idbdocs)\n",
    "            return('idbdocs_directly',url)\n",
    "            #r = requests.get(df_2['Link Descarga'][index], headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    else:\n",
    "        if 'www.iadb.org' in url:\n",
    "            if 'EZSHARE' in url:\n",
    "                url = url.replace('https://www.iadb.org/Document.cfm?id=', 'https://idbdocs.iadb.org/wsdocs/getdocument.aspx?docnum=')\n",
    "                return('connect_twice',url)\n",
    "            \n",
    "        else: \n",
    "            if 'sharepoint' in url:\n",
    "                \n",
    "                return('sharepoint_directly', sharepoint_adjust(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharepoint_adjust(original):\n",
    "    '''\n",
    "    Description: Adjusts a sharepoint url to download the file it points to.\n",
    "    @author: camilode; emilianoco\n",
    "    \n",
    "    Version: \n",
    "        - v0.3 - added control for trailing parameters in URL (07/04/2020)\n",
    "        - v0.2 - Added control for url with path 'WopiFrame.aspx' \n",
    "        - v0.1 - (01/09/2020)\n",
    "    '''\n",
    "    #posicion_corte = 0\n",
    "    #del posicion_corte\n",
    "    if '{' in original:\n",
    "        original = original.replace('{','%7B')\n",
    "    if '}' in original:\n",
    "        original = original.replace('}', '%7D')\n",
    "    \n",
    "    if '%7D' in original:\n",
    "        posicion_corte = original.find('%7D')\n",
    "        original = original[:posicion_corte]\n",
    "    #print(original)\n",
    "\n",
    "    if 'Doc.aspx' in original: \n",
    "        original = original.replace('Doc.aspx?sourcedoc=%7B','download.aspx?UniqueId=')\n",
    "    \n",
    "    if 'WopiFrame.aspx' in original:\n",
    "        original = original.replace('WopiFrame.aspx?sourcedoc=%7B','download.aspx?UniqueId=')\n",
    "\n",
    "    return(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content(req):\n",
    "    '''\n",
    "    Description: Checks for certain messages/errors in a request content\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - initial version - (07/07/2020)\n",
    "    '''\n",
    "    if 'could not be found in Sharepoint EzShare' in str(req.content):\n",
    "        return('not found')\n",
    "    elif ('AccessDenied.aspx' in str(req.content)) or ('does not have permissions to access this resource' in str(req.content)): \n",
    "        return('access denied')\n",
    "    else:\n",
    "        return('content undefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_dir, req, name_prefix=''):\n",
    "    '''\n",
    "    Description: Saves to 'file_dir' the file under 'req' as 'file_name', obtained either from the URL or \n",
    "    the HTTP response. Optional parameter added to insert a prefix to the file_name.\n",
    "    \n",
    "    The function also controls 'file_dir' + 'file_name' lengths to avoid OS constraints.\n",
    "    \n",
    "    To control duplicates and not overwrite already downloaded files, the function iterates over\n",
    "    the destination folder and adds a counter if the 'file_name' is already present.\n",
    "    \n",
    "    @ author: emilianoco\n",
    "    \n",
    "    Version:\n",
    "        - v0.4 - optional parameter (07/07/2020)\n",
    "        - v0.3 - (06/17/2020)\n",
    "        - v0.2 - (06/16/2020)\n",
    "        - v0.1 - (01/02/2020)\n",
    "    '''\n",
    "    \n",
    "    if req.headers.get('Content-Disposition') == None: \n",
    "        #file_name not in 'Content-Disposition' but in in url - usually sharepoint\n",
    "        if '&file=' in requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace'):\n",
    "            \n",
    "            # case where the file_name is defined in parameter &file, usually a 'docx' file\n",
    "            file_name_orig = requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace').split('&file=')[-1].split('&')[0]\n",
    "            \n",
    "            # the request url needs to be re-written and a new connection is required:\n",
    "            req = requests.get(sharepoint_adjust(req.url), headers = h_sharepoint, allow_redirects = True) \n",
    "            \n",
    "        else:\n",
    "            file_name_orig = requests.utils.unquote(req.request.url.split('/')[-1], encoding='utf-8', errors='replace')\n",
    "    else:\n",
    "        #file_name extracted from the content - usually idbdocs\n",
    "        file_name_orig = requests.utils.unquote(req.headers['Content-Disposition'].split('filename=')[-1].encode('latin-1').decode('utf-8')).replace('\"', '')\n",
    "    \n",
    "    \n",
    "    # Set name_prefix (v0.4)\n",
    "    if name_prefix != '':\n",
    "        #not empty\n",
    "        name_prefix = name_prefix + '_'\n",
    "    \n",
    "    \n",
    "    # Check file_name length (v0.4)\n",
    "    if len(file_dir + name_prefix + file_name_orig) > 240: \n",
    "        file_name_ini = file_name_orig[0:180]\n",
    "        file_name_fin = file_name_orig[-20:]\n",
    "        file_name = name_prefix + file_name_ini + '~' + file_name_fin\n",
    "    else:\n",
    "        file_name = name_prefix + file_name_orig\n",
    "    \n",
    "\n",
    "    # Check if file_name already present in destination folder (v0.4)\n",
    "    if file_name in os.listdir(file_dir):\n",
    "        file_name = ''.join(file_name.split('.')[:-1]) + '_' + '%s' + str('.') + file_name.split('.')[-1]\n",
    "        i = 1\n",
    "        while os.path.exists(file_dir + '\\\\' + file_name %i):\n",
    "            i += 1\n",
    "    \n",
    "        # Save the file\n",
    "        with open(file_dir + '\\\\' + file_name %i, 'wb') as f:\n",
    "            f.write(req.content)\n",
    "        print('Downloaded: ' + file_name %i)   #v0.3\n",
    "        return file_name %i \n",
    "    \n",
    "    else:\n",
    "        with open(file_dir + '\\\\' + file_name, 'wb') as f:\n",
    "            f.write(req.content)\n",
    "        print('Downloaded: ' + file_name)\n",
    "        return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Headers configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following variables must be set! ###\n",
    "cookie_idbdocs = 'XXXXXX'    ## <----\n",
    "cookie_sharepoint = 'YYYYYY' ## <----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idbdocs\n",
    "h_idbdocs = {\n",
    "    'method': 'GET',\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'cookie': cookie_idbdocs,\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36', #Chrome\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharepoint\n",
    "h_sharepoint = {\n",
    "    'method': 'GET',\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'cookie': cookie_sharepoint,\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36', #Chrome\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cookies & headers clean-up:\n",
    "#del cookie_idbdocs\n",
    "#del cookie_sharepoint\n",
    "#del h_idbdocs\n",
    "#del h_sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source xlsx file:\n",
    "data_ = pd.read_excel(r\"./input/Data-01 July 2020.xlsx\", sheet_name='data_filtered', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', \n",
    "       'CREATED', 'CREATED_BY', 'MODIFIED', 'MODIFIED_BY', 'PROJECT_NUMBER', \n",
    "     'APPROVAL_NUMBER', 'FUND', 'FK_COUNTRY_ID', 'FK_SUB_SECTOR_ID',\n",
    "    'DOCUMENT_NAME', 'URL']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approval Documents Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the filtered dataframe, and we'll add additional columns for storing the results:\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the document's name and its status, i.e.: 'downloaded', exception message \n",
    "df['Document_Name'] = '' #\n",
    "df['Document_Status'] = '' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document_URL'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination folder setup: all files will be downloaded to 'file_dir'\n",
    "\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir = desktop_dir + \"\\\\Approvals\"\n",
    "\n",
    "print(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approval Documents collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_test, r, df['OPERATION_NUMBER'][index]) \n",
    "        df.at[index, 'Document_Name'] = file_name\n",
    "        df.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_test, s, df['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df.at[index, 'Document_Name'] = file_name\n",
    "                                df.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_1 = df[df.Document_Status == ''].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# continuation of previous run, using pending_1 as index list\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index in pending_1:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_test, r, df['OPERATION_NUMBER'][index]) \n",
    "        df.at[index, 'Document_Name'] = file_name\n",
    "        df.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_test, s, df['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df.at[index, 'Document_Name'] = file_name\n",
    "                                df.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 5) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 5 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[480:485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_2 = df[df.Document_Status == ''].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# continuation of previous run, using pending_2 as index list\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index in pending_2:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_test, r, df['OPERATION_NUMBER'][index]) \n",
    "        df.at[index, 'Document_Name'] = file_name\n",
    "        df.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_test, s, df['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df.at[index, 'Document_Name'] = file_name\n",
    "                                df.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 5) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 5 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1010:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_3 = df[df.Document_Status == ''].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# continuation of previous run, using pending_3 as index list\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index in pending_3:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_test, r, df['OPERATION_NUMBER'][index]) \n",
    "        df.at[index, 'Document_Name'] = file_name\n",
    "        df.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_test, s, df['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df.at[index, 'Document_Name'] = file_name\n",
    "                                df.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 5) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 5 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Document_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store results and reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results: \n",
    "# v0.7 - 07/15: (loans)\n",
    "joblib.dump(df_4, './output/' + 'Loans-Doc_Collection_2020-07-15_v07_.joblib' + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.to_excel('Loans-Doc_Collection_2020-07-15_v07.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ ####\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results: \n",
    "# v0.6 - 07/14: (grants)\n",
    "joblib.dump(df_3, './output/' + 'Grants_Approvals-Doc_Collection_2020-07-14_v06_.joblib' + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_excel('Grants_Approvals-Doc_Collection_2020-07-14_v06.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ ####\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results: \n",
    "# v0.5 - 07/14: (TCs with null URL)\n",
    "joblib.dump(df_2, './output/' + 'TCs_Approval-NULL_URL-Doc_Collection_2020-07-14_v05_.joblib' + '.bz2', compress=('bz2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_excel('TCs_Approval-NULL_URL-Doc_Collection_2020-07-14_v05.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ ####\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results: \n",
    "# v0.4 - 07/08\n",
    "joblib.dump(df, './output/' + 'Approval_Documents_Collection_2020-07-08_v04_.joblib' + '.bz2', compress=('bz2', 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Approval_Documents_Collection_2020-07-08_v04.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~ ####\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******* ~ *** ~ * ~ *** ~ ******* #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Approval Documents w/NULL URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC's Approval Documents that presented a NULL URL, but there is a valid EzShare document reference. \n",
    "After filtering, <b>58 documents</b> were identified (07/14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source xlsx file:\n",
    "data_ = pd.read_excel(r\"./input/Data-01 July 2020.xlsx\", sheet_name='data_filtered_2', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', \n",
    "       'CREATED', 'CREATED_BY', 'MODIFIED', 'MODIFIED_BY', 'PROJECT_NUMBER', \n",
    "     'APPROVAL_NUMBER', 'FUND', 'FK_COUNTRY_ID', 'FK_SUB_SECTOR_ID',\n",
    "    'DOCUMENT_NAME', 'URL']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Using the EzShare code, fill in the URL field by including the common idbdocs url to request as:\n",
    "URL base: `\"https://idbdocs.iadb.org/wsdocs/getDocument.aspx?DOCNUM=\"` + `\"Codigo_EZSHARE\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_base = 'https://idbdocs.iadb.org/wsdocs/getDocument.aspx?DOCNUM='\n",
    "\n",
    "# column is float due to NaNs\n",
    "data_2.URL = data_2.URL.astype(str)\n",
    "\n",
    "for index, row in data_2.iterrows():\n",
    "    print('Processing index:', str(index))\n",
    "    url = url_base + data_2['DOCUMENT_REFERENCE'][index]\n",
    "    data_2.at[index, 'URL'] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.URL[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the filtered dataframe, and we'll add additional columns for storing the results:\n",
    "df_2 = data_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the document's name and its status, i.e.: 'downloaded', exception message and final URL\n",
    "df_2['Document_Name'] = '' #\n",
    "df_2['Document_Status'] = '' #\n",
    "df_2['Document_URL'] = '' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination folder setup: all files will be downloaded to 'file_dir_2'\n",
    "\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir_2 = desktop_dir + \"\\\\Approvals_NULLs\"\n",
    "\n",
    "print(file_dir_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approval Documents w/NULL URL collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index, row in df_2.iterrows():\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df_2['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_2, r, df_2['OPERATION_NUMBER'][index]) \n",
    "        df_2.at[index, 'Document_Name'] = file_name\n",
    "        df_2.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_2.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_2.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_2.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_2.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_2.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_2, s, df_2['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_2.at[index, 'Document_Name'] = file_name\n",
    "                                df_2.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_2.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_2.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_2.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[~(df_2['Document_Status'] == 'OK - download from redirect')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test not found OK:\n",
    "df_2['URL'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(saved results under v05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source xlsx file:\n",
    "data_ = pd.read_excel(r\"./input/Data-01 July 2020.xlsx\", sheet_name='data_3_grants', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pre filtering: remove those with NaN value in the 'DOCUMENT_NAME' field:\n",
    "data_ = data_[~data_['DOCUMENT_NAME'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = data_[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', \n",
    "       'CREATED', 'CREATED_BY', 'MODIFIED', 'MODIFIED_BY', 'PROJECT_NUMBER', \n",
    "     'APPROVAL_NUMBER', 'FUND', 'FK_COUNTRY_ID', 'FK_SUB_SECTOR_ID',\n",
    "    'DOCUMENT_NAME', 'URL']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the filtered dataframe, and we'll add additional columns for storing the results:\n",
    "df_3 = data_3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the document's name and its status, i.e.: 'downloaded', exception message and final URL\n",
    "df_3['Document_Name'] = '' #\n",
    "df_3['Document_Status'] = '' #\n",
    "df_3['Document_URL'] = '' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination folder setup: all files will be downloaded to 'file_dir_3'\n",
    "\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir_3 = desktop_dir + \"\\\\Grants_Approvals\"\n",
    "\n",
    "print(file_dir_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grants collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index, row in df_3.iterrows():\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df_3['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_3, r, df_3['OPERATION_NUMBER'][index]) \n",
    "        df_3.at[index, 'Document_Name'] = file_name\n",
    "        df_3.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_3.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_3.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_3.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_3.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_3.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_3, s, df_3['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_3.at[index, 'Document_Name'] = file_name\n",
    "                                df_3.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_3.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_3.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_3.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[~(df_3['Document_Status'] == 'OK - download from redirect')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2nd run for indexes: [54, 57, 58]\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index in [54, 57, 58]:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(df_3['URL'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_3, r, df_3['OPERATION_NUMBER'][index]) \n",
    "        df_3.at[index, 'Document_Name'] = file_name\n",
    "        df_3.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_3.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_3.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_3.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_3.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_3.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_3, s, df_3['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_3.at[index, 'Document_Name'] = file_name\n",
    "                                df_3.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_3.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_3.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_3.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(saved results under v0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source xlsx file:\n",
    "data_ = pd.read_excel(r\"./input/Data-01 July 2020.xlsx\", sheet_name='data_4_loans', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loans = data_[['FK_OPERATION_ID', 'OPERATION_NUMBER', 'DOCUMENT_ID', 'DOCUMENT_REFERENCE', 'DESCRIPTION', \n",
    "       'CREATED', 'CREATED_BY', 'MODIFIED', 'MODIFIED_BY', 'PROJECT_NUMBER', \n",
    "     'APPROVAL_NUMBER', 'FUND', 'FK_COUNTRY_ID', 'FK_SUB_SECTOR_ID',\n",
    "    'DOCUMENT_NAME', 'URL']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with the filtered dataframe, and we'll add additional columns for storing the results:\n",
    "df_4 = data_loans.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the document's name and its status, i.e.: 'downloaded', exception message and final URL\n",
    "df_4['Document_Name'] = '' #\n",
    "df_4['Document_Status'] = '' #\n",
    "df_4['Document_URL'] = '' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination folder setup: all files will be downloaded to 'file_dir_4'\n",
    "\n",
    "desktop_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\"\n",
    "file_dir_4 = desktop_dir + \"\\\\Loans_Approvals\"\n",
    "\n",
    "print(file_dir_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loans collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the EzShare code approach:\n",
    "url_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "# Using idbdocs + EzShare 'DOCUMENT_REFERENCE': \n",
    "\n",
    "for index, row in df_4.iterrows():\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(url_base + df_4['DOCUMENT_REFERENCE'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_4, r, df_4['OPERATION_NUMBER'][index]) \n",
    "        df_4.at[index, 'Document_Name'] = file_name\n",
    "        df_4.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_4.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_4.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_4.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_4, s, df_4['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_4.at[index, 'Document_Name'] = file_name\n",
    "                                df_4.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_4.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_4.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_4.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4[~(df_4['Document_Status'] == 'OK - download from redirect')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_1 = df_4[(df_4['Document_Status'] == '')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2nd run for indexes in pending_1\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "# Using idbdocs + EzShare 'DOCUMENT_REFERENCE': \n",
    "\n",
    "for index in pending_1:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(url_base + df_4['DOCUMENT_REFERENCE'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_4, r, df_4['OPERATION_NUMBER'][index]) \n",
    "        df_4.at[index, 'Document_Name'] = file_name\n",
    "        df_4.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_4.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_4.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_4.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_4, s, df_4['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_4.at[index, 'Document_Name'] = file_name\n",
    "                                df_4.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_4.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_4.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_4.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_2 = df_4[(df_4['Document_Status'] == '')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 3rd run for indexes in pending_2\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "# Using idbdocs + EzShare 'DOCUMENT_REFERENCE': \n",
    "\n",
    "for index in pending_2:\n",
    "    print('## processing index', str(index))\n",
    "    \n",
    "    checked_message, checked_url = url_check(url_base + df_4['DOCUMENT_REFERENCE'][index])\n",
    "    print(checked_message, checked_url)\n",
    "    r = requests.get(checked_url, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "    if 'application/' in r.headers['Content-Type']:\n",
    "        # download\n",
    "        print('download document')\n",
    "        file_name = download_file(file_dir_4, r, df_4['OPERATION_NUMBER'][index]) \n",
    "        df_4.at[index, 'Document_Name'] = file_name\n",
    "        df_4.at[index, 'Document_Status'] = 'OK - direct download'\n",
    "        df_4.at[index, 'Document_URL'] = r.url\n",
    "\n",
    "    else: \n",
    "        status = check_content(r)\n",
    "        if status in ['access denied','not found']:\n",
    "            print(status)\n",
    "            print('save result and break')\n",
    "            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "            df_4.at[index, 'Document_Status'] = status\n",
    "        \n",
    "        else:\n",
    "            print('continue')\n",
    "            print('... checking request.history[i].url ...')\n",
    "            for i in range(len(r.history)):      # cross-site authentication control\n",
    "                if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                    location = r.history[i].url  # effective URL after the redirects\n",
    "                    print(location)\n",
    "                    \n",
    "                    if 'sharepoint' in location:  # connect using sharepoint headers and cookie\n",
    "                        s = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                        status = check_content(s)\n",
    "                        if status in ['access denied','not found']:\n",
    "                            print('***')\n",
    "                            print(status)\n",
    "                            print('save result and break')\n",
    "                            df_4.at[index, 'Document_Name'] = 'not downloaded'\n",
    "                            df_4.at[index, 'Document_Status'] = status\n",
    "                            print('***')\n",
    "                            \n",
    "                        else: \n",
    "                            print('try downloading from', location)\n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_4, s, df_4['OPERATION_NUMBER'][index])             # download the document and get the filename\n",
    "                                df_4.at[index, 'Document_Name'] = file_name\n",
    "                                df_4.at[index, 'Document_Status'] = 'OK - download from redirect'\n",
    "                                df_4.at[index, 'Document_URL'] = s.url\n",
    "                                print('Downloaded!')\n",
    "                                #count =+ 1 \n",
    "                                break\n",
    "                                \n",
    "                            except Exception as e: \n",
    "                                df_4.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_4.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                            \n",
    "                    \n",
    "                    else: \n",
    "                        print('\\'sharepoint\\' not found in url', location)\n",
    "     \n",
    "                    del location\n",
    "    print('##')\n",
    "    print('')\n",
    "    t = t + 1\n",
    "    if (t % 7) == 0:\n",
    "        print('')\n",
    "        print(\"* * 2 seconds pause\")\n",
    "        time.sleep(2) # 2 sec pause inserted every 7 docs\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### PREVIOUS scraper - NOT TO BE USED\n",
    "\n",
    "%%time\n",
    "\n",
    "### Run 07/03-v_updated to correct idbdocs/sharepoint response on not found documents [latest version (06/18/2020) and variables dataframe df_2 and file_dir_2 - added .strip() to the URL parsing]\n",
    "\n",
    "t = 1     # counter set\n",
    "\n",
    "for index in [1187, 1223]:\n",
    "#for index in to_redownload:\n",
    "    print(\"Processing index: \", str(index))\n",
    "    if df_2.URL[index] != '':\n",
    "    \n",
    "        print(pcr_validate(df_2['URL'][index].strip()))\n",
    "    \n",
    "        if pcr_validate(df_2['URL'][index].strip())[0] == 'sharepoint_directly':\n",
    "\n",
    "            test = pcr_validate(df_2['URL'][index].strip())[1].strip()\n",
    "\n",
    "            r = requests.get(test, headers = h_sharepoint, allow_redirects = True)\n",
    "            #print('paso request')\n",
    "\n",
    "            if file_extension.search(test): #check if extension is present\n",
    "                file_name = download_file(file_dir_2, r)             # download the document and get the filename\n",
    "            else:\n",
    "                file_name = download_file_2(file_dir_2, r)             # download the document and get the filename\n",
    "\n",
    "            print('Downloading file: ' + file_name) \n",
    "            df_2.at[index, 'Document_Name'] = file_name\n",
    "            df_2.at[index, 'Document_Status'] = 'OK'\n",
    "    \n",
    "        elif pcr_validate(df_2['URL'][index].strip())[0] == 'idbdocs_directly':\n",
    "\n",
    "            test = pcr_validate(df_2['URL'][index].strip())[1].strip()\n",
    "\n",
    "            r = requests.get(test, headers = h_idbdocs, allow_redirects = True)\n",
    "            \n",
    "            if 'could not be found in Sharepoint EzShare' in str(r.content):  # document not found\n",
    "                df_2.at[index, 'Document_Name'] = 'Not found'\n",
    "                df_2.at[index, 'Document_Status'] = 'Not found'\n",
    "                print('Not Downloaded! - document not found')\n",
    "                \n",
    "            \n",
    "            elif 'Content-Disposition' in r.headers: \n",
    "                file_name = download_file_2(file_dir_2, r)             # download the document and get the filename\n",
    "                print('Downloading file: ' + file_name) \n",
    "                df_2.at[index, 'Document_Name'] = file_name\n",
    "                df_2.at[index, 'Document_Status'] = 'OK'\n",
    "\n",
    "            else:\n",
    "                for i in range(len(r.history)):      # cross-site authentication control\n",
    "                    if bool(re.search(r'\\.[a-z]{3}[a-z]?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)):\n",
    "                        location = r.history[i].url  # effective URL after the redirects\n",
    "                        print(location)\n",
    "\n",
    "\n",
    "                        r = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        file_name = download_file(file_dir_2, r)             # download the document and get the filename\n",
    "                        print('Downloading file: ' + file_name) \n",
    "                        df_2.at[index, 'Document_Name'] = file_name\n",
    "                        df_2.at[index, 'Document_Status'] = 'OK - redirected'\n",
    "                        print('Downloaded idbdocs redirected!')\n",
    "                        break\n",
    "\n",
    "        else: # 'connect_twice'\n",
    "\n",
    "            test = pcr_validate(df_2['URL'][index].strip())[1].strip()\n",
    "\n",
    "            r = requests.get(test, headers = h_idbdocs, allow_redirects = True)\n",
    "\n",
    "            if 'could not be found in Sharepoint EzShare' in str(r.content):  # document not found\n",
    "                df_2.at[index, 'Document_Name'] = 'Not found'\n",
    "                df_2.at[index, 'Document_Status'] = 'Not found'\n",
    "                print('Not Downloaded! - document not found')\n",
    "                \n",
    "            \n",
    "            elif 'Content-Disposition' in r.headers: \n",
    "                file_name = download_file_2(file_dir_2, r)             # download the document and get the filename\n",
    "                print('Downloading file: ' + file_name) \n",
    "                df_2.at[index, 'Document_Name'] = file_name\n",
    "                df_2.at[index, 'Document_Status'] = 'OK'\n",
    "\n",
    "            else:\n",
    "                for i in range(len(r.history)):      # cross-site authentication control\n",
    "                    if bool(re.search(r'\\.[a-z]{3}[a-z]?(\\?d\\=[a-z0-9]+)?$',r.history[i].request.url.lower())) and not ('Authenticate.aspx' in str(r.history[i].request.url)): # added control when url ends in ?d=...\n",
    "                        location = r.history[i].url  # effective URL after the redirects\n",
    "                        print(location)\n",
    "\n",
    "\n",
    "                        r = requests.get(location, headers = h_sharepoint, allow_redirects = True)\n",
    "                        \n",
    "                                                \n",
    "                        if ('AccessDenied.aspx' in str(r.content)) or ('does not have permissions to access this resource' in str(r.content)): # access denied!!! \n",
    "                            df_2.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                            df_2.at[index, 'Document_Status'] = 'Not downloaded'\n",
    "                            print('Not Downloaded! - access restricted')\n",
    "                            break\n",
    "                        \n",
    "                        else: \n",
    "                            try:\n",
    "                                file_name = download_file(file_dir_2, r)             # download the document and get the filename\n",
    "                                print('Downloading file: ' + file_name) \n",
    "                                df_2.at[index, 'Document_Name'] = file_name\n",
    "                                df_2.at[index, 'Document_Status'] = 'OK - redirected'\n",
    "                                print('Downloaded!')\n",
    "                                i = i + 1\n",
    "                                break\n",
    "                            \n",
    "                            except Exception as e: \n",
    "                                df_2.at[index, 'Document_Name'] = 'Not downloaded'\n",
    "                                df_2.at[index, 'Document_Status'] = str(e)\n",
    "                                print('Not downloaded: '+ str(e))\n",
    "                    \n",
    "        print('')\n",
    "        print('Processed document: ', str(t))                \n",
    "        t = t + 1\n",
    "        print('')\n",
    "        print('-----------------------------')\n",
    "        \n",
    "        if (t % 6) == 0:\n",
    "            print('')\n",
    "            print(\"*** 3 seconds pause\")\n",
    "            time.sleep(3) # 3 sec pause inserted every 6 docs\n",
    "            print('')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# ******************************************************************************************************************** #\n",
    "# *************************************************  Version Control  ************************************************ #\n",
    "# ******************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                    Change:                                          #\n",
    "\n",
    "#   - 0.7           07/15/2020        Emiliano Colina       - Loans document type added\n",
    "#\n",
    "\n",
    "#   - 0.6           07/14/2020        Emiliano Colina       - Grants document type added to the data collection process\n",
    "#\n",
    "\n",
    "#   - 0.5           07/14/2020        Emiliano Colina       - Added TCs that had 'NULL' value in the URL field, but \n",
    "#                                                           an EzShare code was present\n",
    "\n",
    "#   - 0.4           07/07/2020        Emiliano Colina       - Updated functions for data collection process\n",
    "#                                                           \n",
    "\n",
    "#   - 0.3           07/03/2020        Emiliano Colina       - Re-run data collection process on document list having \n",
    "#                                                           07/01 as cut-off date\n",
    "\n",
    "#   - 0.2           06/18/2020        Emiliano Colina       - Included TCs 'Approval Documents' type\n",
    "#                                                            \n",
    "\n",
    "#   - 0.1           06/16/2020        Emiliano Colina       - Initial version, started with 'Approval Registry' type\n",
    "#                                                            \n",
    "\n",
    "#\n",
    "# ******************************************************************************************************************** #\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
