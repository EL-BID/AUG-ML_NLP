{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 04.0 - Clustering Analysis Spanish - biased (workpaper)\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): joblib file\n",
    "#-- Description:  \n",
    "#                - CLustering analysis on Loans and TCs documents in Spanish\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @authors:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  0.8\n",
    "#-- Last Update: 01/22/2021\n",
    "#-- Last Revision Date: 10/20/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish TCs and Loans containing texts lemmatized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Spanish documents: TCs and Loans\n",
    "df_base = joblib.load('./output/nlp_df_result_tokens_terms_2021-01-21_spanish_v1.3_final.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df_base.alt2_data_lemmatized.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Specialized documents \n",
    "df_specialized = joblib.load('./output/nlp_spec_docs_2020-11-17_spanish_v1.3.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_docs = df_specialized.alt2_data_lemmatized.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Innovation\n",
    "innovacion_list = ['chatbot', '3_-d_printing', '3d_printing', '3d_printing', '4ir', '4ri', '5_g', '5g', 'ai', 'artificial_intelligence', 'augmented_reality', 'automatización_robot_proceso', 'automatización_robótica_proceso', 'autonomous_vehicle', 'autonomous_vehicles', 'base_dato_geoespacial', 'big_data',  'bigdata', 'bim',   'blockchain', 'blockchain_aplicado', 'building_information_modeling', 'building_information_modelling', 'cadena_de_bloque', 'captura_información_satelital', 'ciencia_computacional', 'ciudad_inteligente', 'cloud_computing', 'coche_autónomo', 'computación_cuántica', 'computación_nube', 'connected_vehicles', 'control_satélite', 'data_governance', 'data_science', 'dato_satelital', 'desarrollo_e-commerce', 'desarrollo_herramienta_big', 'desarrollo_innovación', 'desarrollo_modelo_predictivo', 'desarrollo_tecnología_satelital', 'digital_business', 'digital_economy', 'digital_governance', 'digital_identity', 'digital_transformation', 'digital_transformation_roadmaps', 'drone', 'drón', 'drón_espacio_aéreo', 'e-commerce', 'e-government', 'e-learning', 'e_-_government', 'e_-_learning', 'economía_digital', 'escalabilidad_nube', 'fintech', 'firma_digital', 'firma_electrónico', 'generación_producto_analítico', 'gobernanza_dato', 'gobierno_dato', 'gobierno_electrónico', 'herramienta_analítica', 'herramienta_analítica_dato', 'ia', 'identidad_digital', 'implementación_firma_digital', 'implementación_smart_citie', 'impresión_3_-_d', 'impresión_3d', 'imprimir_3d', 'incorporación_drón', 'información_origen_satelital', 'infraestructura_dato_espacial', 'infraestructura_digital', 'infraestructura_red', 'infraestructura_red_comunicación', 'infraestructura_red_crítico', 'innovación', 'innovación_tecnológica', 'innovation', 'innovative_public_procurement', 'integración_drón', 'inteligencia_artificial', 'internet_cosa', 'internet_de_las_cosas', 'internet_of_things', 'internet_thing', 'internet_things', 'iot', 'machine_learning', 'micro_computadora', 'modelo_predictivo', 'monitoreo_satelital', 'nanotechnology', 'nanotecnología', 'negocio_digital', 'nube_firma_digital', 'open_government_data', 'outcome_-_driven_innovation', 'piloto_blockchain', 'proyecto_piloto_blockchain', 'quantum_computing', 'realidad_aumentada', 'realidad_virtual', 'regulación_drón', 'robot_process_automation', 'robotic_process_automation', 'rpa', 'satellite_technology', 'satellites', 'satélite', 'sistema_energía_solar', 'smart_citie', 'smart_city', 'smart_city', 'technological_innovation', 'tecnología_blockchain', 'tecnología_satelital', 'transformación_digital', 'usuario_información_satelital', 'utilización_dato_geoespacial', 'vehículo_autónomo', 'virtual_reality', 'adopción_inteligencia_artificial', 'adopción_solución_inteligencia_artificial', 'contrato_inteligente_aplicado', 'contrato_inteligente', 'smart_contract', 'desarrollo_inteligencia_artificial', 'herramienta_inteligencia_negocio', 'sistema_inteligente', 'solución_inteligencia_artificial', 'tecnología_inteligencia_artificial', \\\n",
    "                  'hub_digital', 'digital_innovation_hub', 'implementación_sistema_identidad_digital', 'empresa_digital', 'apoyo_desarrollo_empresa_digital', 'apoyo_transformación_digital_empresarial', \\\n",
    "                  'apoyo_transformación_digital_sectorial', 'ecosistema_digital', 'empleo_digital', 'emprendimiento_digital', 'empresa_digital', 'entorno_digital', \\\n",
    "              'agenda_digital', 'estrategia_digital', 'estrategia_transformación_digital', 'ficha_médico_digital', 'tecnología_emergente', 'empresa_biotecnología', \\\n",
    "              'identificación_digital', 'historia_clínico_electrónico', 'proceso_transformación_digital_educación', 'bróker_digital', 'comercio_digital', 'control_digital', 'declaración_jurado_digital', 'desafío_digital', \\\n",
    "              'desarrollo_identidad_digital', 'trámite_digital', 'venta_digital', 'ventanilla_único_digital', 'visibilidad_digital', 'tecnología_digital_emergente','tecnología_disruptivo', 'tecnología_inteligencia_artificial', \\\n",
    "                  'proceso_digitalización_empresarial', 'proceso_fiscalización_digital', 'proceso_transformación_digital', 'reforma_digital', 'reforma_digital_administración_público', 'reforma_gobierno_digital', 'regulación_economía_digital', ]\n",
    "\n",
    "innovacion_list = list(sorted(innovacion_list))\n",
    "sorted(innovacion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciber_list = ['equipo_respuesta', 'ciberseguridad_transporte', 'ciberseguridad_salud', 'ciberataque_institución', 'conocimiento_ciberseguridad', 'conocimiento_seguridad_cibernético', \\\n",
    "              'estrategia_nacional_seguridad_cibernético', 'estrategia_seguridad_cibernético', 'fortalecimiento_ciberseguridad', 'gestión_ataque_cibernético', 'incidente_ciberseguridad', \\\n",
    "              'política_nacional_ciberseguridad', 'técnica_seguridad_cibernético', 'área_ciberseguridad', 'tema_ciberseguridad', 'respuesta_incidente_cibernético', \\\n",
    "              'respuesta_incidente', 'incidente_ciberseguridad', 'seguridad_sistema_información', 'sabotaje_informático', 'infraestructura_crítico', 'centro_inteligencia', \\\n",
    "              'control_digital', 'control_aeropuerto', 'control_aduanero', 'centro_control_tráfico', 'centro_control_metro', 'protocolo_intercambio_seguro_información', \\\n",
    "              'seguridad_infraestructura','seguridad_canal_comunicación', 'infraestructura_seguro', 'torre_control', 'torre_control_aeropuerto', 'herramienta_tecnológico_control', 'implementación_centro_control', \\\n",
    "              'proyecto_hidroeléctrico', 'infraestructura_red_comunicación', 'infraestructura_red_crítico', 'infraestructura_tecnología_información', 'infraestructura_telecomunicación', \\\n",
    "              'infraestructura_tic', 'infraestructura_resiliente', 'protocolo_comunicación', 'delito_informático', 'ley_protección_dato', 'riesgo_aplicación', 'infraestructura_telecomunicación', 'infraestructura_tecnológico', \\\n",
    "              'infraestructura_dato_espacial', 'protección_dato', 'información_biométrico', 'dato_biométrico', 'registro_biométrico',                \n",
    "             ]\n",
    "ciber_list = list(set(ciber_list))\n",
    "\n",
    "sistemas_list = ['aplicativo_móvil', 'aplicativo_core', 'red_privado_virtual', 'plataforma_software', 'diseño_software', 'diseño_aplicación', 'software_recurso_humano', 'software_solución_móvil', \\\n",
    "              'tipo_software', 'intervención_tipo_software', 'sistema_automatizado_monitoreo', \\\n",
    "              'software_comunicación', 'paquete_software', 'estructura_hardware', 'hardware_requerido', 'micro_computadora', \\\n",
    "              'proceso_computarizado', 'sistema_electrónico', 'sistema_automatizado_monitoreo', 'implementación_sistema_informático', 'modernización_sistema_informático', \\\n",
    "              'automatización_sistema_informático', 'sistema_informático_off-the-shelf', 'sistema_informático_salud', \\\n",
    "              'sistema_informático_transversal', 'implementación_sistema_informático_gestión', 'sistema_información_recurso_humano', \\\n",
    "              'sistema_información_sanitario', 'sistema_información_seguro', 'sistema_información_integrado', 'diseño_plataforma_digital', 'diseño_transformación_digital',\\\n",
    "              'seguridad_sistema_información', 'inventario_sistema_información', \\\n",
    "              'diseño_sistema_información', 'desarrollo_sistema_información', 'apoyo_sistema_información', 'diseño_sistema_monitoreo', 'fortalecimiento_sistema_monitoreo', \\\n",
    "              'implementación_sistema_monitoreo', 'sistema_monitoreo_electrónico', 'sistema_control_industria', 'centro_monitoreo', \\\n",
    "              'datar_center', 'solución_red', 'seguridad_red', 'resiliencia_red', 'scada', 'protocolo_comunicación', 'plataforma_gestión_información', \\\n",
    "              'conectado_red', 'conectividad_red', 'conexión_red', 'sitio_virtual', 'sitio_seguro', 'sitio_misión_crítico', 'servicio_móvil_acceso_internet', \\\n",
    "              'servicio_telecomunicación', 'telecomunicación_móvil', 'sistema_información_automatizado', 'sistema_información', 'desarrollo_software', 'desarrollo_plataforma_web', \\\n",
    "              'desarrollo_apps', 'desarrollo_app', 'aplicación_móvil', 'aplicación_informática', 'tecnología_información', 'sistema_tecnología_información', 'red_comunicación', \\\n",
    "              'aplicación_informático', 'aplicativo_informático', 'apoyo_informático', 'capacidad_solución_informático', 'desarrollo_programa_informático', 'equipamiento_informático', 'equipo_informático', 'fortalecimiento_sistema_apoyo_informático', 'herramienta_informático', 'implementación_herramienta_informático', \\\n",
    "              'programa_informático', 'solución_informático', 'servicio_digital_interoperable', 'plataforma_web', 'aplicación_web', 'aplicación_digital', 'base_dato', 'dato_telefonía_móvil', 'dato_telefonía_celular', \\\n",
    "              'infraestructura_informática', 'infraestructura_red', 'visualización_dato_masivo', 'volumen_dato', 'aplicación_dispositivo', 'aplicación_dispositivo_móvil', 'aplicativo_informático' ]\n",
    "\n",
    "sistemas_list = list(set(sistemas_list))\n",
    "\n",
    "\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation = pd.DataFrame(columns=['Short_Name', 'data_lemmatized'])\n",
    "df_innovation.at[0, 'Short_Name'] = 'innovation'\n",
    "df_innovation.at[0, 'data_lemmatized'] = innovacion_list\n",
    "df_innovation.at[1, 'Short_Name'] = 'sistemas'\n",
    "df_innovation.at[1, 'data_lemmatized'] = sistemas_list\n",
    "df_innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***********************************************************************************************\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is already tokenized in a custom way, so a dummy function is built in order to pass along what it receives\n",
    "# Source: http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (without specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 3 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None, \\\n",
    "                      max_features=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_countvectorizer = vec_count.fit_transform(data_lemmatized)\n",
    "matriz_count_vect = pd.DataFrame(matrix_countvectorizer.toarray(), columns=vec_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.columns[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect[matriz_count_vect['cibernético'] != 0]['cibernético']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.iloc[matriz_count_vect[matriz_count_vect['cibernético'] != 0]['cibernético'].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'ciber' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'software' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'hydro' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.sort_values(by='seguridad_cibernético', ascending=False)['seguridad_cibernético'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: sin agregar la \"base de conocimiento\", mediante CountVectorizer, el contenido de interés se pierde al realizar el filtrado.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\\\n",
    "                                   use_idf=True, \\\n",
    "                                max_df=0.6, min_df=3, \\\n",
    "                                preprocessor=dummy_fun, \\\n",
    "                                tokenizer=dummy_fun, \\\n",
    "                                token_pattern=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including 'knowledge base' by adding specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding innovation and a list of terms to keep (01/22/2021):\n",
    "data_full = data_lemmatized + specialized_docs + specialized_docs  + specialized_docs + \\\n",
    "    [specialized_docs[1]] + [specialized_docs[2]] + [specialized_docs[3]] + [specialized_docs[4]] + \\\n",
    "    [specialized_docs[6]] + \\\n",
    "    [specialized_docs[1]] + [specialized_docs[2]] + [specialized_docs[3]] + \\\n",
    "    [specialized_docs[6]] + \\\n",
    "    [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + \\\n",
    "    [df_innovation.data_lemmatized[1]] \n",
    "    \n",
    "#df_specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (including specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 2 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_countvectorizer_full = vec_count.fit_transform(data_full)\n",
    "matriz_count_vect_full = pd.DataFrame(matrix_countvectorizer_full.toarray(), columns=vec_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matriz_count_vect_full.shape)\n",
    "matriz_count_vect_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matriz_count_vect_full.columns[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect_full[matriz_count_vect_full['cibernético'] != 0]['cibernético']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if 'ciber' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if 'covid' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if 'software' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: Con la \"base de conocimiento\" incluida, el contenido de interés es enriquecido y así emerge más claramente.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer (including specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, \\\n",
    "                      analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 3 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None) #, \\\n",
    "                      #encoding='latin-1', \\\n",
    "                      #stop_words=final_stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix_full = tfidf_vectorizer.fit_transform(data_full) #fit the vectorizer to data_full\n",
    "idf_df_full = pd.DataFrame(tfidf_matrix_full.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "idf_df_full  # Doc-Term Matrix as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full.sort_values(by='seguridad_cibernético', ascending=False)['seguridad_cibernético'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota caso anterior: Con 20 Clusters, el agregado de los documentos especializados reforzó aquellos clusters por el lado de seguridad y de tecnología, al mismo tiempo que el glosario de ciberseguridad \n",
    " del Incibe quedó en el cluster 5: dato turístico productivo regional turismo niño competitividad inversión_público coordinación crédito rural pobreza mercar economía\n",
    "\n",
    " La solucion fue agregar más veces los documentos especializados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal cluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(np.asarray(X))\n",
    "X = tfidf_matrix_full\n",
    "X_array = np.asarray(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### current run: \"salted\" with specialized documents included multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "\n",
    "# run kmeans with many different k\n",
    "K = range(10, 40)\n",
    "for k in K:\n",
    "    print('Processing with k = ', k)\n",
    "    k_means = KMeans(n_clusters=k, random_state=100) #.fit(X_reduced)\n",
    "    k_means.fit(X_array)\n",
    "    distortions.append(sum(np.min(cdist(X_array, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method - optimal k')\n",
    "plt.grid(True)\n",
    "plt.savefig('clusters_elbow_distortion_jan22_v2_60xciento_3min_final.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Clusters - (2021/01/22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the 20 clusters generated by k-means:\n",
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, random_state=100)\n",
    "y_fit = kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "clusters = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared distance to cluster center\n",
    "X_dist = kmeans.transform(X)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the KMeans model:\n",
    "pickle.dump(kmeans, open(\"./output/clustering_kmeans_model_20clusters_2022-01-22_60xciento_3min.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the KMeans model from disk:\n",
    "kmeans = pickle.load(open(\"./output/clustering_kmeans_model_20clusters_2022-01-22_60xciento_3min.pkl\", 'rb'))\n",
    "y_fit = kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "clusters = kmeans.labels_.tolist()\n",
    "\n",
    "# squared distance to cluster center\n",
    "X_dist = kmeans.transform(X)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Clusters: Terms per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "for i in range(20):\n",
    "    #top_ten_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    #print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "    #print()\n",
    "    top_30_words = [terms[ind] for ind in order_centroids[i, :30]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_30_words)))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df specialized docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specialized docs\n",
    "df_spec_aux = pd.concat([df_specialized, df_specialized, df_specialized, \\\n",
    "                         df_specialized.iloc[1:5], \\\n",
    "                         df_specialized.iloc[6:7], \\\n",
    "                         df_specialized.iloc[1:4], \\\n",
    "                         df_specialized.iloc[6:7]], \\\n",
    "                         ignore_index=True)\n",
    "df_spec_aux.insert(loc=0, column='doc_type', value='specialized')\n",
    "df_spec_aux.rename(columns={'Short_Name': 'OPERATION_NUMBER'}, inplace=True)\n",
    "df_spec_aux.drop(['extracted_cleaned', 'alt2_terms', 'alt2_tokens', 'alt2_data_lemmatized'], axis=1, inplace=True)\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# innovation\n",
    "for i in range(36,39):\n",
    "    df_spec_aux.at[i, 'doc_type'] = 'specialized'\n",
    "    df_spec_aux.at[i, 'OPERATION_NUMBER'] = 'innovation'\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms to keep\n",
    "for i in range(39, 40): \n",
    "    df_spec_aux.at[i, 'doc_type'] = 'specialized'\n",
    "    df_spec_aux.at[i, 'OPERATION_NUMBER'] = 'sistemas'\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec_aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_base[['doc_type', 'OPERATION_NUMBER']], df_spec_aux], ignore_index=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters = pd.DataFrame({\n",
    "    'doc_type': df_all.doc_type,\n",
    "    'operation': df_all.OPERATION_NUMBER,\n",
    "    #'text': data_full,\n",
    "    'category': kmeans.labels_\n",
    "})\n",
    "results_full_20_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters.head(-40).category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters[results_full_20_clusters.category == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters[results_full_20_clusters.category == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del results_full_20_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top100 terms in Cluster 14\n",
    "[terms[ind] for ind in order_centroids[14, :100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge operations with the specialized_docs and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionado el modelo con 20 clusters (01/23)\n",
    "results_full = pd.DataFrame({\n",
    "    'doc_type': df_all.doc_type,\n",
    "    'operation': df_all.OPERATION_NUMBER,\n",
    "    #'text': data_full,\n",
    "    'category': kmeans.labels_\n",
    "})\n",
    "results_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge clustering results with all squared distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate results full using X_dist 20 clusters (squared distance to centroid)\n",
    "result_clustering = pd.concat([results_full, pd.DataFrame(np.column_stack(list(zip(*X_dist))), columns=['d0', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', \\\n",
    "                                                          'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd14', 'd15', 'd16', 'd17', 'd18', 'd19'])], axis=1, sort=False)\n",
    "result_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(idf_df_full, idf_df_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute similarity matrix (a numpy 2D array) from the idf_ matrix.\n",
    "similarity = cosine_similarity(idf_df_full)\n",
    "print(similarity.shape)\n",
    "\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_df = pd.DataFrame(similarity,\n",
    "                                columns = idf_df_full.index)\n",
    "                                #index = valid_snippets_ex)\n",
    "    \n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Evaluation - selected operations vs specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clustering.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#similarity_df[[1086, 1087, 1040, 1041, 1042, 1043, 1044, 1045, 1046]].head(-29)\n",
    "#df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation = similarity_df[[1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1122, 1125]].head(-40).copy()\n",
    "cosine_evaluation.rename(columns={1086:df_specialized.Short_Name[0], 1087:df_specialized.Short_Name[1], 1088:df_specialized.Short_Name[2], \\\n",
    "                                  1089:df_specialized.Short_Name[3], 1090:df_specialized.Short_Name[4], 1091:df_specialized.Short_Name[5], \\\n",
    "                                  1092:df_specialized.Short_Name[6], 1093:df_specialized.Short_Name[7], 1094:df_specialized.Short_Name[8], \\\n",
    "                                  1122:df_innovation.Short_Name[0], 1125:df_innovation.Short_Name[1]}, inplace=True)\n",
    "cosine_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results: \n",
    "cos_test = pd.concat([results_full[:-40], cosine_evaluation], axis=1)\n",
    "cos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Ciberseguridad - 10\n",
    "cos_test[cos_test['category'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "cos_test[cos_test['operation'] == 'RG-T3024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster Digital - 5\n",
    "cos_test[cos_test['category'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matriz_count_vect_full.shape)\n",
    "matriz_count_vect_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Doc Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_matrix = tfidf_matrix_full.todense().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term = pd.DataFrame(term_doc_matrix, \n",
    "                  columns=idf_df_full.index.to_list(), \n",
    "                  index=tfidf_vectorizer.get_feature_names()\n",
    "                      )\n",
    "df_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takes forever to run the following cell. A dimension reduction might be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix (a numpy 2D array) from the idf_ matrix.\n",
    "similarity = cosine_similarity(idf_df_full)\n",
    "print(similarity.shape)\n",
    "\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_df = pd.DataFrame(similarity,\n",
    "                                columns = idf_df_full.index)\n",
    "                                #index = valid_snippets_ex)\n",
    "    \n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute similarity matrix (a numpy 2D array).\n",
    "similarity_term = cosine_similarity(df_term)\n",
    "print(similarity_term.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_term_df = pd.DataFrame(similarity_term,\n",
    "                                columns = df_term.index,#)\n",
    "                                index = df_term.index)\n",
    "\n",
    "similarity_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_term_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top100 terms in Cluster 'Digital': 5\n",
    "[terms[ind] for ind in order_centroids[5, :300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top400 terms in Cluster 'Cybersecurity' : 10\n",
    "top_400_words_ciber = [terms[ind] for ind in order_centroids[10, :1000]]\n",
    "#print(top_400_words_ciber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_400_words_ciber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['organización', 'service', 'definición', 'respuesta', 'inventario', 'resultado', 'desarrollo', 'tipo', 'herramienta', 'operador', 'inventariado', 'código', 'requisito', 'comunicación', 'nombre', 'customer', 'procedimiento', 'equipo', 'confianza', 'empresa', 'dirección', 'figura', 'configuración', 'evento', 'medida', 'perfil', 'versión', 'persona', 'país', 'sector_eléctrico', 'integridad', 'prueba', 'criptológico_nacional', 'combinación', 'information', 'centro', 'elemento', 'función', 'categoría', 'entendimiento_estado_arte', 'identidad', 'entidad', 'proveedor', 'peligrosidad', 'fabricante', 'negocio', 'certificado', 'referencia', 'resource', 'misión', 'daño', 'recuperación', 'registro', 'víctima', 'legislación', 'técnica', 'copia', 'instalación', 'guía', 'distribución', 'formación', 'tipo_inventariado', 'número_activo', 'id', 'source', 'dominio', 'criticidad', 'system', 'alerta', 'crítica', 'número', 'coste', 'componente', 'provider', 'generación', 'virus', 'autoridad', 'autenticación', 'revisión', 'cantidad', 'método', 'mensaje', 'clasificación', 'ámbito', 'norma', 'suministro', 'sitio', 'tarea', 'término', 'personal', 'ventaja', 'mecanismo', 'posibilidad', 'archivo', 'propietario', 'cumplimiento', 'entendimiento', 'servicio_esencial', 'descripción', 'relación', 'certificación', 'comando', 'ciudadano', 'fuerza', 'entorno', 'inventariado_activo', 'cantidad_información', 'cantidad_activo', 'herramienta_open', 'herramienta_comercial', 'arte', 'fallo', 'subsector_eléctrico', 'documento', 'identificación', 'protocol', 'américa', 'intercambio', 'responsable', 'madurez', 'dependencia', 'sistema_control', 'glosario_término', 'cliente', 'operador_sistema_generación', 'alcance', 'vulnerabilidad', 'control_industrial', 'inspección_físico', 'hora', 'campo', 'objeto', 'requisito_seguridad', 'actividad_seguridad', 'cadena', 'privilegio', 'ley', 'preparación', 'frecuencia', 'energía', 'evento_seguridad', 'estación', 'práctica', 'transmisión', 'servidor_control', 'envío', 'formulador_política', 'redundancia', 'parche', 'aproximación', 'derecho', 'red_operacional', 'ubicación', 'conjunto', 'open', 'error', 'petición', 'profesional', 'página', 'continuidad', 'rol', 'carácter', 'manejo_crisis', 'información_línea', 'mecanismo_denuncia', 'característica', 'comprensión', 'estándar', 'soporte', 'red_social', 'decisión', 'tabla', 'herramienta_automatizado', 'control_técnico', 'principio', 'divulgación', 'evidencia', 'recomendación', 'proceso_gestión', 'dinero', 'punto', 'capacitación', 'unidad', 'informe', 'notificación', 'proceso_gestión_riesgo', 'tarjeta', 'criterio', 'investigación', 'contenido', 'access', 'autorización', 'grado', 'prevención', 'modificación', 'propiedad', 'control_interno', 'cpd_principal', 'respaldo', 'sistema_gestión', 'administración', 'teléfono', 'sociedad', 'alerta_temprano', 'sensibilización', 'consecuencia', 'plan_respuesta', 'tolerancia', 'realización', 'agreement', 'mejora_continuo', 'ocasión', 'planta', 'interrupción', 'lista', 'autenticidad', 'actividad_control_interno', 'actividad_control', 'implantación', 'inspección', 'riesgo_organización', 'inventario_manera_manual', 'forma_activo', 'implementación_mixto', 'inventariado_pasivo', 'herramienta_propietario', 'posible_activo', 'realización_inventario', 'jefe_planta', 'forma_pasivo', 'segmento', 'proyecto', 'conclusión', 'pauta', 'firma', 'valoración', 'processing', 'virtual_resource', 'deployment', 'información_activo', 'valoración_activo', 'información_necesario', 'glosario', 'responsabilidad', \\\n",
    "                  'paso', 'ingeniería', 'distribución_respuesta', 'libertad', 'máquina', 'núcleo', 'tolerancia_riesgo', 'monitoreo', 'dato_carácter', 'carácter_personal', 'cláusula', 'licencia', 'activo_físico', 'imagen', 'plan_continuidad', 'solución', 'conexión_remoto', 'separación', 'coordinación', 'inyección', 'entrada', 'cultura', 'recomendación_operador', 'reportar', 'arquitectura', 'mundo', 'estafador', 'red_área', 'área_local', 'cable', 'problema', 'disco', 'sector_energético', 'contingencia', 'servicio_institucional', 'business', 'materia', 'application', 'compromiso', 'denuncia', 'crisis', 'defecto', 'specific', 'nombre_dominio', 'regla', 'voz', 'credencial', 'directriz', 'funcionalidad', 'opinión', 'fase', 'consentimiento', 'prestación', 'compañía', 'paquete', 'libertad_civil', 'propósito', 'revisión_periódico', 'empresario', 'resources', 'sistema_organización', 'oferta', 'sistema_alerta', 'política_gestión', 'organismo_afectado', 'sistema_categoría', 'extensión_geográfico', 'reportar_anual', 'umbral_rojo', 'umbral_amarillo', 'nivel_peligrosidad', 'nivel_impacto', 'user', 'interconexión', 'canal', 'robo', 'transferencia', 'reputación', 'monitorización', 'penetración', 'metodología', 'conciencia', 'ciclo_vida', 'enfoque', 'nación', 'medio_comunicación', 'individuo', 'aspecto', 'pérdida', 'acceso_físico', 'disposición', 'naturaleza', 'implicación', 'puerta', 'transfer', 'procedimiento_documentado', 'mac', 'artículo', 'prioridad', 'política_documentado', 'creación', 'fuente', 'actuación', 'validación', 'navegador', 'facilidad', 'situación', 'exposición', 'canal_comunicación', 'habilidad', 'permiso', 'actor', 'referencia_informativo', 'log', 'especial_atención', 'industria', 'red_lano', 'repudio', 'centro_respaldo', 'glosario_término_ciberseguridad', 'seguridad_definición', 'remitente', 'autoridad_validación', 'sistema_conectado', 'puerta_trasero', 'model', 'medición', 'sistema_afectado', 'concienciación', 'inteligencia', 'equipamiento', 'entity', 'collection', 'informe_auditoría', 'contexto_guía', 'objetivo_auditoría', 'digital', 'puesto', 'factor', 'network', 'plan_contingencia', 'prestación_servicio', 'documentación', 'ámbito_aplicación', 'provision', 'desarrollo_estrategia', 'desbordamiento', 'verificación', 'campaña', 'país_alc', 'compraventa', 'comportamiento', 'autor', 'colaboración', 'recomendación_formulador', 'empleado', 'razón', 'aviso', 'divulgación_responsable', 'concientización', 'probabilidad', 'conducta', 'tercero', 'plataforma', 'texto', 'utilización', 'sat', 'consumidor', 'fuga', 'departamento', 'falsificación', 'marco_formación', 'marco_cooperación', 'servicio_gobierno', 'mentalidad', 'process', 'publicidad', 'formativo', 'interés', 'segmentación', 'institución_académico', 'españo', 'entrenamiento', 'continuación', 'marco_capacitación_profesional', 'marco_capacitación', 'crisis_identificación', 'protección_infantil', 'operación_coordinación', 'consumidor_legislación', 'línea_medio', 'electrónico_confianza', 'sobreprotección', 'sensibilización_ejecutivo_provisión_administración', 'orden_protección_infantil_línea', 'orden_protección_infantil', 'sensibilización_ejecutivo_provisión', 'provisión_administración', 'servicio_comercio_electrónico_confianza', 'formal_cooperación', 'formal_cooperación_informal', 'sensibilización_ejecutivo', 'servicio_comercio', 'comercio_electrónico_confianza', 'mercado_seguridad', 'tic_estándar_adquisición_estándar', 'ejecutivo_provisión_administración', 'ejecutivo_provisión', 'infantil_línea', 'orden_protección', 'persona_acceso', 'identificación_incidente_organización', 'privado_usuario', 'programa_sensibilización', 'dato_legislación', 'adquisición_estándar', 'mecanismo_denuncia_comprensión', 'respuesta_estrategia', 'persona_acceso_internet', 'identificación_incidente', 'cooperación_informal', 'penetración_internet', 'manejo_crisis_identificación', 'humanos_línea', 'empresa_segmento', 'estación_ingeniería', 'recomendación_formulador_política', 'laboral_ciberseguridad', 'condición', 'importancia', 'jefe', 'persecución', 'finalidad', 'socio', 'normativa', 'sensibilización_ejecutivo_provisión_administración_formación', 'provisión_administración_formación', 'administración_formación', 'ejecutivo_provisión_administración_formación', 'mail', 'competencia', 'marco_legislativo', 'resto', 'carta', 'lección_aprendido', 'sistema_justicia', 'tipo_información', 'exterior', 'participación', 'forma_remoto', 'ciclo', 'orden', 'capacitación_profesional', 'validez', 'desvío', 'mantenimiento', 'manejo', 'dato_almacenado', 'esfuerzo', 'responsable_seguro_cibernético_provisión', 'habitante_abono', 'cibernético_provisión', 'criptográfico_estándar', 'responsable_seguro', 'divulgación_responsable_seguro', 'divulgación_responsable_seguro_cibernético', 'divulgación_responsable_seguro_cibernético_provisión', 'seguridad_calidad', 'software_control_criptográfico_estándar', 'extensión', 'evento_sistema', 'level', 'avanzado', 'ausencia', 'humano', 'resultado_seguridad', 'serie', 'atención', 'transacción', 'notificación_incidente', 'característica_consistente', 'autoridad_responsable', 'concepto_operación',\\\n",
    "                   'setting', 'language', 'controlled', 'persona_físico_identificado', 'vida', 'provisión',  'destinatario', 'contexto', 'expresión',  'justicia_penal', 'radio', 'mercado', 'secuencia',  'efecto', 'extensión_geográfico_superior', 'autorizado_información',  'ens_anexo', 'servicio_superior', 'geográfico_superior',   'interrupción_prestación_servicio', 'persona_físico', 'desarrollador', 'comprobación',  'capa', 'diferencia', 'foro',   'conjunto_norma',    'sistema_reputación', 'acuerdo_licencia',      'aviso_legal', 'transferencia_dinero', 'rasgo', 'interfaz', 'escenario', 'abono', 'revocación', 'ocurrencia', 'card', 'destrucción', 'standard', 'movilidad_humano', 'tools', 'directiva',  'procedimiento_respuesta', 'acceso_lógico', 'universidad', 'platform', 'lección', 'esquema', 'privado_virtual', 'caducidad', 'código_conducta', 'agujero', 'unión',  'procedimiento_operativo', 'complejidad', 'noticia', 'segregación', 'tratamiento', 'infrastructure', 'placa', 'información_organización',  'conformidad', 'limitación', 'recurso_humano', 'comprador', 'continuidad_operación', 'humanos', 'declaración', 'mitigación', 'mensajería', 'proceso_negocio', 'oferta_formativo',  'desperfecto', 'toma', 'repercusión', 'comercio', 'sharing', 'visión', 'cross', 'management', 'siemens', 'examen', 'entrega', 'implicación_privacidad',   'contacto', 'administrador',  'comité', 'iniciativa', 'seguridad_ciudadano',  'carta_nación',  'crítico_centro',  'alto_dirección', 'mejor_práctica', 'edificio', 'señal',  'regulación', 'decreto', 'formulario', 'justicia', 'ecuatoriano', 'conectividad', 'aplicativo', 'nivel_seguridad', 'resolución', 'hoja', 'desinformación', 'tecnológico', 'level_agreement', 'recopilación',  'concreto', 'gestión_identidad', 'bloqueo', 'análisis_riesgo', 'gerencia', 'anexo', 'custodia', 'basura', 'escritorio', 'espacio',  'tribunal', 'riesgo_dependencia', 'responsabilidad_asignado', 'soporte_tercero', 'monitoreo_imagen', 'recurso_adecuado', 'práctica_documentado', 'dimensión', 'progreso',  'requerimiento',   'area', 'circunstancia',   'determinación', 'resumen', 'fomento', 'punto_vista', 'interesado_interno', \\\n",
    "                  'abierto', 'acceso_justicia', 'actor_involucrado', 'acusación', 'acusatorio', 'adecuación', 'adecuado', 'administrativo', 'adolescente', 'adopción', 'agenda', 'agente', 'alineado_estrategia', 'alternativo', 'anotado', 'análisis_criminal', 'análisis_dato', 'análisis_información', 'aplicado', 'aprendido', 'apropiación', 'arma', 'arquitectura_institucional', 'articulación', 'asalto', 'asistencia', 'audiencia', 'aumento', 'avance', 'barrio', 'barrio_seguro', 'barrio_vulnerable', 'basado', 'basado_evidencia', 'beneficiado', 'beneficiario', 'beneficio', 'bid_financiamiento', 'bienestar', 'big', 'bnde', 'brecha', 'básico', 'capacidad_gestión', 'capacidad_técnico', 'capacitado', 'capital_humano', 'capital_ordinario', 'carabinero', 'carpeta', 'carrera', 'caso_violencia', 'causa', 'ccp', 'centrado', 'ch', 'científico', 'ciudad', 'ciudadanía', 'civil', 'clínica', 'clínica_seguridad', 'comisaría', 'comisión', 'comuna', 'comunidad', 'comunitario', 'concentración', 'condena', 'conflicto', 'conflicto_ley', 'congreso', 'conocimiento_experiencia', 'conocimiento_experiencia_seguridad', 'consolidación_modelo', 'constitucional', 'construcción', 'consulta', 'consultoría', 'contagiado', 'contagio', 'continuo', 'contratación', 'contratación_servicio', 'convivencia', 'convivencia_ciudadano', 'corporativo', 'corrupción', 'covid', 'crimen', 'criminal', 'criminalidad', 'crisis_sanitario', 'cuadro', 'cuerpo', 'curso', 'custodio', 'cárcel', 'cívico', 'debilidad', 'delegación', 'delictivo', 'delictual', 'delincuencia', 'delito_violento', 'demanda', 'denunciado', 'desafío_seguridad', 'desafío_seguridad_ciudadano', 'desarrollo_capacidad_técnico', 'desarrollo_institución', 'desarrollo_institución_financiado', 'desarrollo_personal', 'desempeño', 'detalle', 'diagnóstico', 'difusión_conocimiento', 'difusión_conocimiento_experiencia', 'digitalización', 'diseminación', 'distrito', 'disuasión', 'diversidad', 'diálogo', 'doméstico', 'dp', 'década', 'edad', 'edición', 'educación', 'efectividad_policial', 'eficacia', 'eficaz', 'eficiencia', 'eficiente', 'eje', 'ejecución_plan', 'ejecutivo', 'elaboración', 'elaboración_manual', 'electrónico', 'emergencia', 'emergencia_sanitario', 'encarcelamiento', 'encargado', 'encuesta', 'escala', 'escaso', 'espacio_intercambio', 'espacio_intercambio_experiencia', 'especialización', 'especializado', 'establecimiento', 'estadística', 'estadístico', 'estatal', 'estratégico_desarrollo', 'estratégico_desarrollo_institución', 'expediente', 'experiencia_exitoso', 'experiencia_regional', 'experiencia_seguridad', 'experto', 'experto_técnico', 'factor_riesgo', 'federal', 'femicidio', 'fiscal', 'fiscalía', 'forense', 'formulación', 'fortalecimiento_institucional', 'fortalecimiento_sistema', 'frente', 'fuerza_policial', 'funcionario', 'gasto', 'generación_conocimiento', 'gestión_local', 'gestión_local_seguridad', 'gestión_penitenciario', 'gestión_técnico', 'global', 'gobernanza', 'gobierno_estatal', 'gobierno_federal', 'gobierno_panamá', 'gubernamental', 'género', 'habitante', 'hacinamiento', 'homicidio', 'hurto', 'idb', 'igualdad', 'impartición', 'implementación_modelo', 'implementación_plan', 'implementación_política', 'implementado', 'impunidad', 'incidencia', 'incidente_cibernético', 'inclusión', 'infractor', 'innovación_seguridad', 'innovador', 'ins', 'inseguridad', 'instancia', 'institución_financiado', 'institución_seguridad', 'instituto', 'instrumento', 'integrado', 'integral', 'intercambio_experiencia', 'intercambio_experiencia_regional', 'interior', 'interno', 'interoperabilidad_sistema', 'intervención', 'intervención_integral', 'intrafamiliar', 'introducción', 'investigación_criminal', 'involucrado', 'joven', 'joven_conflicto', 'joven_conflicto_ley', 'judicial', 'juez', 'jurisdicción', 'justicia_penal_acusatorio', 'justicia_región', 'juvenil', 'laboratorio', 'lac', 'latino', 'legislador', 'legitimidad', 'limitado', 'lineamiento', 'local_seguridad', 'lucha', 'maestro', 'manual', 'mapa', 'maquetación', 'marcha', 'marco_semana', 'material', 'mdc', 'mejora_gestión', 'mejora_sistema', 'mejoramiento', 'menor', 'meta', 'ministerio', 'mjp', 'modelo_atención', 'modelo_intervención', 'modelo_intervención_integral', 'modelo_prevención', 'modernización', 'modernización_policial', 'moderno', 'msp', 'mujer', 'mujer_víctima', 'municipal', 'municipio', 'mínimo', 'nivel_gobierno', 'nivel_local', 'nivel_provincial', 'nivel_regional', 'oficina', 'operativo', 'oportunidad', 'oportuno', 'ordinario', 'orientación', 'orientado', 'orientar', 'pandemia', 'papel', 'participante', 'particular', 'patrullaje', 'paz', 'país_región', 'pdi', 'pena', 'penal', 'penal_acusatorio', 'penitenciario', 'percepción', 'personal_policial', 'pgr', 'piloto', 'pj', 'plan_capacitación', 'plan_estratégico', 'plan_maestro', 'planificación', 'plataforma_diálogo', 'pn', 'policial', 'policía', 'policía_comunitario', 'política_público', 'porcentaje', 'posconflicto', 'ppl', 'preocupación', 'presente', 'preso', 'presupuesto_indicativo', 'prevención_delito', 'prevención_social', 'prevención_social_violencia', 'prevención_violencia', 'preventivo', 'prioritario', 'priorizado', 'prisión', 'privado_libertad', 'procesal', 'procesamiento', 'procesamiento_dato', 'proceso_investigación', 'procuración', 'procuración_justicia', 'procuraduría', 'producción', 'programa_estratégico', 'programa_estratégico_desarrollo', 'programa_prevención', 'programa_rehabilitación', 'programa_seguridad_ciudadano', 'programático', 'promedio', 'promisorio', 'propuesta', 'provincia', 'provincial', 'proyecto_ejecutivo', 'práctico', 'publicación', 'párrafo', 'público_seguridad', 'recepción', 'recolección', 'reducción', 'reforma', 'regional', 'región_alc', 'rehabilitación', 'reincidencia', 'reinserción', 'reinserción_social', 'relevante', 'resocialización', 'resolución_delito', 'resultado_esperado', 'rico', 'saj', 'salud', 'salud_público', 'sanción', 'sanitario', 'sección', 'sector_seguridad', 'seguimiento', 'seguridad_nación', 'seguridad_público', 'seguro', 'selección', 'semana_seguridad', 'semana_seguridad_ciudadano', 'sentencia', 'servicio_consultoría', 'servicio_prevención', 'sexual', 'sistema_administración_justicia', 'sistema_justicia_penal', 'sistema_penitenciario', 'sistematización', 'snic', 'social_violencia', 'sociedad_civil', 'sps', 'sub', 'subtotal', 'supervisión', 'susp', 'sólido', 'tasa', 'tasa_encarcelamiento', 'tasa_homicidio', 'temático', 'tendencia', 'territorio', 'toma_decisión', 'tomador', 'tomador_decisión', 'tramitación', 'transformación', 'transición', 'transparencia', 'unido', 'ur', 'urbano', 'victimario', 'victimización', 'vif', 'vigilancia', 'violencia', 'violencia_doméstico', 'violencia_intrafamiliar', 'violencia_mujer', 'violento', 'virtual', 'visita', 'vulnerable', 'víctima_vif', 'víctima_violencia', 'zona', 'índice', 'órgano']\n",
    "                  \n",
    "\n",
    "words_to_remove = list(set(words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_test = [word for word in top_400_words_ciber if word not in words_to_remove]\n",
    "words_to_test.append('seguridad_informático')\n",
    "words_to_test.append('servicio_seguridad')\n",
    "#words_to_test.append('servicio_de_seguridad_cibernético')\n",
    "words_to_test.append('sistema_seguridad')\n",
    "words_to_test.append('scada')\n",
    "words_to_test.append('sistema_scada')\n",
    "sorted(words_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_to_test_2 = ['acceso_remoto', 'access', 'actividad_del_operador', 'alertar', 'amenazar', 'application', 'archivo', 'arp', 'atacante', 'atacar', 'ataque', 'auditor', 'auditoría', 'autenticación', 'automático', 'cadena_de_suministrar', 'capabilities_type', 'capturar_de_tráfico', 'capturar_de_tráfico_de_red', 'centrar_de_dato', 'centro_criptológico', 'certificar_digital', 'ciberamenazas', 'ciberataques', 'ciberespacio', 'ciberincidente', 'ciberincidentes', 'cibernético', 'ciberseguridad', 'clavar_público', 'cloud', 'cobit', 'configuración', 'contraseña', 'control', 'control_industrial', 'copiar_de_seguridad', 'correar_electrónico', 'criptografía', 'criptográfico', 'criptológico', 'criptológico_nacional', 'csirt', 'customer', 'datar', 'dato_personal', 'dañino', 'daño', 'ddos', 'denegación', 'denegación_de_servicio', 'descifrar', 'descubrimiento_de_activo', 'detección', 'dirección_ip', 'dispositivo', 'dns', 'estrategia_de_seguridad', 'estrategia_nacional', 'estrategia_nacional_de_ciberseguridad', 'estrategia_nacional_de_seguridad', 'evento_de_ciberseguridad', 'evento_de_seguridad', 'fichero', 'firewall', 'gestión_de_riesgo', 'gusano', 'hardware', 'herramienta', 'http', 'ids', 'ilícito', 'inalámbrico', 'incidente', 'incidente_de_seguridad', 'infectar', 'información_personal', 'informático', 'infraestructura_crítico', 'ingeniería', 'intrusión', 'inventario_de_activo', 'inyección', 'ip', 'lan', 'legítimo', 'malicioso', 'malware', 'network', 'nist_sp_800-53', 'operador_de_sistema', 'operador_de_sistema_distribución', 'operador_de_sistema_generación', 'ordenador', 'parche', 'phishing', 'physical', 'plc', 'político_de_seguridad', 'privacidad', 'protección', 'provider', 'red', 'redar', 'resiliencia', 'respuesta', 'respuesta_a_incidente', 'respuesta_a_incidentes', 'riesgo_de_seguridad', 'riesgo_de_seguridad_cibernético', 'scada', 'security', 'seguridad_cibernética', 'seguridad_cibernético', 'seguridad_informático', 'seguridad_informático', 'seguridad_nacional', 'service', 'service_customer', 'service_provider', 'servicio_crítico', 'servicio_de_seguridad', 'servidor', 'sistema_de_información', 'sistema_de_seguridad', 'sistema_distribución', 'sistema_generación', 'sistema_scada', 'sniffer', 'software', 'source', 'sp_800-53', 'spoofing', 'sql', 'suplantación', 'suplantación_de_identidad', 'system', 'tcp', 'tecnología', 'tecnológico', 'tic', 'troyano', 'tráfico_de_red', 'término_de_ciberseguridad', 'vlan', 'vulnerabilidad', 'web', 'xss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innovacion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_test = list(sorted(set((words_to_test + words_to_test_2 + sistemas_list + innovacion_list))))\n",
    "words_to_test[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sistemas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean words_to test list:\n",
    "words_to_test_curated = []\n",
    "for word in words_to_test:\n",
    "    for column in idf_df_full.columns:\n",
    "        if word == column:\n",
    "            words_to_test_curated.append(word)\n",
    "\n",
    "len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_df_full[idf_df_full['scada'] > 0]['scada']\n",
    "idf_df_full[idf_df_full['machine_learning'] > 0]['machine_learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/21\n",
    "words_limpieza = ['activo_organización', 'alcance_auditoría', 'alteración', 'ambiente', 'analysis', 'based_on', 'capacidad_prevención', 'caracterización', 'componente_crítico', 'configuration', 'consenso', 'control_técnico_seguridad_calidad', 'criterio_auditoría', 'crítico', 'cumplimiento_procedimiento', 'datar_portability', 'declaración_requisito', 'deficiencia', 'difusión', 'ecosistema', 'ejercicio', 'enlace', 'entrega_servicio', 'estrategia_gestión', 'infrastructure_capabilitie', 'institución_público', 'jornada', 'manage', 'mando',  'motivación', 'nación_unido', 'necesidad_empresarial', 'nivel_servicio', 'objetivo_negocio', 'obligación', 'obtención', 'origen', 'pago', 'perímetro', 'platform_capabilitie', 'posición', 'propagación', 'práctica_seguridad', 'query', 'receptor', 'requisito_empresarial', 'resource_are_controlled', 'smart', 'structured', 'talento', 'violación', 'vista',\\\n",
    "                  'anticipación', 'atribución', 'autonomía', 'capítulo', 'catálogo', 'ciberdefensa_activo', 'ciberespacio_abierto', 'comportamiento_responsable', 'cosa', 'enjuiciamiento', 'español_ciberseguridad', 'estabilidad', 'estabilidad_ciberespacio', 'fiable_ciberespacio', 'fomento_confianza', 'implementación_marco', 'industria_español', 'industria_español_ciberseguridad', 'interés_nacional', 'miembro', 'normalización', 'responsabilidad_compartido', 'retención', 'sector_empresarial', 'soberanía', 'sofisticación']\n",
    "words_to_test_curated = [word for word in words_to_test_curated if word not in words_limpieza]\n",
    "len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((words_to_test_curated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df test: get the columns from the idf_df_full Doc-Term Matrix dataframe\n",
    "df_test = idf_df_full[words_to_test_curated].copy()\n",
    "print(df_test.columns)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste c/ponderacion (01/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xlsx with words:  (updated 01/22)\n",
    "df_ponderacion = pd.read_excel('./input/Ponderación Términos Cyber.xlsx', sheet_name = 'Sheet1')\n",
    "df_ponderacion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_innovacion = df_ponderacion['Innovación'].apply(str).str.strip().to_list()\n",
    "pond_cyber01 = df_ponderacion['Cyber Group1'].apply(str).str.strip().to_list()\n",
    "pond_cyber02 = df_ponderacion['Cyber Group2'].apply(str).str.strip().to_list()\n",
    "pond_cyber03 = df_ponderacion['Cyber Group3'].apply(str).str.strip().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pond_innovacion) + len(pond_cyber01) + len(pond_cyber02)+ len(pond_cyber03)\n",
    "len(set(pond_innovacion + pond_cyber01 + pond_cyber02 + pond_cyber03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "matches_inno = []\n",
    "to_review = []\n",
    "for index, row in df_test.iterrows():\n",
    "    count = 0\n",
    "    matching_col = []\n",
    "    matching_col_inno = []\n",
    "    ##\n",
    "    cyber_pond = []  #lista para almacenar valores ponderados de cada match de cada operacion\n",
    "    inno_pond = []\n",
    "    ##\n",
    "    for j in range(df_test.shape[1]):\n",
    "        if df_test.iloc[index,j] > 0.000001: \n",
    "            # ajuste para ponderar por pesos\n",
    "            \n",
    "            #### !!!!\n",
    "            df_test.iloc[index,j] = df_test.iloc[index,j] + 1\n",
    "            count = count + 1\n",
    "            matching_col.append(df_test.columns[j]) #store the matched term\n",
    "            \n",
    "            # inicializacion:\n",
    "            inno_pond_value = 0\n",
    "            cyber_pond_value = 0\n",
    "            \n",
    "            # ponderacion:\n",
    "            if df_test.columns[j] in pond_innovacion: # si esta en innovacion, la almaceno\n",
    "                inno_pond_value = df_test.iloc[index,j]\n",
    "                matching_col_inno.append(df_test.columns[j])\n",
    "                \n",
    "            elif df_test.columns[j] in pond_cyber01:\n",
    "                cyber_pond_value = 0.05 * df_test.iloc[index,j] # ponderacion al 5% \n",
    "            elif df_test.columns[j] in pond_cyber02:\n",
    "                cyber_pond_value = 0.4 * df_test.iloc[index,j] # ponderacion al 40% \n",
    "            elif df_test.columns[j] in pond_cyber03:\n",
    "                cyber_pond_value = df_test.iloc[index,j] # ponderacion al 100%\n",
    "                \n",
    "            else:\n",
    "                cyber_pond_value = 0.5 * df_test.iloc[index,j] # ponderacion al 50% y avisar que no está en las listas\n",
    "                print('Not found in ponderations lists:', df_test.columns[j])\n",
    "                to_review.append(df_test.columns[j])\n",
    "            \n",
    "            cyber_pond.append(cyber_pond_value)\n",
    "            inno_pond.append(inno_pond_value)\n",
    "    \n",
    "    cyb_sum = sum(cyber_pond)\n",
    "    inno_sum = sum(inno_pond)\n",
    "    print(matching_col, str(index), 'cyber_pond:', str(cyb_sum), 'inno_pond:', str(inno_sum))\n",
    "    \n",
    "    matches.append(matching_col)\n",
    "    matches_inno.append(matching_col_inno)\n",
    "    \n",
    "    df_test.at[index, 'count_findings'] = count\n",
    "    ####\n",
    "    df_test.at[index, 'pond_cyber'] = cyb_sum\n",
    "    df_test.at[index, 'pond_innovation'] = inno_sum\n",
    "    ####\n",
    "\n",
    "#Total sum per row: \n",
    "#df_test.loc[:,'total'] = df_test.sum(axis=1)\n",
    "df_test['total'] = df_test.apply(lambda col: col['3_-d_printing':'xss'].sum(),axis=1)\n",
    "\n",
    "df_test['matches_cyber'] = pd.Series(matches)\n",
    "df_test['matches_innovation'] = pd.Series(matches_inno)\n",
    "\n",
    "print('terminos para revisar porque no estan en ninguna lista:')\n",
    "set(to_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(to_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "matches = []\n",
    "for index, row in df_test.iterrows():\n",
    "    count = 0\n",
    "    matching_col = []\n",
    "    for j in range(df_test.shape[1]):\n",
    "        if df_test.iloc[index,j] > 0.00001: \n",
    "            # ajuste para ponderar por pesos\n",
    "            \n",
    "            #### !!!!\n",
    "            df_test.iloc[index,j] = df_test.iloc[index,j] + 1\n",
    "            count = count + 1\n",
    "            matching_col.append(df_test.columns[j]) #store the matched term\n",
    "    print(matching_col, str(index))\n",
    "    \n",
    "    matches.append(matching_col)\n",
    "    df_test.at[index, 'count_findings'] = count\n",
    "\n",
    "#Total sum per row: \n",
    "#df_test.loc[:,'total'] = df_test.sum(axis=1)\n",
    "df_test['total'] = df_test.apply(lambda col: col['3_-d_printing':'ámbito_ciberseguridad'].sum(),axis=1)\n",
    "\n",
    "df_test['matches'] = pd.Series(matches)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital - Cluster 5\n",
    "# Cybersecurity - Cluster 10\n",
    "# Innovation - \n",
    "\n",
    "df_test['d_cluster_digital'] = X_dist[:,5]\n",
    "df_test['d_cluster_ciber'] = X_dist[:,10]\n",
    "#df_test['d_cluster_innovation'] = X_dist[:,17]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.matches_cyber[863]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.matches_innovation[470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_aux = df_test[df_test.index.isin(cos_test.index)]\n",
    "df_test_aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_aux.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_aux = pd.concat([cos_test,df_test_aux[['d_cluster_digital', 'd_cluster_ciber',  \\\n",
    "                                               'matches_cyber', 'pond_cyber', 'count_findings', 'total', 'matches_innovation', 'pond_innovation']]], axis=1)\n",
    "df_test_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store all results that will feed the analysis\n",
    "result_clustering_total = pd.concat([result_clustering.head(-40), cos_test[['fwrk_agesic', 'report_electric_bid', 'fwrk_crit_infra_nist', 'report_bid_oas', \\\n",
    "       'lexicon_cyber_incibe', 'guide_ics_mgmt_inventory_incibe', \\\n",
    "       'nat_strat_spain', 'lexicon_ens_spain', 'guide_incident_ens_spain', 'innovation', 'sistemas']], \\\n",
    "       df_test_aux[['d_cluster_digital', 'd_cluster_ciber', 'matches_cyber', 'pond_cyber', 'count_findings', \\\n",
    "                    'total', 'matches_innovation', 'pond_innovation']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clustering_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# words_cluster_cyber_in_docs\n",
    "df_test_aux_2 = pd.concat([cos_test[['doc_type', 'operation', 'category']],df_test[df_test.index.isin(cos_test.index)]], axis=1)\n",
    "df_test_aux_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_results_spanish_2021-01-23' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    result_clustering_total.to_excel(writer, sheet_name='clustering_results')\n",
    "    df_test_aux_2.to_excel(writer, sheet_name='words_cluster_cyber_in_docs')\n",
    "    cos_test.to_excel(writer, sheet_name='similarity_specialized_docs')\n",
    "    similarity_df.to_excel(writer, sheet_name='similarity_all_docs')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_result_clustering = 'result_clustering_spanish_2021-01-23.joblib'\n",
    "joblib.dump(result_clustering_total, './output/' + df_result_clustering + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_cos_test = 'similarity_specialized_docs_spanish_2021-01-23.joblib'\n",
    "joblib.dump(cos_test, './output/' + df_cos_test + '.bz2', compress=('bz2', 3))   # similarity_specialized_docs\n",
    "\n",
    "df_similarity_df = 'similarity_all_docs_spanish_2021-01-23.joblib'\n",
    "joblib.dump(similarity_df, './output/' + df_similarity_df + '.bz2', compress=('bz2', 3))   # similarity_all_docs\n",
    "\n",
    "df_words_cluster_cyber = 'words_cluster_cyber_in_docs_spanish_2021-01-23.joblib'\n",
    "joblib.dump(df_test_aux_2, './output/' + df_words_cluster_cyber + '.bz2', compress=('bz2', 3))   # words_cluster_cyber_in_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[['doc_type', 'language', 'OPERATION_NUMBER', 'DOCUMENT_REFERENCE', 'extracted']].to_excel('./output/operaciones_spanish_content_2021_01_23.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_similarity_term_df = 'similarity_terms_spanish_2021-01-18.joblib'\n",
    "#joblib.dump(similarity_term_df, './output/' + df_similarity_term_df + '.bz2', compress=('bz2', 3))   # similiarity_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_results_spanish_2020-11-18' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    result_clustering_total.to_excel(writer, sheet_name='clustering_results')\n",
    "    df_test_aux_2.to_excel(writer, sheet_name='words_cluster_cyber_in_docs')\n",
    "    cos_test.to_excel(writer, sheet_name='similarity_specialized_docs')\n",
    "    similarity_df.to_excel(writer, sheet_name='similarity_all_docs')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_result_clustering = 'result_clustering_spanish_2020-11-18.joblib'\n",
    "joblib.dump(result_clustering_total, './output/' + df_result_clustering + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_cos_test = 'similarity_specialized_docs_spanish_2020-11-18.joblib'\n",
    "joblib.dump(cos_test, './output/' + df_cos_test + '.bz2', compress=('bz2', 3))   # similarity_specialized_docs\n",
    "\n",
    "df_similarity_df = 'similarity_all_docs_spanish_2020-11-18.joblib'\n",
    "joblib.dump(similarity_df, './output/' + df_similarity_df + '.bz2', compress=('bz2', 3))   # similarity_all_docs\n",
    "\n",
    "df_words_cluster_cyber = 'words_cluster_cyber_in_docs_spanish_2020-11-18.joblib'\n",
    "joblib.dump(df_test_aux_2, './output/' + df_words_cluster_cyber + '.bz2', compress=('bz2', 3))   # words_cluster_cyber_in_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[['doc_type', 'language', 'OPERATION_NUMBER', 'DOCUMENT_REFERENCE', 'extracted']].to_excel('./output/operaciones_spanish_content.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_term_df = 'similarity_terms_spanish_2020-11-18.joblib'\n",
    "joblib.dump(similarity_term_df, './output/' + df_similarity_term_df + '.bz2', compress=('bz2', 3))   # similiarity_term_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **************************************************************************************************************** #\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_results_spanish_2020-11-10' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    result_clustering_total.to_excel(writer, sheet_name='clustering_results')\n",
    "    df_test_aux_2.to_excel(writer, sheet_name='words_cluster_cyber_in_docs')\n",
    "    cos_test.to_excel(writer, sheet_name='similarity_specialized_docs')\n",
    "    similarity_df.to_excel(writer, sheet_name='similarity_all_docs')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_result_clustering = 'result_clustering_spanish_2020-11-10.joblib'\n",
    "joblib.dump(result_clustering_total, './output/' + df_result_clustering + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_cos_test = 'similarity_specialized_docs_spanish_2020-11-10.joblib'\n",
    "joblib.dump(cos_test, './output/' + df_cos_test + '.bz2', compress=('bz2', 3))   # similarity_specialized_docs\n",
    "\n",
    "df_similarity_df = 'similarity_all_docs_spanish_2020-11-10.joblib'\n",
    "joblib.dump(similarity_df, './output/' + df_similarity_df + '.bz2', compress=('bz2', 3))   # similarity_all_docs\n",
    "\n",
    "df_words_cluster_cyber = 'words_cluster_cyber_in_docs_spanish_2020-11-10.joblib'\n",
    "joblib.dump(df_test_aux_2, './output/' + df_words_cluster_cyber + '.bz2', compress=('bz2', 3))   # words_cluster_cyber_in_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Clusters' Keywords\n",
    "#values = model_list[2].print_topics(num_words=30)\n",
    "\n",
    "#with open(\"./output/LDA_spanish_12_topics_2020-10-21.txt\", \"w\") as output:\n",
    "#    output.write(str(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Euclidean distance between centroids:\n",
    "euclidean_distances(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance to cluster center\n",
    "X_dist_base = kmeans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_dist_base[0])\n",
    "print()\n",
    "print(X_dist_base[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[0]], [X_dist_base[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[0]], [X_dist_base[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[1066]], [X_dist_base[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_lemmas = pd.concat([result_clustering.head(-40), df_base[['alt2_data_lemmatized']]], axis=1)\n",
    "df_cluster_lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal number of topics, w.r.t. topic coherence\n",
    "\n",
    "def compute_coherence_values(id2word, corpus, texts,  \n",
    "                             k_start_val=2, k_end_val=18, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various numbers of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    id2word : Gensim dictionary.id2word\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    k_start_val: min num of topics\n",
    "    k_end_val : Max num of topics\n",
    "    step: the gap between one number of topics and another\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(k_start_val, k_end_val, step):\n",
    "        print(\"\\t *Building an lda model for number of topics = \", num_topics)\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           #alpha='symmetric',\n",
    "                                           alpha='auto',\n",
    "                                           minimum_probability=0,\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, \n",
    "                                        dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cluster_5 - Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 5\n",
    "# \n",
    "cluster5_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == 5]['alt2_data_lemmatized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "id2word = corpora.Dictionary(cluster5_data_lemmatized)\n",
    "print('Number of unique words in initital documents:', len(id2word))\n",
    "\n",
    "# Filter out words that occur less than 2 documents, or more than 60% of the documents.\n",
    "id2word.filter_extremes(no_below=2, no_above=0.60)\n",
    "\n",
    "#\n",
    "print('Number of unique words after removing common words:', len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words (BOW) Corpus\n",
    "#(Vectorize data, bag-of-words representation of each doc.)\n",
    "corpus = [id2word.doc2bow(text) for text in cluster5_data_lemmatized]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run including n-grams:\n",
    "model_list, coherence_values = compute_coherence_values(id2word=id2word, \n",
    "            corpus=corpus, texts=cluster5_data_lemmatized, k_start_val=2, \n",
    "            k_end_val=10, step=1)\n",
    "\n",
    "print(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=10; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "#plt.savefig(\"./output/Topic_Modeling-Coherence-english-2020-10-20.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) select best coherence value\n",
    "# 2) save model associated to the cluster\n",
    "# 3) visualization (?)\n",
    "# 4) classify each document in the cluster\n",
    "# 5) include contributions (top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View the topics in LDA model - 4 topics\n",
    "pprint(model_list[2].print_topics(num_words=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds of Top N Keywords in Each Topic\n",
    "\n",
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from matplotlib import cm\n",
    "from collections import OrderedDict\n",
    "cmaps = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmaps = OrderedDict()\n",
    "# more colors: 'mcolors.XKCD_COLORS'\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud(\n",
    "                  background_color='white',\n",
    "                  width=1500,\n",
    "                  height=1000,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10', # \n",
    "                  #colormap='tab20', # \n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "topics = model_list[2].show_topics(num_topics=4, num_words=16, log=False, formatted=False) # modificado a print_topics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=2, hspace=2)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"./output/WordCloud_12_topics-2020-10-20.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics_df = find_dominant_topics(ldamodel=model_list[2], corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dominant_topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cluster_8 - Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 8\n",
    "# \n",
    "cluster8_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == 8]['alt2_data_lemmatized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cluster_lemmas[df_cluster_lemmas['category'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "id2word8 = corpora.Dictionary(cluster8_data_lemmatized)\n",
    "print('Number of unique words in initital documents:', len(id2word8))\n",
    "\n",
    "# Filter out words that occur less than 2 documents, or more than 60% of the documents.\n",
    "id2word8.filter_extremes(no_below=2, no_above=0.60)\n",
    "\n",
    "#\n",
    "print('Number of unique words after removing common words:', len(id2word8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words (BOW) Corpus\n",
    "#(Vectorize data, bag-of-words representation of each doc.)\n",
    "corpus8 = [id2word8.doc2bow(text) for text in cluster8_data_lemmatized]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(id2word8))\n",
    "print('Number of documents: %d' % len(corpus8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run including n-grams:\n",
    "model_list8, coherence_values = compute_coherence_values(id2word=id2word8, \n",
    "            corpus=corpus8, texts=cluster8_data_lemmatized, k_start_val=2, \n",
    "            k_end_val=10, step=1)\n",
    "\n",
    "print(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=10; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "#plt.savefig(\"./output/Topic_Modeling-Coherence-english-2020-10-20.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) select best coherence value\n",
    "# 2) save model associated to the cluster\n",
    "# 3) visualization (?)\n",
    "# 4) classify each document in the cluster\n",
    "# 5) include contributions (top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## View the topics in LDA model - 4 topics\n",
    "pprint(model_list8[2].print_topics(num_words=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cmaps = OrderedDict()\n",
    "# more colors: 'mcolors.XKCD_COLORS'\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "cloud = WordCloud(\n",
    "                  background_color='white',\n",
    "                  width=1500,\n",
    "                  height=1000,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10', # \n",
    "                  #colormap='tab20', # \n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "topics = model_list8[2].show_topics(num_topics=4, num_words=16, log=False, formatted=False) # modificado a print_topics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,12), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=2, hspace=2)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"./output/WordCloud_12_topics-2020-10-20.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics_df_8 = find_dominant_topics(ldamodel=model_list8[2], corpus=corpus8)\n",
    "dominant_topics_df_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topics_df_8.Dominant_Topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling on every cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df_cluster_lemmas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_dict(data, low_filter=2, high_filter=0.60):\n",
    "    '''\n",
    "    Returns a Gensim's id2word and filtered dictionary and a corpus\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (11/19/2020)\n",
    "    '''   \n",
    "    # Create a dictionary representation of the documents.\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    print('\\t *Number of unique words in initital documents:', len(id2word))\n",
    "\n",
    "    # Filter out words that occur less than 2 documents, or more than 60% of the documents.\n",
    "    id2word.filter_extremes(no_below=low_filter, no_above=high_filter)\n",
    "\n",
    "    #\n",
    "    print('\\t *Number of unique words after removing common words:', len(id2word))\n",
    "    \n",
    "    # Create a bag-of-words (BOW) Corpus\n",
    "    #(Vectorize data, bag-of-words representation of each doc.)\n",
    "    corpus = [id2word.doc2bow(text) for text in data]    \n",
    "    \n",
    "    print('\\t *Number of documents: %d' % len(corpus))\n",
    "    \n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_modeling = pd.read_excel('./output/LDA_analisis.xlsx', sheet_name='cluster_topics')\n",
    "df_topic_modeling = df_topic_modeling.head(20).copy()\n",
    "df_topic_modeling = df_topic_modeling.astype(object)\n",
    "df_topic_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics = pd.concat([df_cluster_lemmas, pd.DataFrame(columns=['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic',\n",
    "       '2nd_Contrib'])])\n",
    "df_cluster_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics['Dominant_Topic'] = df_cluster_topics['Dominant_Topic'].astype(object)\n",
    "df_cluster_topics['Perc_Contrib'] = df_cluster_topics['Perc_Contrib'].astype(object)\n",
    "df_cluster_topics['Topic_Keywords'] = df_cluster_topics['Topic_Keywords'].astype(object)\n",
    "df_cluster_topics['2nd_Topic'] = df_cluster_topics['2nd_Topic'].astype(object)\n",
    "df_cluster_topics['2nd_Contrib'] = df_cluster_topics['2nd_Contrib'].astype(object)\n",
    "df_cluster_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics(ldamodel=None, corpus=corpus):\n",
    "\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        topic_computation = sorted(ldamodel.get_document_topics(corpus[i]), key=lambda x: x[1], reverse=True)[:3]\n",
    "        topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_computation[0][0])])\n",
    "        sent_topics_df = sent_topics_df.append(\n",
    "            pd.Series([topic_computation[0][0], round(topic_computation[0][1]*100,2), \\\n",
    "                       topic_keywords, \\\n",
    "                      topic_computation[1][0], round(topic_computation[1][1]*100,2), \\\n",
    "                      #topic_computation[2][0], round(topic_computation[2][1]*100,2) \\\n",
    "                      ]), ignore_index=True)\n",
    "        \n",
    "    \n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib', '3rd_Topic', '3rd_Contrib']\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics_one_topic(ldamodel=None, corpus=corpus):\n",
    "\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        topic_computation = sorted(ldamodel.get_document_topics(corpus[i]), key=lambda x: x[1], reverse=True)[:3]\n",
    "        topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_computation[0][0])])\n",
    "        sent_topics_df = sent_topics_df.append(\n",
    "            pd.Series([topic_computation[0][0], round(topic_computation[0][1]*100,2), \\\n",
    "                       topic_keywords, \\\n",
    "                      #topic_computation[1][0], round(topic_computation[1][1]*100,2), \\\n",
    "                      #topic_computation[2][0], round(topic_computation[2][1]*100,2) \\\n",
    "                      ]), ignore_index=True)\n",
    "        \n",
    "    \n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib', '3rd_Topic', '3rd_Contrib']\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(20):\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=2, high_filter=0.60 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/fina_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "\n",
    "    else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# continuation - clusters 11 to 19:\n",
    "for i in range(11,20):\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=2, high_filter=0.60 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/fina_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "\n",
    "    else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Topic Modeling on Cluster 5 - Cyber:\n",
    "for i in [10]:\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=1, high_filter=0.999 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/final_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "#        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "#        dominant_topics_df_x\n",
    "#    \n",
    "#        # adjust topic number to int:\n",
    "#        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "#        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "#        \n",
    "#        # append results to the original dataframe:\n",
    "#        for index, row in dominant_topics_df_x.iterrows():\n",
    "#            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "#            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "#            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "#            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "#            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "#\n",
    "    #else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics['category'] = df_cluster_topics['category'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_topic_modeling_spanish_2021-01-23' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    df_cluster_topics.to_excel(writer, sheet_name='cluster_topics_new')\n",
    "    df_topic_modeling.to_excel(writer, sheet_name='topic_modeling_decisions_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_df_cluster_topics = 'df_cluster_topics_spanish_2021-01-23.joblib'\n",
    "joblib.dump(df_cluster_topics, './output/' + df_df_cluster_topics + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_df_topic_modeling = 'df_topic_modeling_decisions_spanish_2021-01-23.joblib'\n",
    "joblib.dump(df_topic_modeling, './output/' + df_df_topic_modeling + '.bz2', compress=('bz2', 3))   # topic model decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## FIN - latest version - 01/23/2021 #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 0.6            01/16/2021         Emiliano Colina      - Latest version      \n",
    "#                                                        \n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
