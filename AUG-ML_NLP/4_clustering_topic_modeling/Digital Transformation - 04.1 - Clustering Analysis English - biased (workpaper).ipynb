{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "#*****************************************  IDB - AUG Data Analytics  ******************************************** #\n",
    "# **************************************************************************************************************** #\n",
    "#\n",
    "#-- Notebook Number: 04.1 - Clustering Analysis English - biased (workpaper)\n",
    "#-- Title: Digital Transformation Advisory\n",
    "#-- Audit Segment: \n",
    "#-- Continuous Auditing: Yes\n",
    "#-- System(s): joblib file\n",
    "#-- Description:  \n",
    "#                - CLustering analysis on Loans and TCs documents in English\n",
    "#                \n",
    "#                \n",
    "#                \n",
    "#\n",
    "#-- @authors:  Emiliano Colina <emilianoco@iadb.org>\n",
    "#-- Version:  0.8\n",
    "#-- Last Update: 02/09/2021\n",
    "#-- Last Revision Date: 10/20/2020 - Emiliano Colina <emilianoco@iadb.org> \n",
    "#                                    \n",
    "\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "main_dir = \"C:\\\\Users\\\\emilianoco\\\\Desktop\\\\2020\"\n",
    "data_dir = \"/Digital_Transformation\"\n",
    "\n",
    "\n",
    "os.chdir(main_dir + data_dir) # working directory set\n",
    "print('Working folder set to: ' + os.getcwd()) # working directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English TCs and Loans containing texts lemmatized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load English documents: TCs and Loans\n",
    "df_base = joblib.load('./output/nlp_df_result_tokens_terms_2021-01-22_english_v1.2.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df_base.alt2_data_lemmatized.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(df_base[df_base.OPERATION_NUMBER == 'NI-T1268']['extracted'])\n",
    "#df_base['extracted'][470]\n",
    "df_base['alt2_terms'][470]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Specialized documents \n",
    "df_specialized = joblib.load('./output/nlp_spec_docs_2021-02-08_english_v12_final.joblib.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 02/11: after several runs, the \"dig_paper\" is removed from the list of specialized documents\n",
    "df_specialized.drop(df_specialized.tail(1).index,inplace=True) # drop last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional clean-up since some tokens and terms were flagged to be removed when doing the clustering analysis:\n",
    "flagged_terms = ['-PRON-', '_', 'aadt', 'aaf', 'aaps', 'aastaraamatu', 'ababa', 'abac', 'abbreviation', 'abc', 'abd', \\\n",
    "                 'abovementioned', 'aforementioned', 'abraham', 'abrams', 'abs', 'zzz', 'ºc', 'õppetunnid', 'ˇthi', 'μs', '_recommendation', \\\n",
    "                 '→_insufficient', '→_insufficient', '→_weak', '→_weak', 'resolution_→', 'resolution_→', '→_weak_strategic_integral_management', '→_weak_strategic_integral_management', '→_lack', '→_lack', 'sector_→', 'sector_→', 'sector_→', 'private_sector_→', 'private_sector_→', 'private_sector_→', '→_weak_strategic_integral', '→_weak_strategic_integral', '→_lack_of_specialist', '→_lack_of_specialist', '→_weak_strategic', '→_weak_strategic', \\\n",
    "                 'framework_©_customer', 'framework_©_customer', 'framework_©', 'framework_©', 'framework_©', 'digital_transformation_framework_©_customer_insight', 'digital_transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'transformation_framework_©_customer_insight', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'digital_transformation_framework_©', 'framework_©_customer_insight', 'framework_©_customer_insight', 'use_digital_transformation_framework_©', 'use_digital_transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', 'transformation_framework_©', '©_customer_insight', '©_customer_insight', 'digital_transformation_framework_©_customer', 'digital_transformation_framework_©_customer', 'transformation_framework_©_customer_insight_customer', 'transformation_framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'framework_©_customer_insight_customer', 'transformation_framework_©_customer', 'transformation_framework_©_customer', '©_customer', '©_customer', '©_customer_insight_customer', '©_customer_insight_customer', 'use_digital_transformation_framework_©_customer', 'use_digital_transformation_framework_©_customer', \\\n",
    "                 'μs', 'offset_of_μs']\n",
    "\n",
    "\n",
    "for index, row in df_specialized.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_specialized.at[index, 'alt2_data_lemmatized'] = [word for word in df_specialized.alt2_data_lemmatized[index] if word not in flagged_terms]\n",
    "    \n",
    "# Additional terms were identified below and added to the to_remove list:\n",
    "for index, row in df_specialized.iterrows():\n",
    "    #print('Processing index:', str(index))\n",
    "    df_specialized.at[index, 'alt2_data_lemmatized'] = [word for word in df_specialized.alt2_data_lemmatized[index] if word not in to_remove]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialized_docs = df_specialized.alt2_data_lemmatized.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Innovation\n",
    "innovacion_list = ['3-d_printing', '3d_print', '3d_printing', '4ir', '4ri', '5_g', '5g', 'adoption_artificial_intelligence_solution', 'ai', 'analytical_product_generation', 'analytical_tool', 'applied_blockchain', 'artificial_intelligence', 'artificial_intelligence_adoption', 'artificial_intelligence_development', 'artificial_intelligence_solution', 'artificial_intelligence_technology', 'augmented_reality', 'autonomous_car', 'autonomous_vehicle', 'autonomous_vehicles', 'big_data', 'big_tool_development', 'bigdata', 'bim', 'biotechnology_company', 'block_chain', 'blockchain', 'blockchain_pilot', 'blockchain_pilot_project', 'blockchain_technology', 'bot', 'building_information_modeling', 'business_digitization_process', 'business_intelligence_tool', 'captures_satellite_information', 'chatbot', 'cloud_computing', 'cloud_digital_signature', 'cloud_scalability', 'communication_network_infrastructure', 'computational_science', 'connected_vehicles', 'critical_network_infrastructure', 'cryptocurrency', 'data_analytical_tool', 'data_governance', 'data_government', 'data_science', 'development_innovation', 'development_satellite_technology', 'digital_affidavit', 'digital_agenda', 'digital_broker', 'digital_business', 'digital_challenge', 'digital_commerce', 'digital_company', 'digital_company_development_support', 'digital_control', 'digital_economy', 'digital_economy_regulation', 'digital_ecosystem', 'digital_employment', 'digital_entrepreneurship', 'digital_environment', 'digital_governance', 'digital_government_reform', 'digital_hub', 'digital_identification', 'digital_identity', 'digital_identity_development', 'digital_identity_system_implementation', 'digital_infrastructure', 'digital_innovation_hub', 'digital_inspection_process', 'digital_medical_record', 'digital_procedure', 'digital_reform', 'digital_reform_public_administration', 'digital_sale', 'digital_signature', 'digital_signature_implementation', 'digital_single_window', 'digital_strategy', 'digital_transformation', 'digital_transformation_process', 'digital_transformation_roadmaps', 'digital_transformation_strategy', 'digital_visibility', 'disruptive_technology', 'drone', 'drone_airspace', 'drone_incorporation', 'drone_integration', 'drone_regulation', 'e_-_government', 'e_-_learning', 'e-commerce', 'e-commerce_development', 'e-government', 'e-learning', 'education_digital_transformation_process', 'electronic_government', 'electronic_medical_record', 'electronic_signature', 'emerging_digital_technology', 'emerging_technology', 'fintech', 'geospatial_data_base', 'ia', 'innovation', 'innovative_public_procurement', 'internet_thing', 'internet_things', 'internet_thing', 'internet_things', 'iot', 'machine_learning', 'micro_computer', 'nanotechnology', 'network_infrastructure', 'open_government_data', 'outcome-driven_innovation', 'predictive_model', 'predictive_model_development', 'print_3d', 'quantum_computing', 'robot_process_automation', 'robotic_process_automation', 'rpa', 'satelite', 'satellite_control', 'satellite_data', 'satellite_information_user', 'satellite_monitoring', 'satellite_origin_information', 'satellite_technology', 'satellites', 'sector_digital_transformation_support', 'smart_citie', 'smart_citie_implementation', 'smart_cities', 'smart_city', 'smart_contract', 'smart_contract_applied', 'smart_system', 'solar_energy_system', 'spatial_data_infrastructure', 'support_business_digital_transformation', 'technological_innovation', 'use_geospatial_data', 'virtual_reality']\n",
    "\n",
    "innovacion_list = list(sorted(innovacion_list))\n",
    "sorted(innovacion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sistemas_list = ['it_infrastructure', 'it_solution', 'it_support_system_strengthening', 'it_tool_implementation', 'informatic_team', 'web_application', 'web_platform', 'app_development', 'app_development', 'application_design', 'automated_information_system', 'automated_monitoring_system', 'cell_phone_data', 'communication_network', 'communication_protocol', 'communication_software', 'computer_application', 'computer_application', 'computer_application', 'computer_equipment', 'computer_program', 'computer_program_development', 'computer_support', 'computer_system_automation', 'computer_system_implementation', 'computer_system_modernization', 'computer_tool', 'computerized_process', 'computing_solution_capacity', 'core_application', 'data_volume', 'database', 'data_center', 'device_app', 'digital_application', 'digital_platform_design', 'digital_transformation_design', 'electronic_monitoring_system', 'electronic_system', 'hardware_structure', 'health_information_system', 'health_information_system', 'human_resource_information_system', 'human_resource_software', 'implementation_management_computer_system', 'implementation_monitoring_system', 'industry_control_system', 'information_management_platform', 'information_system', 'information_system_design', 'information_system_development', 'information_system_security', 'information_system_support', 'information_technology', 'information_technology_system', 'integrated_information_system', 'interoperable_digital_service', 'inventory_information_system', 'massive_data_display', 'micro_computer', 'mission_critical_site', 'mobile_app', 'mobile_application', 'mobile_device_application', 'mobile_internet_access_service', 'mobile_phone_data', 'mobile_solution_software', 'mobile_telecommunication', 'monitoring_center', 'monitoring_system_design', 'network_connected', 'network_connection', 'network_connectivity', 'network_infrastructure', 'network_resilience', 'network_security', 'network_solution', 'off-the-shelf_computer_system', 'required_hardware', 'safe_zone', 'scada', 'secure_information_system', 'software_design', 'software_development', 'software_package', 'software_platform', 'software_type', 'software_type_intervention', 'strengthening_monitoring_system', 'telecommunication_service', 'transversal_computer_system', 'virtual_private_network', 'virtual_site', 'web_platform_development']\n",
    "sistemas_list = list(set(sistemas_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation = pd.DataFrame(columns=['Short_Name', 'data_lemmatized'])\n",
    "df_innovation.at[0, 'Short_Name'] = 'innovation'\n",
    "df_innovation.at[0, 'data_lemmatized'] = innovacion_list\n",
    "df_innovation.at[1, 'Short_Name'] = 'sistemas'\n",
    "df_innovation.at[1, 'data_lemmatized'] = sistemas_list\n",
    "df_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***********************************************************************************************\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is already tokenized in a custom way, so a dummy function is built in order to pass along what it receives\n",
    "# Source: http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (without specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 2 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None, \\\n",
    "                      max_features=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_countvectorizer = vec_count.fit_transform(data_lemmatized)\n",
    "matriz_count_vect = pd.DataFrame(matrix_countvectorizer.toarray(), columns=vec_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_count_vect.columns[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.iloc[matriz_count_vect[matriz_count_vect['cybersecurity'] != 0]['cybersecurity'].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'cyber' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'software' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect.columns:\n",
    "    if 'hydro' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: sin agregar la \"base de conocimiento\", mediante CountVectorizer, el contenido de interés se pierde al realizar el filtrado.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\\\n",
    "                                   use_idf=True, \\\n",
    "                                max_df=0.6, min_df=3, \\\n",
    "                                preprocessor=dummy_fun, \\\n",
    "                                tokenizer=dummy_fun, \\\n",
    "                                token_pattern=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including 'knowledge base' by adding specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02/11:\n",
    "# adding innovation and systems related terms to keep:\n",
    "data_full = data_lemmatized + specialized_docs + specialized_docs + \\\n",
    "            [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + \\\n",
    "            [df_innovation.data_lemmatized[1]] \n",
    "#+ [df_innovation.data_lemmatized[1]] + [df_innovation.data_lemmatized[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (including specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 2 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_countvectorizer_full = vec_count.fit_transform(data_full)\n",
    "matriz_count_vect_full = pd.DataFrame(matrix_countvectorizer_full.toarray(), columns=vec_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matriz_count_vect_full.shape)\n",
    "matriz_count_vect_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matriz_count_vect_full.columns[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if 'covid' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if 'software' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in matriz_count_vect_full.columns:\n",
    "    if '-' in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: Con la \"base de conocimiento\" incluida, el contenido de interés es enriquecido y así emerge más claramente.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer (including specialized documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, \\\n",
    "                      analyzer='word', \\\n",
    "                      max_df=0.60, min_df=3, # cut terms that appear in more than 60% and less than 3 documents \\\n",
    "                      tokenizer=dummy_fun, \\\n",
    "                      preprocessor=dummy_fun, \\\n",
    "                      token_pattern=None) #, \\\n",
    "                      #encoding='latin-1', \\\n",
    "                      #stop_words=final_stop_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix_full = tfidf_vectorizer.fit_transform(data_full) #fit the vectorizer to data_full\n",
    "idf_df_full = pd.DataFrame(tfidf_matrix_full.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "idf_df_full  # Doc-Term Matrix as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full.sort_values(by='cybersecurity', ascending=False)['cybersecurity'].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal cluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(np.asarray(X))\n",
    "X = tfidf_matrix_full\n",
    "X_array = np.asarray(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### current run: \"salted\" with specialized documents included multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "\n",
    "# run kmeans with many different k\n",
    "K = range(10, 40)\n",
    "for k in K:\n",
    "    print('Processing with k = ', k)\n",
    "    k_means = KMeans(n_clusters=k, random_state=100) #.fit(X_reduced)\n",
    "    k_means.fit(X_array)\n",
    "    distortions.append(sum(np.min(cdist(X_array, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method - optimal k')\n",
    "plt.grid(True)\n",
    "plt.savefig('clusters_elbow_distortion_feb09_60xciento_3min_English_final.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 Clusters (2021/02/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the 20 clusters generated by k-means:\n",
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, random_state=100)\n",
    "y_fit = kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "clusters = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared distance to cluster center\n",
    "X_dist = kmeans.transform(X)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the KMeans model:\n",
    "pickle.dump(kmeans, open(\"./output/clustering_kmeans_model_20clusters_2021-02-11_60xciento_3min_ENGLISH.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 Clusters: Terms per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "for i in range(20):\n",
    "    #top_ten_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    #print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "    #print()\n",
    "    top_30_words = [terms[ind] for ind in order_centroids[i, :30]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_30_words)))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df specialized docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# initial run 02/09:\n",
    "# adding innovation and systems related terms to keep:\n",
    "data_full = data_lemmatized + specialized_docs + specialized_docs + \\\n",
    "            [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + \\\n",
    "            [df_innovation.data_lemmatized[1]] + [df_innovation.data_lemmatized[1]] + [df_innovation.data_lemmatized[1]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specialized docs\n",
    "df_spec_aux = pd.concat([df_specialized, df_specialized], ignore_index=True)\n",
    "df_spec_aux.insert(loc=0, column='doc_type', value='specialized')\n",
    "df_spec_aux.rename(columns={'Short_Name': 'OPERATION_NUMBER'}, inplace=True)\n",
    "df_spec_aux.drop(['extracted_cleaned', 'alt2_terms', 'alt2_tokens', 'alt2_data_lemmatized'], axis=1, inplace=True)\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding innovation and a list of terms to keep (01/21/2021):\n",
    "#data_full = data_lemmatized + specialized_docs + specialized_docs + [specialized_docs[1]] + [specialized_docs[2]] + \\\n",
    "#    [specialized_docs[4]] + [specialized_docs[7]] + \\\n",
    "#    [specialized_docs[1]] + [specialized_docs[4]] + [specialized_docs[7]] + \\\n",
    "#    [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + [df_innovation.data_lemmatized[0]] + \\\n",
    "#    [df_innovation.data_lemmatized[1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# innovation\n",
    "for i in range(36,39):\n",
    "    df_spec_aux.at[i, 'doc_type'] = 'specialized'\n",
    "    df_spec_aux.at[i, 'OPERATION_NUMBER'] = 'innovation'\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms to keep\n",
    "#for i in range(41, 44): \n",
    "for i in range(39, 40): \n",
    "    df_spec_aux.at[i, 'doc_type'] = 'specialized'\n",
    "    df_spec_aux.at[i, 'OPERATION_NUMBER'] = 'sistemas'\n",
    "df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spec_aux.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_base[['doc_type', 'OPERATION_NUMBER']], df_spec_aux], ignore_index=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters = pd.DataFrame({\n",
    "    'doc_type': df_all.doc_type,\n",
    "    'operation': df_all.OPERATION_NUMBER,\n",
    "    #'text': data_full,\n",
    "    'category': kmeans.labels_\n",
    "})\n",
    "results_full_20_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters.head(-40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters.head(-40).category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_20_clusters.head(-40)[results_full_20_clusters.category == 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top100 terms in Cluster 18 - Cyber\n",
    "[terms[ind] for ind in order_centroids[18, :100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge operations with the specialized_docs and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionado el modelo con 20 clusters (02/12)\n",
    "results_full = pd.DataFrame({\n",
    "    'doc_type': df_all.doc_type,\n",
    "    'operation': df_all.OPERATION_NUMBER,\n",
    "    #'text': data_full,\n",
    "    'category': kmeans.labels_\n",
    "})\n",
    "results_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge clustering results with all squared distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenate results full using X_dist - 20 clusters (squared distance to centroid)\n",
    "result_clustering = pd.concat([results_full, pd.DataFrame(np.column_stack(list(zip(*X_dist))), columns=['d0', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', \\\n",
    "                                                          'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd14', 'd15', 'd16', 'd17', 'd18', 'd19'])], axis=1, sort=False)\n",
    "result_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(idf_df_full, idf_df_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute similarity matrix (a numpy 2D array) from the idf_ matrix.\n",
    "similarity = cosine_similarity(idf_df_full)\n",
    "print(similarity.shape)\n",
    "\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_df = pd.DataFrame(similarity,\n",
    "                                columns = idf_df_full.index)\n",
    "                                #index = valid_snippets_ex)\n",
    "    \n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Evaluation - selected operations vs specialized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clustering.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#similarity_df[[1086, 1087, 1040, 1041, 1042, 1043, 1044, 1045, 1046]].head(-29)\n",
    "#df_spec_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation = similarity_df[[826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 862, 865]].head(-40).copy()\n",
    "\n",
    "cosine_evaluation.rename(columns={826:df_specialized.Short_Name[0], 827:df_specialized.Short_Name[1], 828:df_specialized.Short_Name[2], \\\n",
    "                                  829:df_specialized.Short_Name[3], 830:df_specialized.Short_Name[4], 831:df_specialized.Short_Name[5], \\\n",
    "                                  832:df_specialized.Short_Name[6], 833:df_specialized.Short_Name[7], 834:df_specialized.Short_Name[8], \\\n",
    "                                  835:df_specialized.Short_Name[9], 836:df_specialized.Short_Name[10], 837:df_specialized.Short_Name[11], \\\n",
    "                                  838:df_specialized.Short_Name[12], 839:df_specialized.Short_Name[13], 840:df_specialized.Short_Name[14], \\\n",
    "                                  841:df_specialized.Short_Name[15], 842:df_specialized.Short_Name[16], 843:df_specialized.Short_Name[17], \\\n",
    "                                  862:df_innovation.Short_Name[0], 865:df_innovation.Short_Name[1]}, inplace=True)\n",
    "cosine_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<b> * SU-L1055</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.iloc[df_base[df_base.OPERATION_NUMBER == 'SU-L1055'].index.values.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation[611:612]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<b> * UR-L1152</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation.iloc[df_base[df_base.OPERATION_NUMBER == 'UR-L1152'].index.values.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<b> * CH-L1142</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation.iloc[df_base[df_base.OPERATION_NUMBER == 'CH-L1142'].index.values.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<b> * RG-T3024</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_evaluation.iloc[df_base[df_base.OPERATION_NUMBER == 'RG-T3024'].index.values.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<b> * CO-T1496</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_evaluation.iloc[df_base[df_base.OPERATION_NUMBER == 'CO-T1496'].index.values.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ~ ~ ~\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_evaluation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results: \n",
    "cos_test = pd.concat([results_full[:-40], cosine_evaluation], axis=1)\n",
    "cos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Ciberseguridad\n",
    "cos_test[cos_test['category'] == 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "cos_test[cos_test['operation'] == 'RG-T3024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster Digital\n",
    "cos_test[cos_test['category'] == 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matriz_count_vect_full.shape)\n",
    "matriz_count_vect_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Doc Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_matrix = tfidf_matrix_full.todense().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term = pd.DataFrame(term_doc_matrix, \n",
    "                  columns=idf_df_full.index.to_list(), \n",
    "                  index=tfidf_vectorizer.get_feature_names()\n",
    "                      )\n",
    "df_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takes forever to run the following cell. A dimension reduction might be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix (a numpy 2D array) from the idf_ matrix.\n",
    "similarity = cosine_similarity(idf_df_full)\n",
    "print(similarity.shape)\n",
    "\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_df = pd.DataFrame(similarity,\n",
    "                                columns = idf_df_full.index)\n",
    "                                #index = valid_snippets_ex)\n",
    "    \n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute similarity matrix (a numpy 2D array).\n",
    "similarity_term = cosine_similarity(df_term)\n",
    "print(similarity_term.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create similarity dataframe with appropriate column names and indices.\n",
    "similarity_term_df = pd.DataFrame(similarity_term,\n",
    "                                columns = df_term.index,#)\n",
    "                                index = df_term.index)\n",
    "\n",
    "similarity_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_term_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital - Cluster 19\n",
    "# Cybersecurity - Cluster 18\n",
    "# Innovation - Cluster 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top400 terms in Cluster 'Cybersecurity' : 18\n",
    "top_400_words_ciber = [terms[ind] for ind in order_centroids[18, :1000]]\n",
    "#print(top_400_words_ciber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_400_words_ciber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After cleaning-up the previous list:\n",
    "lista_final = ['cybersecurity', 'security', 'cyber', 'attack', 'device', 'threat', 'source', 'network', 'asset', 'action', 'control', 'incident', 'privacy', 'controls', 'information_system', 'scada', 'software', 'cybercrime', 'information_share', 'cyberspace', 'authentication', 'inventory', 'user', 'estonia', 'protocol', 'malicious', 'internet', 'cryptographic', 'server', 'national_cybersecurity', 'communication', 'vulnerability', 'cybersecurity_risk', 'scada_system', 'computer', 'medical_device', 'access_control', 'control_system', 'cybersecurity_strategy', 'capability', 'identity', 'risk_management', 'critical_infrastructure', 'digital', 'security_controls', 'firewall', 'information_security', 'configuration', 'defence', 'defense', 'application', 'cloud', 'response', 'encryption', 'national_cybersecurity_strategy', 'confidentiality', 'hardware', 'cyber_incident', 'code', 'awareness', 'packet', 'gps', 'password', 'security_measure', 'intelligence', 'asset_inventory', 'unauthorized', 'firmware', 'detection', 'environment', 'control_network', 'ip', 'media', 'disclosure', 'incident_response', 'ict', 'privacy_controls', 'trust', 'supply_chain', 'system_component', 'grid', 'resilience', 'failure', 'attack_surface', 'computing', 'risk_assessment', 'law_enforcement', 'national_security', 'attacker', 'estonian', 'cyberthreat', 'architecture', 'availability', 'wireless', 'controller', 'machine', 'industrial_control', 'technical_security_controls', 'property', 'controls_information_system', 'controls_information', 'cyber_threat', 'authorization', 'industrial_control_system', 'technical_security', 'cloud_computing', 'pii', 'identifiable_information', 'patch', 'denial', 'service_provider', 'message', 'command', 'available', 'plc', 'pmu', 'supplier', 'investigation', 'cybersecurity_policy', 'cyberattack', 'enforcement', 'cybersecurity_activity', 'identification', 'sensor', 'legislation', 'interface', 'malware', 'algorithm', 'distribution', 'supply', 'cert', 'criminal', 'security_policy', 'intrusion', 'clinical_information_system', 'online', 'version', 'transmission', 'share', 'person', 'clinical_information', 'manual', 'biometric', 'threat_intelligence', 'control_enhancement', 'prevention', 'corporate_network', 'automation', 'damage', 'cyber_defence', 'cloud_service', 'data_protection', 'mobile', 'power_grid', 'public_key', 'tactic', 'threat_actor', 'storage', 'nato', 'network_traffic', 'safety', 'system_security', 'likelihood', 'spoofing', 'csirt', 'defense', 'information_protection', 'router', 'penetration', 'advanced', 'cybersecurity_requirement', 'signal', 'patient', 'attack_scenario', 'crime', 'jam', 'functionality', 'security_requirement', 'assurance', 'clock', 'test', 'compromise', 'community', 'redundancy', 'ml', 'dmz', 'processing', 'information_technology', 'client', 'criticality', 'personal_data', 'risk_management_process', 'adversary', 'card', 'management_process', 'automatic', 'exposure', 'client_device', 'manipulation', 'attack_vector', 'platform', 'team', 'insight', 'communication_network', 'lack', 'exploit', 'timing', 'system_failure', 'prosecution', 'boundary', 'personal_information', 'issue', 'host', 'audit', 'medium', 'minister', 'record', 'notification', 'core', 'federal', 'denial_service', 'systematic', 'signature', 'content', 'item', 'remote_care', 'virtual', 'port', 'health_information', 'lexicon', 'passive', 'people', 'education', 'cryptographic_key', 'repudiation', 'phase', 'stakeholder', 'loss', 'actor', 'technological', 'security_service', 'accidental', 'theft', 'weakness', 'expert', 'hostile', 'insurance', 'type_procurement', 'barrier', 'web', 'possibility', 'clinical', 'identification_system', 'life_cycle', 'account', 'cyber_resilience', 'credential', 'partner', 'verification', 'collaboration', 'complexity', 'identifier', 'right', 'legacy', 'provision', 'classification', 'appropriate_activity', 'social_media', 'computation', 'nation', 'scan', 'operating', 'table', 'crisis', 'government_institution', 'dos', 'service_model', 'international_law', 'agent', 'compliance', 'complex', 'remote_access', 'anti', 'cyber_security', 'figure', 'outcome', 'station', 'smart', 'approach', 'secret', 'care_system', 'security_program', 'accountability', 'directive', 'office', 'member', 'means', 'process_control', 'insider', 'token', 'cryptography', 'cybersecurity_challenge', 'degree', 'email', 'certification', 'future', 'task', 'overview', 'confidence', 'guideline', 'network_attack', 'breach', 'wide', 'slave', 'programme', 'mode', 'procedural', 'senior', 'maturity', 'logical', 'specific_recommendation', 'frequency', 'accuracy', 'formal', 'intent', 'documentation', 'tolerance', 'source_code', 'computer_system', 'developer', 'acceptable', 'modern', 'rb', 'physical_access', 'course', 'engineering', 'check', 'commercial', 'problem', 'channel', 'care', 'coordination', 'range', 'physical_inspection', 'expertise', 'sectoral', 'vpn', 'administration', 'basis', 'rule', 'security_control', 'historian', 'intellectual', 'malicious_code', 'mind', 'evidence', 'actuator', 'penetration_testing', 'query', 'relay', 'network_security', 'greater', 'enhancement', 'database', 'separate', 'military', 'civil_liberty', 'glossary', 'customer', 'detail', 'nature', 'hand', 'thing', 'rfp', 'traffic_analysis', 'life', 'impact_assessment', 'mac', 'additional', 'importance', 'illicit', 'reliable', 'regulation', 'jurisdiction', 'access_controls', 'subject', 'detailed', 'perimeter', 'radio', 'installation', 'occurrence', 'major', 'planning', 'significant', 'deployment', 'scope', 'specialist', 'differential_privacy', 'security_risk', 'risk_tolerance', 'open_source', 'gsoc', 'governance', 'network_access', 'recovery', 'switch', 'unauthorized_access', 'gateway', 'public_sector', 'emergency', 'virtual_machine', 'backdoor', 'cyber_risk', 'backup', 'cyberdefence', 'internet_user', 'acquisition', 'authenticity', 'cybersecurity_incident', 'laptop', 'use_cyberspace', 'response_plan', 'methodology', 'reliability', 'bit', 'security_personnel', 'reputation', 'security_standard', 'defensive', 'exploitable', 'continuous', 'forensic', 'critical_infrastructure_protection', 'sensitive_data', 'cybersecurity_education', 'patching', 'incident_response_plan', 'cybersecurity_field', 'eavesdropping', 'physical_security', 'cybersecurity_culture', 'sensitive_information', 'antivirus', 'infrastructure_protection', 'professional_training', 'power_sector', 'supply_chain_risk', 'hmi', 'content_identification', 'online_media', 'service_user', 'chain_risk', 'modbus', 'aspect_cybersecurity', 'tcp', 'intellectual_property', 'contingency' 'contingency_plan', 'iot', 'networking', 'advanced_persistent', 'audit_record', 'physical_asset', 'decryption', 'asset_management', 'risk_management_practice', 'http', 'industrial_protocol', 'security_feature', 'network_segmentation', 'digital_signature', 'persistent_threat', 'advanced_persistent_threat', 'software_development', 'cybersecurity_community', 'authenticator', 'ip_address', 'cryptographic_algorithm', 'disk', 'factor_authentication', 'cipher', 'certification_authority', 'rbac', 'intrusion_detection', 'information_system_security', 'version_software', 'data_historian', 'virus', 'nist_special_publication', 'operating_system', 'configuration_file', 'malicious_activity', 'digital_society', 'human_resource', 'segmentation', 'organizational_asset', 'persistent', 'risk_management_program', 'configuration_management', 'topology', 'ciphertext', 'system_development', 'response_team', 'script', 'iac', 'absence', 'interconnection', 'exploitation', 'management_system', 'security_officer', 'investigative', 'cybersecurity_information', 'security_function', 'node', 'management_practice', 'security_strategy', 'communication_protocol', 'information_security_program', 'confidential', 'business_function', 'security_plan', 'boundary_protection', 'organizational_risk', 'adverse_effect', 'apt', 'government_management', 'exchange_information', 'private_key', 'website', 'security_domain', 'business_continuity', 'privacy_policy', 'segregation', 'process_level', 'critical_system', 'cybersecurity_training', 'ransomware', 'network_device', 'digital_agenda', 'network_communication', 'system_development_life_cycle', 'cybersecurity_industry', 'development_life_cycle', 'cryptographic_module', 'external_network', 'emergency_response', 'global_economy', 'cybersecurity_event', 'system_resource', 'authentication_protocol', 'identity_verification', 'power_station', 'real_time', 'communication_technology', 'exfiltration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['source', 'action', 'control', 'controls', 'inventory', 'user', 'estonia', 'protocol', 'internet', 'capability', 'configuration', 'response', 'packet', 'unauthorized', 'detection', 'environment', 'media', 'disclosure', 'privacy_controls', 'trust', 'failure', 'estonian', 'availability', 'controller', 'technical_security_controls', 'property', 'controls_information_system', 'controls_information', 'authorization', 'identifiable_information', 'patch', 'denial', 'message', 'available', 'supplier', 'investigation', 'enforcement', 'identification', 'legislation', 'interface', 'distribution', 'supply', 'criminal', 'online', 'version', 'transmission', 'share', 'person', 'manual', 'prevention', 'damage', 'tactic', 'storage', 'nato', 'safety', 'likelihood', 'penetration', 'advanced', 'signal', 'patient', 'jam', 'functionality', 'assurance', 'clock', 'test', 'compromise', 'community', 'redundancy', 'processing', 'client', 'criticality', 'adversary', 'card', 'automatic', 'manipulation', 'platform', 'team', 'insight', 'lack', 'timing', 'prosecution', 'boundary', 'issue', 'host', 'medium', 'minister', 'record', 'notification', 'core', 'federal', 'systematic', 'signature', 'content', 'item', 'remote_care', 'virtual', 'port', 'health_information', 'lexicon', \\\n",
    "                   'passive', 'people', 'education', 'phase', 'stakeholder', 'loss', 'actor', 'technological', 'accidental', 'theft', 'weakness', 'expert', 'hostile', 'insurance', 'type_procurement', 'barrier', 'possibility', 'clinical', 'account', 'credential', 'partner', 'verification', 'collaboration', 'complexity', 'identifier', 'right', 'provision', 'classification', 'appropriate_activity', 'nation', 'operating', 'table', 'crisis', 'government_institution', 'dos', 'international_law', 'agent', 'complex', 'anti', 'figure', 'outcome', 'station', 'smart', 'approach', 'secret', 'care_system', 'accountability', 'directive', 'office', 'member', 'means', 'process_control', 'insider', 'token', 'degree', 'certification', 'future', 'task', 'overview', 'confidence', 'guideline', 'wide', 'slave', 'programme', 'mode', 'procedural', 'senior', 'maturity', 'logical', 'specific_recommendation', 'frequency', 'accuracy', 'formal', 'intent', 'documentation', 'tolerance', 'acceptable', 'modern', 'rb', 'physical_access', 'course', 'engineering', 'check', 'commercial', 'problem', 'channel', 'care', 'coordination', 'range', 'physical_inspection', 'expertise', 'sectoral', 'administration', 'basis', 'rule', 'intellectual', 'mind', 'evidence', 'actuator', 'query', 'relay', 'greater', \\\n",
    "                   'enhancement', 'separate', 'military', 'civil_liberty', 'glossary', 'customer', 'detail', 'nature', 'hand', 'thing', 'life', 'impact_assessment', 'mac', 'additional', 'importance', 'illicit', 'reliable', 'regulation', 'jurisdiction', 'subject', 'detailed', 'perimeter', 'radio', 'installation', 'occurrence', 'major', 'planning', 'significant', 'deployment', 'scope', 'specialist', 'differential_privacy', 'gsoc', 'governance', 'recovery', 'switch', 'gateway', 'public_sector', 'emergency', 'backup', 'internet_user', 'acquisition', 'authenticity', 'response_plan', 'methodology', 'reliability', 'bit', 'reputation', 'security_standard', 'defensive', 'exploitable', 'continuous', 'antivirus', 'professional_training', 'power_sector', 'content_identification', 'service_user', 'contingencycontingency_plan', 'advanced_persistent', 'disk', 'factor_authentication', 'rbac', 'virus', 'nist_special_publication', 'human_resource', 'segmentation', 'organizational_asset', 'persistent', 'risk_management_program', 'topology', 'script', 'iac', 'absence', 'interconnection', 'investigative', 'node', 'management_practice', 'confidential', 'business_function', 'boundary_protection', 'organizational_risk', 'adverse_effect', 'government_management', 'segregation', 'process_level', \\\n",
    "                   'development_life_cycle', 'cryptographic_module', 'external_network', 'global_economy', 'power_station']\n",
    "                  \n",
    "\n",
    "words_to_remove = list(set(words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_test = [word for word in lista_final if word not in words_to_remove]\n",
    "words_to_test.append('scada')\n",
    "words_to_test.append('scada_system')\n",
    "sorted(words_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innovacion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sistemas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_test = list(sorted(set((words_to_test + sistemas_list + innovacion_list))))\n",
    "words_to_test[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean words_to test list:\n",
    "words_to_test_curated = []\n",
    "for word in words_to_test:\n",
    "    for column in idf_df_full.columns:\n",
    "        if word == column:\n",
    "            words_to_test_curated.append(word)\n",
    "\n",
    "len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_df_full[idf_df_full['scada'] > 0]['scada']\n",
    "idf_df_full[idf_df_full['machine_learning'] > 0]['machine_learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01/21\n",
    "#words_limpieza = ['activo_organización', 'alcance_auditoría', 'alteración', 'ambiente', 'analysis', 'based_on', 'capacidad_prevención', 'caracterización', 'componente_crítico', 'configuration', 'consenso', 'control_técnico_seguridad_calidad', 'criterio_auditoría', 'crítico', 'cumplimiento_procedimiento', 'datar_portability', 'declaración_requisito', 'deficiencia', 'difusión', 'ecosistema', 'ejercicio', 'enlace', 'entrega_servicio', 'estrategia_gestión', 'infrastructure_capabilitie', 'institución_público', 'jornada', 'manage', 'mando',  'motivación', 'nación_unido', 'necesidad_empresarial', 'nivel_servicio', 'objetivo_negocio', 'obligación', 'obtención', 'origen', 'pago', 'perímetro', 'platform_capabilitie', 'posición', 'propagación', 'práctica_seguridad', 'query', 'receptor', 'requisito_empresarial', 'resource_are_controlled', 'smart', 'structured', 'talento', 'violación', 'vista']\n",
    "#words_to_test_curated = [word for word in words_to_test_curated if word not in words_limpieza]\n",
    "#len(words_to_test_curated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((words_to_test_curated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df test: get the columns from the idf_df_full Doc-Term Matrix dataframe\n",
    "df_test = idf_df_full[words_to_test_curated].copy()\n",
    "print(df_test.columns)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste c/ponderacion (01/18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xlsx with words: \n",
    "df_ponderacion = pd.read_excel('./input/Ponderación Términos Cyber-EN.xlsx', sheet_name = 'Sheet1')\n",
    "df_ponderacion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_innovacion = df_ponderacion['Innovation'].apply(str).str.strip().to_list()\n",
    "pond_cyber01 = df_ponderacion['Cyber Group1'].apply(str).str.strip().to_list()\n",
    "pond_cyber02 = df_ponderacion['Cyber Group2'].apply(str).str.strip().to_list()\n",
    "pond_cyber03 = df_ponderacion['Cyber Group3'].apply(str).str.strip().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pond_innovacion) + len(pond_cyber01) + len(pond_cyber02)+ len(pond_cyber03)\n",
    "len(set(pond_innovacion + pond_cyber01 + pond_cyber02 + pond_cyber03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pond_cyber03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'machine_learning'in df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "matches_inno = []\n",
    "to_review = []\n",
    "for index, row in df_test.iterrows():\n",
    "    count = 0\n",
    "    matching_col = []\n",
    "    matching_col_inno = []\n",
    "    ##\n",
    "    cyber_pond = []  #lista para almacenar valores ponderados de cada match de cada operacion\n",
    "    inno_pond = []\n",
    "    ##\n",
    "    for j in range(df_test.shape[1]):\n",
    "        if df_test.iloc[index,j] > 0.00001: \n",
    "            # ajuste para ponderar por pesos\n",
    "            \n",
    "            #### !!!!\n",
    "            df_test.iloc[index,j] = df_test.iloc[index,j] + 1\n",
    "            count = count + 1\n",
    "            matching_col.append(df_test.columns[j]) #store the matched term\n",
    "            \n",
    "            # inicializacion:\n",
    "            inno_pond_value = 0\n",
    "            cyber_pond_value = 0\n",
    "            \n",
    "            # ponderacion:\n",
    "            if df_test.columns[j] in pond_innovacion: # si esta en innovacion, la almaceno\n",
    "                inno_pond_value = df_test.iloc[index,j]\n",
    "                matching_col_inno.append(df_test.columns[j])\n",
    "                \n",
    "            elif df_test.columns[j] in pond_cyber01:\n",
    "                cyber_pond_value = 0.1 * df_test.iloc[index,j] # ponderacion al 10% \n",
    "            elif df_test.columns[j] in pond_cyber02:\n",
    "                cyber_pond_value = 0.4 * df_test.iloc[index,j] # ponderacion al 40% \n",
    "            elif df_test.columns[j] in pond_cyber03:\n",
    "                cyber_pond_value = df_test.iloc[index,j] # ponderacion al 100%\n",
    "                \n",
    "            else:\n",
    "                cyber_pond_value = 0.5 * df_test.iloc[index,j] # ponderacion al 50% y avisar que no está en las listas\n",
    "                print('Not found in ponderations lists:', df_test.columns[j])\n",
    "                to_review.append(df_test.columns[j])\n",
    "            \n",
    "            cyber_pond.append(cyber_pond_value)\n",
    "            inno_pond.append(inno_pond_value)\n",
    "    \n",
    "    cyb_sum = sum(cyber_pond)\n",
    "    inno_sum = sum(inno_pond)\n",
    "    print(matching_col, str(index), 'cyber_pond:', str(cyb_sum), 'inno_pond:', str(inno_sum))\n",
    "    \n",
    "    matches.append(matching_col)\n",
    "    matches_inno.append(matching_col_inno)\n",
    "    \n",
    "    df_test.at[index, 'count_findings'] = count\n",
    "    ####\n",
    "    df_test.at[index, 'pond_cyber'] = cyb_sum\n",
    "    df_test.at[index, 'pond_innovation'] = inno_sum\n",
    "    ####\n",
    "\n",
    "#Total sum per row: \n",
    "#df_test.loc[:,'total'] = df_test.sum(axis=1)\n",
    "df_test['total'] = df_test.apply(lambda col: col['3-d_printing':'wireless'].sum(),axis=1)\n",
    "\n",
    "df_test['matches_cyber'] = pd.Series(matches)\n",
    "df_test['matches_innovation'] = pd.Series(matches_inno)\n",
    "\n",
    "print('terminos para revisar porque no estan en ninguna lista:')\n",
    "set(to_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(to_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital - Cluster 19\n",
    "# Cybersecurity - Cluster 18\n",
    "# Innovation - \n",
    "\n",
    "df_test['d_cluster_digital'] = X_dist[:,19]\n",
    "df_test['d_cluster_ciber'] = X_dist[:,18]\n",
    "#df_test['d_cluster_innovation'] = X_dist[:,17]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.matches_cyber[865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.matches_innovation[862]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_aux = df_test[df_test.index.isin(cos_test.index)]\n",
    "df_test_aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_aux.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_aux = pd.concat([cos_test,df_test_aux[['d_cluster_digital', 'd_cluster_ciber',  \\\n",
    "                                               'matches_cyber', 'pond_cyber', 'count_findings', 'total', 'matches_innovation', 'pond_innovation']]], axis=1)\n",
    "df_test_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store all results that will feed the analysis\n",
    "result_clustering_total = pd.concat([result_clustering.head(-40), cos_test[['fwrk_crit_infra_nist', 'report_bid_oas', 'guide_800_53_nist', \\\n",
    "    'lexicon_nist', 'guide_power_enisa', 'guide_ics_mgmt_inventory_incibe', 'guide_hospitals_enisa', 'report_ics_enisa', 'nat_strat_spain', \\\n",
    "    'nat_strat_estonia', 'guide_800_82_nist', 'lexicon_ics2', 'lexicon_fsb', 'report_info_sharing_wef', 'dig_business', 'dig_workgrp_1', \\\n",
    "    'dig_govmnt_assesmnt', 'dig_workgrp_2', 'innovation', 'sistemas']], \\\n",
    "                                     df_test_aux[['d_cluster_digital', 'd_cluster_ciber', 'matches_cyber', 'pond_cyber', 'count_findings', \\\n",
    "                    'total', 'matches_innovation', 'pond_innovation']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_clustering_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# words_cluster_cyber_in_docs\n",
    "df_test_aux_2 = pd.concat([cos_test[['doc_type', 'operation', 'category']],df_test[df_test.index.isin(cos_test.index)]], axis=1)\n",
    "df_test_aux_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_results_english_2021-02-16' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    result_clustering_total.to_excel(writer, sheet_name='clustering_results')\n",
    "    df_test_aux_2.to_excel(writer, sheet_name='words_cluster_cyber_in_docs')\n",
    "    cos_test.to_excel(writer, sheet_name='similarity_specialized_docs')\n",
    "    similarity_df.to_excel(writer, sheet_name='similarity_all_docs')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_result_clustering = 'result_clustering_english_2021-02-16.joblib'\n",
    "joblib.dump(result_clustering_total, './output/' + df_result_clustering + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_cos_test = 'similarity_specialized_docs_english_2021-02-16.joblib'\n",
    "joblib.dump(cos_test, './output/' + df_cos_test + '.bz2', compress=('bz2', 3))   # similarity_specialized_docs\n",
    "\n",
    "df_similarity_df = 'similarity_all_docs_english_2021-02-16.joblib'\n",
    "joblib.dump(similarity_df, './output/' + df_similarity_df + '.bz2', compress=('bz2', 3))   # similarity_all_docs\n",
    "\n",
    "df_words_cluster_cyber = 'words_cluster_cyber_in_docs_english_2021-02-16.joblib'\n",
    "joblib.dump(df_test_aux_2, './output/' + df_words_cluster_cyber + '.bz2', compress=('bz2', 3))   # words_cluster_cyber_in_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base[['doc_type', 'language', 'OPERATION_NUMBER', 'DOCUMENT_REFERENCE', 'extracted']].to_excel('./output/operaciones_english_2021-02-16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_similarity_term_df = 'similarity_terms_spanish_2021-01-18.joblib'\n",
    "#joblib.dump(similarity_term_df, './output/' + df_similarity_term_df + '.bz2', compress=('bz2', 3))   # similiarity_term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **************************************************************************************************************** #\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************** #\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Euclidean distance between centroids:\n",
    "euclidean_distances(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance to cluster center\n",
    "X_dist_base = kmeans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_dist_base[0])\n",
    "print()\n",
    "print(X_dist_base[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[0]], [X_dist_base[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[0]], [X_dist_base[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean_distances([X_dist_base[1066]], [X_dist_base[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_lemmas = pd.concat([result_clustering.head(-40), df_base[['alt2_data_lemmatized']]], axis=1)\n",
    "df_cluster_lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal number of topics, w.r.t. topic coherence\n",
    "\n",
    "def compute_coherence_values(id2word, corpus, texts,  \n",
    "                             k_start_val=2, k_end_val=18, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various numbers of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    id2word : Gensim dictionary.id2word\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    k_start_val: min num of topics\n",
    "    k_end_val : Max num of topics\n",
    "    step: the gap between one number of topics and another\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(k_start_val, k_end_val, step):\n",
    "        print(\"\\t *Building an lda model for number of topics = \", num_topics)\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           #alpha='symmetric',\n",
    "                                           alpha='auto',\n",
    "                                           minimum_probability=0,\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, \n",
    "                                        dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************  **************************************************  **************************************************\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling on every cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(df_cluster_lemmas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_dict(data, low_filter=2, high_filter=0.60):\n",
    "    '''\n",
    "    Returns a Gensim's id2word and filtered dictionary and a corpus\n",
    "    @ author: emilianoco\n",
    "    Version:\n",
    "        - v0.1 - (11/19/2020)\n",
    "    '''   \n",
    "    # Create a dictionary representation of the documents.\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    print('\\t *Number of unique words in initital documents:', len(id2word))\n",
    "\n",
    "    # Filter out words that occur less than 2 documents, or more than 60% of the documents.\n",
    "    id2word.filter_extremes(no_below=low_filter, no_above=high_filter)\n",
    "\n",
    "    #\n",
    "    print('\\t *Number of unique words after removing common words:', len(id2word))\n",
    "    \n",
    "    # Create a bag-of-words (BOW) Corpus\n",
    "    #(Vectorize data, bag-of-words representation of each doc.)\n",
    "    corpus = [id2word.doc2bow(text) for text in data]    \n",
    "    \n",
    "    print('\\t *Number of documents: %d' % len(corpus))\n",
    "    \n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_modeling = pd.read_excel('./output/LDA_analisis.xlsx', sheet_name='cluster_topics')\n",
    "df_topic_modeling = df_topic_modeling.head(20).copy()\n",
    "df_topic_modeling = df_topic_modeling.astype(object)\n",
    "df_topic_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics = pd.concat([df_cluster_lemmas, pd.DataFrame(columns=['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic',\n",
    "       '2nd_Contrib'])])\n",
    "df_cluster_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics['Dominant_Topic'] = df_cluster_topics['Dominant_Topic'].astype(object)\n",
    "df_cluster_topics['Perc_Contrib'] = df_cluster_topics['Perc_Contrib'].astype(object)\n",
    "df_cluster_topics['Topic_Keywords'] = df_cluster_topics['Topic_Keywords'].astype(object)\n",
    "df_cluster_topics['2nd_Topic'] = df_cluster_topics['2nd_Topic'].astype(object)\n",
    "df_cluster_topics['2nd_Contrib'] = df_cluster_topics['2nd_Contrib'].astype(object)\n",
    "df_cluster_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(20):\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=2, high_filter=0.60 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_Topic_Modeling-Coherence-ENGLISH_cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "\n",
    "    else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics(ldamodel=None, corpus=corpus):\n",
    "\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        topic_computation = sorted(ldamodel.get_document_topics(corpus[i]), key=lambda x: x[1], reverse=True)[:3]\n",
    "        topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_computation[0][0])])\n",
    "        sent_topics_df = sent_topics_df.append(\n",
    "            pd.Series([topic_computation[0][0], round(topic_computation[0][1]*100,2), \\\n",
    "                       topic_keywords, \\\n",
    "                      topic_computation[1][0], round(topic_computation[1][1]*100,2), \\\n",
    "                      #topic_computation[2][0], round(topic_computation[2][1]*100,2) \\\n",
    "                      ]), ignore_index=True)\n",
    "        \n",
    "    \n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib', '3rd_Topic', '3rd_Contrib']\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics_one_topic(ldamodel=None, corpus=corpus):\n",
    "\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        topic_computation = sorted(ldamodel.get_document_topics(corpus[i]), key=lambda x: x[1], reverse=True)[:3]\n",
    "        topic_keywords = \", \".join([word for word, prop in ldamodel.show_topic(topic_computation[0][0])])\n",
    "        sent_topics_df = sent_topics_df.append(\n",
    "            pd.Series([topic_computation[0][0], round(topic_computation[0][1]*100,2), \\\n",
    "                       topic_keywords, \\\n",
    "                      #topic_computation[1][0], round(topic_computation[1][1]*100,2), \\\n",
    "                      #topic_computation[2][0], round(topic_computation[2][1]*100,2) \\\n",
    "                      ]), ignore_index=True)\n",
    "        \n",
    "    \n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords', '2nd_Topic', '2nd_Contrib', '3rd_Topic', '3rd_Contrib']\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contrib', 'Topic_Keywords']\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(20):\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=2, high_filter=0.60 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/final_ENGLISH_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_ENGLISH_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "\n",
    "    else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in [19]:\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=2, high_filter=0.60 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/final_ENGLISH_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_ENGLISH_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "\n",
    "    else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Cluster Cyber, w/only 2 documents\n",
    "\n",
    "for i in [18]:\n",
    "    print('Processing cluster:', i)\n",
    "    \n",
    "    # get the operations' lemmas for the selected cluster:\n",
    "    clusterx_data_lemmatized = df_cluster_lemmas[df_cluster_lemmas['category'] == i]['alt2_data_lemmatized'].tolist()\n",
    "    \n",
    "    # get the list of original indexes for the selected cluster:\n",
    "    original_index = list(df_cluster_lemmas[df_cluster_lemmas['category'] == i].index)\n",
    "    \n",
    "    # create gensim dictionary: \n",
    "    id2wordx, corpusx = gensim_dict(data=clusterx_data_lemmatized, low_filter=1, high_filter=0.999 )\n",
    "    \n",
    "    print()\n",
    "    # run topic modeling algorithm from 1 to 4 topics:\n",
    "    model_listx, coherence_values = compute_coherence_values(id2word=id2wordx, \n",
    "            corpus=corpusx, texts=clusterx_data_lemmatized, k_start_val=1, \n",
    "            k_end_val=5, step=1)\n",
    "    \n",
    "    # store list of coherence values:\n",
    "    df_topic_modeling.at[i, 'coherence_values'] = coherence_values\n",
    "    \n",
    "    # find the maximun coherence value:\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    \n",
    "    # store max_index:\n",
    "    df_topic_modeling.at[i, 'max_coherence_index'] = max_index\n",
    "    \n",
    "    # save selected model to disk:\n",
    "    file_name = \"./output/final_ENGLISH_cluster_\" + str(i) + \"_lda_\" + str(max_index+1) + \".topic\"\n",
    "    model_listx[max_index].save(file_name)\n",
    "    \n",
    "    # store name of saved model:\n",
    "    df_topic_modeling.at[i, 'model_selected'] = file_name\n",
    "    \n",
    "    # store top30 words per each topic:\n",
    "    df_topic_modeling.at[i, 'topics'] = model_listx[max_index].print_topics(num_words=30)\n",
    "\n",
    "    print()\n",
    "    # generate graph:\n",
    "    limit=5; start=1; step=1;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.title(\"Topic Modeling for Cluster: \" + str(i) )\n",
    "    graph_name = \"./output/final_ENGLISH_Topic_Modeling-Coherence-cluster_\" + str(i) + \".png\"\n",
    "    plt.savefig(graph_name)\n",
    "    \n",
    "    # save graph to disk:\n",
    "    df_topic_modeling.at[i, 'graph'] = graph_name\n",
    "    \n",
    "    # show graph:\n",
    "    plt.show()\n",
    "    print()\n",
    "   \n",
    "    # calculate dominant topics and contribution:\n",
    "    if max_index > 0: # the cluster has more than 1 topic\n",
    "#        dominant_topics_df_x = find_dominant_topics(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "#        dominant_topics_df_x\n",
    "#    \n",
    "#        # adjust topic number to int:\n",
    "#        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "#        dominant_topics_df_x['2nd_Topic'] = dominant_topics_df_x['2nd_Topic'].astype(int)\n",
    "#        \n",
    "#        # append results to the original dataframe:\n",
    "#        for index, row in dominant_topics_df_x.iterrows():\n",
    "#            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "#            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "#            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "#            df_cluster_topics.at[original_index[index], '2nd_Topic'] = dominant_topics_df_x['2nd_Topic'][index]\n",
    "#            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = dominant_topics_df_x['2nd_Contrib'][index]\n",
    "#\n",
    "    #else: # the cluster has 1 topic only\n",
    "        print('The cluster', i, 'has only one topic!')\n",
    "        dominant_topics_df_x = find_dominant_topics_one_topic(ldamodel=model_listx[max_index], corpus=corpusx)\n",
    "        dominant_topics_df_x\n",
    "    \n",
    "        # adjust topic number to int:\n",
    "        dominant_topics_df_x['Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'].astype(int)\n",
    "        \n",
    "        \n",
    "        # append results to the original dataframe:\n",
    "        for index, row in dominant_topics_df_x.iterrows():\n",
    "            df_cluster_topics.at[original_index[index], 'Dominant_Topic'] = dominant_topics_df_x['Dominant_Topic'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Perc_Contrib'] = dominant_topics_df_x['Perc_Contrib'][index]\n",
    "            df_cluster_topics.at[original_index[index], 'Topic_Keywords'] = dominant_topics_df_x['Topic_Keywords'][index]\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Topic'] = 'na'\n",
    "            df_cluster_topics.at[original_index[index], '2nd_Contrib'] = 'na'\n",
    "    \n",
    "    \n",
    "\n",
    "    print('#####')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics['category'] = df_cluster_topics['category'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = './output/clustering_topic_modeling_english_2021-02-16' + '.xlsx' # file name\n",
    "## Output to new Excel containing each test on a different sheet\n",
    "\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    df_cluster_topics.to_excel(writer, sheet_name='cluster_topics_new')\n",
    "    df_topic_modeling.to_excel(writer, sheet_name='topic_modeling_decisions_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all results:\n",
    "df_df_cluster_topics = 'df_cluster_topics_english_2021-02-16.joblib'\n",
    "joblib.dump(df_cluster_topics, './output/' + df_df_cluster_topics + '.bz2', compress=('bz2', 3))  # clustering_results\n",
    "\n",
    "df_df_topic_modeling = 'df_topic_modeling_decisions_english_2021-02-16.joblib'\n",
    "joblib.dump(df_topic_modeling, './output/' + df_df_topic_modeling + '.bz2', compress=('bz2', 3))   # topic model decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## FIN - latest version - 02/16/2021 #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# **************************************************************************************************************** #\n",
    "# ********************************************  Version Control  ************************************************* #\n",
    "# **************************************************************************************************************** #\n",
    "  \n",
    "#   Version:            Date:                User:                   Change:                                       \n",
    "\n",
    "#   - 0.8            02/16/2021         Emiliano Colina      - Latest version English     \n",
    "#   - 0.6            01/16/2021         Emiliano Colina      - Latest version - Spanish forked    \n",
    "#                                                        \n",
    "\n",
    "#\n",
    "# **************************************************************************************************************** #\n",
    "#'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
